{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook runs the experiments of the other notebook with names in Hindi script. (essentially an attempt at feature engineering)\n",
    "\n",
    "Pronunciations of Hindi characters are more consistent than the combinations of English characters used to represent them, and this might help the models learn better.\n",
    "\n",
    "An issue may be that the library used for transliteration does not do a well enough job in converting the names. For instance, incorrectly converting 'u' between 'ु' and 'ू'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('15000_demoscrape.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library for transliteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['rajesh', 'ashok', 'vijay', 'amit', 'abhishek'], dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top5names = df['Name'].value_counts()[:5].index\n",
    "top5names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['singh', 'jain', 'das', 'patel', 'shah'], dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top5surnames = df['Surname'].value_counts()[:5].index\n",
    "top5surnames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Using the [Translate library](https://pypi.org/project/translate/) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from translate import Translator\n",
    "translator = Translator(to_lang=\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'top5names' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-4e28f6706de7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtop5names\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtranslator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'top5names' is not defined"
     ]
    }
   ],
   "source": [
    "for name in top5names:\n",
    "    print(translator.translate(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "दीप्ति\n",
      "जैन\n",
      "à¤¨à¥à¤¯à¥à¤à¥à¤°à¥à¤¨\n",
      "पटेल\n",
      "शाह\n"
     ]
    }
   ],
   "source": [
    "for surname in top5surnames:\n",
    "    print(translator.translate(surname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rajesh फिलिपिनो में सुरक्षा गार्ड के 11 जनरल के आदेश\n",
      "Singh सिंह\n",
      "Das दास\n",
      "Neha Neha\n"
     ]
    }
   ],
   "source": [
    "print('Rajesh', translator.translate(\"Rajesh\"))\n",
    "print(\"Singh\", translator.translate(\"Singh\"))\n",
    "print(\"Das\", translator.translate(\"Das\"))\n",
    "print(\"Neha\", translator.translate(\"Neha\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Translate library fails in unexpected ways and is therefore unusable, for example;\n",
    "* Some names are given a meaning instead (like with Rajesh)\n",
    "* Some names fail depending on capitalisation\n",
    "* Some names fail completely, where the output is in English (like with Neha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Using [Transliteration module in polyglot library](https://polyglot.readthedocs.io/en/latest/Transliteration.html#example)\n",
    "\n",
    "Reference issue if installing on Windows;\n",
    "https://github.com/aboSamoor/polyglot/issues/57"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from polyglot.transliteration import Transliterator\n",
    "from polyglot.text import Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hindi_t = Transliterator(source_lang='en', target_lang='hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "राजेश\n",
      "ाशोक\n",
      "विजय\n",
      "ामित\n",
      "ाबहिशेक\n"
     ]
    }
   ],
   "source": [
    "for name in top5names:\n",
    "    print(hindi_t.transliterate(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "सिंह\n",
      "जािन\n",
      "दास\n",
      "पटेल\n",
      "शाह\n"
     ]
    }
   ],
   "source": [
    "for surname in top5surnames:\n",
    "    print(hindi_t.transliterate(surname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Library struggles with 'अ' and combination characters like \"भ\" but does a decent job overall.\n",
    "\n",
    "The consistency of the failures may lead to them not affecting the learning significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Another alternative which I have not tried yet; https://pypi.org/project/indic-transliteration/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering out names that have non alphabetic characters or are less than 3 characters long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_name(name):\n",
    "    valid_chars=\"abcdefghijklmnopqrstuvwxyz\"\n",
    "    if(len(name)<=2):\n",
    "        return False\n",
    "    for ch in name:\n",
    "        if ch not in valid_chars:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstnames_mask = df['Name'].apply(valid_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "surnames_mask = df['Surname'].apply(valid_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullnames_mask = [fn and sn for fn,sn in zip(firstnames_mask,surnames_mask)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Hindi names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hindi_Name'] = [np.NaN]*len(df)\n",
    "df['hindi_Name'] = df['Name'][firstnames_mask].apply(hindi_t.transliterate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hindi_Surname'] = [np.NaN]*len(df)\n",
    "df['hindi_Surname'] = df['Surname'][surnames_mask].apply(hindi_t.transliterate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Surname</th>\n",
       "      <th>Community</th>\n",
       "      <th>hindi_Name</th>\n",
       "      <th>hindi_Surname</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yogita</td>\n",
       "      <td>singh</td>\n",
       "      <td>punjabi</td>\n",
       "      <td>योगिता</td>\n",
       "      <td>सिंह</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dibyendu</td>\n",
       "      <td>ghosh</td>\n",
       "      <td>bengali</td>\n",
       "      <td>दीबयेनदु</td>\n",
       "      <td>गहोश</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>veeramony</td>\n",
       "      <td>ramachandran</td>\n",
       "      <td>tamil</td>\n",
       "      <td>वीरामोनी</td>\n",
       "      <td>रामाचानद्रन</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>amlan</td>\n",
       "      <td>datta</td>\n",
       "      <td>bengali</td>\n",
       "      <td>ामलान</td>\n",
       "      <td>दट्टा</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>susmita</td>\n",
       "      <td>poddar</td>\n",
       "      <td>bengali</td>\n",
       "      <td>सुसमिता</td>\n",
       "      <td>पोददार</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>raja</td>\n",
       "      <td>banik</td>\n",
       "      <td>bengali</td>\n",
       "      <td>राजा</td>\n",
       "      <td>बानिक</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>rekha</td>\n",
       "      <td>jain</td>\n",
       "      <td>hindi</td>\n",
       "      <td>रेखा</td>\n",
       "      <td>जािन</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>subrato</td>\n",
       "      <td>roy</td>\n",
       "      <td>bengali</td>\n",
       "      <td>सुबरतो</td>\n",
       "      <td>रॉय</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dinesh</td>\n",
       "      <td>tiwari</td>\n",
       "      <td>hindi</td>\n",
       "      <td>दीनेश</td>\n",
       "      <td>तिवारी</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>shampa</td>\n",
       "      <td>chakraborty</td>\n",
       "      <td>bengali</td>\n",
       "      <td>शामपा</td>\n",
       "      <td>चाकराबोर्टय</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>nitu</td>\n",
       "      <td>drolia</td>\n",
       "      <td>hindi</td>\n",
       "      <td>नितु</td>\n",
       "      <td>दरोलिया</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>sunder</td>\n",
       "      <td>singh</td>\n",
       "      <td>punjabi</td>\n",
       "      <td>सुनडर</td>\n",
       "      <td>सिंह</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>sonam</td>\n",
       "      <td>dadul</td>\n",
       "      <td>hindi</td>\n",
       "      <td>सोनाम</td>\n",
       "      <td>दादुल</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>nesa</td>\n",
       "      <td>mirza</td>\n",
       "      <td>urdu</td>\n",
       "      <td>नेसा</td>\n",
       "      <td>मीरज़ा</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>sony</td>\n",
       "      <td>sahu</td>\n",
       "      <td>oriya</td>\n",
       "      <td>सोनी</td>\n",
       "      <td>साहु</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>madhu</td>\n",
       "      <td>agarwal</td>\n",
       "      <td>hindi</td>\n",
       "      <td>मदहु</td>\n",
       "      <td>ागारवाल</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>raghbir</td>\n",
       "      <td>singh</td>\n",
       "      <td>punjabi</td>\n",
       "      <td>राघबिर</td>\n",
       "      <td>सिंह</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>jagdish</td>\n",
       "      <td>babu</td>\n",
       "      <td>telugu</td>\n",
       "      <td>जागदीश</td>\n",
       "      <td>बाबु</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>manthan</td>\n",
       "      <td>shah</td>\n",
       "      <td>gujarati</td>\n",
       "      <td>मानथान</td>\n",
       "      <td>शाह</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>praveen</td>\n",
       "      <td>shukla</td>\n",
       "      <td>hindi</td>\n",
       "      <td>प्रवीन</td>\n",
       "      <td>सहुकला</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>anukampa</td>\n",
       "      <td>rao</td>\n",
       "      <td>telugu</td>\n",
       "      <td>ानुकामपा</td>\n",
       "      <td>राओ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>sagar</td>\n",
       "      <td>kumawat</td>\n",
       "      <td>gujarati</td>\n",
       "      <td>सागार</td>\n",
       "      <td>कुमावात</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>pradeep</td>\n",
       "      <td>katiha</td>\n",
       "      <td>malayalam</td>\n",
       "      <td>प्रदीप</td>\n",
       "      <td>कातिहा</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>raj</td>\n",
       "      <td>sarin</td>\n",
       "      <td>telugu</td>\n",
       "      <td>राज</td>\n",
       "      <td>सारिन</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>taranj</td>\n",
       "      <td>singh</td>\n",
       "      <td>punjabi</td>\n",
       "      <td>तरानज</td>\n",
       "      <td>सिंह</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>m</td>\n",
       "      <td>mouli</td>\n",
       "      <td>telugu</td>\n",
       "      <td>NaN</td>\n",
       "      <td>मोुली</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>laxmi</td>\n",
       "      <td>mishra</td>\n",
       "      <td>hindi</td>\n",
       "      <td>लाक्समि</td>\n",
       "      <td>मिसहरा</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>arup</td>\n",
       "      <td>purkayastha</td>\n",
       "      <td>bengali</td>\n",
       "      <td>ारुप</td>\n",
       "      <td>पुरकायसथा</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>supriya</td>\n",
       "      <td>maydeo</td>\n",
       "      <td>telugu</td>\n",
       "      <td>सुपरिया</td>\n",
       "      <td>मेदेो</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>azam</td>\n",
       "      <td>shariff</td>\n",
       "      <td>urdu</td>\n",
       "      <td>ाज़ाम</td>\n",
       "      <td>शारीफफ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14969</th>\n",
       "      <td>madhu</td>\n",
       "      <td>singhania</td>\n",
       "      <td>hindi</td>\n",
       "      <td>मदहु</td>\n",
       "      <td>सिंहानिया</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14970</th>\n",
       "      <td>momaamed</td>\n",
       "      <td>ahmed</td>\n",
       "      <td>bengali</td>\n",
       "      <td>मोमाामेद</td>\n",
       "      <td>ाहमेद</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14971</th>\n",
       "      <td>rinku</td>\n",
       "      <td>bhiushan</td>\n",
       "      <td>gujarati</td>\n",
       "      <td>रिनकु</td>\n",
       "      <td>बहिुशान</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14972</th>\n",
       "      <td>rohit</td>\n",
       "      <td>aggarwal</td>\n",
       "      <td>hindi</td>\n",
       "      <td>रोहित</td>\n",
       "      <td>ागगारवाल</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14973</th>\n",
       "      <td>mohan</td>\n",
       "      <td>mohan</td>\n",
       "      <td>tamil</td>\n",
       "      <td>मोहन</td>\n",
       "      <td>मोहन</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14974</th>\n",
       "      <td>sanjoy</td>\n",
       "      <td>singh</td>\n",
       "      <td>punjabi</td>\n",
       "      <td>सानजॉय</td>\n",
       "      <td>सिंह</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14975</th>\n",
       "      <td>jaimin</td>\n",
       "      <td>shah</td>\n",
       "      <td>gujarati</td>\n",
       "      <td>जािमिन</td>\n",
       "      <td>शाह</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14976</th>\n",
       "      <td>junee</td>\n",
       "      <td>bal</td>\n",
       "      <td>punjabi</td>\n",
       "      <td>जुणी</td>\n",
       "      <td>बाल</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14977</th>\n",
       "      <td>shailendra</td>\n",
       "      <td>mishra</td>\n",
       "      <td>hindi</td>\n",
       "      <td>शािलेन्द्र</td>\n",
       "      <td>मिसहरा</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14978</th>\n",
       "      <td>ajeet</td>\n",
       "      <td>singh</td>\n",
       "      <td>english</td>\n",
       "      <td>ाजेेट</td>\n",
       "      <td>सिंह</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14979</th>\n",
       "      <td>lopendr</td>\n",
       "      <td>patel</td>\n",
       "      <td>gujarati</td>\n",
       "      <td>लोपेंडर</td>\n",
       "      <td>पटेल</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14980</th>\n",
       "      <td>ayesha</td>\n",
       "      <td>sayyed</td>\n",
       "      <td>urdu</td>\n",
       "      <td>ेेशा</td>\n",
       "      <td>सेयेद</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14981</th>\n",
       "      <td>pradip</td>\n",
       "      <td>paul</td>\n",
       "      <td>gujarati</td>\n",
       "      <td>प्रदीप</td>\n",
       "      <td>पौल</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14982</th>\n",
       "      <td>partha</td>\n",
       "      <td>paul</td>\n",
       "      <td>bengali</td>\n",
       "      <td>पार्था</td>\n",
       "      <td>पौल</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14983</th>\n",
       "      <td>noora</td>\n",
       "      <td>husain</td>\n",
       "      <td>urdu</td>\n",
       "      <td>नूरा</td>\n",
       "      <td>हुसािन</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14984</th>\n",
       "      <td>harmeet</td>\n",
       "      <td>singh</td>\n",
       "      <td>punjabi</td>\n",
       "      <td>हर्मीत</td>\n",
       "      <td>सिंह</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14985</th>\n",
       "      <td>bala</td>\n",
       "      <td>nair</td>\n",
       "      <td>malayalam</td>\n",
       "      <td>बाला</td>\n",
       "      <td>नािर</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14986</th>\n",
       "      <td>jai</td>\n",
       "      <td>sankar</td>\n",
       "      <td>telugu</td>\n",
       "      <td>जाि</td>\n",
       "      <td>सांकार</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14987</th>\n",
       "      <td>ehsan</td>\n",
       "      <td>ali</td>\n",
       "      <td>bengali</td>\n",
       "      <td>ेहसान</td>\n",
       "      <td>अली</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14988</th>\n",
       "      <td>prasun</td>\n",
       "      <td>ray</td>\n",
       "      <td>bengali</td>\n",
       "      <td>प्रसुन</td>\n",
       "      <td>राय</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14989</th>\n",
       "      <td>arun</td>\n",
       "      <td>pandey</td>\n",
       "      <td>hindi</td>\n",
       "      <td>ारुन</td>\n",
       "      <td>पांडी</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14990</th>\n",
       "      <td>shreya</td>\n",
       "      <td>singh</td>\n",
       "      <td>hindi</td>\n",
       "      <td>शरीा</td>\n",
       "      <td>सिंह</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14991</th>\n",
       "      <td>sagar</td>\n",
       "      <td>karwa</td>\n",
       "      <td>gujarati</td>\n",
       "      <td>सागार</td>\n",
       "      <td>करवा</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14992</th>\n",
       "      <td>kaushik</td>\n",
       "      <td>chowdhari</td>\n",
       "      <td>gujarati</td>\n",
       "      <td>कौशिक</td>\n",
       "      <td>चोवधारी</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14993</th>\n",
       "      <td>dihanta</td>\n",
       "      <td>bhattacharjee</td>\n",
       "      <td>bengali</td>\n",
       "      <td>दीहंता</td>\n",
       "      <td>भट्टाचारजी</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14994</th>\n",
       "      <td>mukesh</td>\n",
       "      <td>arora</td>\n",
       "      <td>punjabi</td>\n",
       "      <td>मुकेश</td>\n",
       "      <td>ारोरा</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14995</th>\n",
       "      <td>ankit</td>\n",
       "      <td>more</td>\n",
       "      <td>marathi</td>\n",
       "      <td>ांकित</td>\n",
       "      <td>मोरे</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14996</th>\n",
       "      <td>shivaji</td>\n",
       "      <td>gaikwad</td>\n",
       "      <td>marathi</td>\n",
       "      <td>सहिवाजी</td>\n",
       "      <td>गािकवाद</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14997</th>\n",
       "      <td>anil</td>\n",
       "      <td>singh</td>\n",
       "      <td>punjabi</td>\n",
       "      <td>ानील</td>\n",
       "      <td>सिंह</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14998</th>\n",
       "      <td>yajid</td>\n",
       "      <td>sajeed</td>\n",
       "      <td>english</td>\n",
       "      <td>याजिद</td>\n",
       "      <td>साजीद</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14999 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Name        Surname  Community  hindi_Name hindi_Surname\n",
       "0          yogita          singh    punjabi      योगिता          सिंह\n",
       "1        dibyendu          ghosh    bengali    दीबयेनदु          गहोश\n",
       "2       veeramony   ramachandran      tamil    वीरामोनी   रामाचानद्रन\n",
       "3           amlan          datta    bengali       ामलान         दट्टा\n",
       "4         susmita         poddar    bengali     सुसमिता        पोददार\n",
       "5            raja          banik    bengali        राजा         बानिक\n",
       "6           rekha           jain      hindi        रेखा          जािन\n",
       "7         subrato            roy    bengali      सुबरतो           रॉय\n",
       "8          dinesh         tiwari      hindi       दीनेश        तिवारी\n",
       "9          shampa    chakraborty    bengali       शामपा   चाकराबोर्टय\n",
       "10           nitu         drolia      hindi        नितु       दरोलिया\n",
       "11         sunder          singh    punjabi       सुनडर          सिंह\n",
       "12          sonam          dadul      hindi       सोनाम         दादुल\n",
       "13           nesa          mirza       urdu        नेसा        मीरज़ा\n",
       "14           sony           sahu      oriya        सोनी          साहु\n",
       "15          madhu        agarwal      hindi        मदहु       ागारवाल\n",
       "16        raghbir          singh    punjabi      राघबिर          सिंह\n",
       "17        jagdish           babu     telugu      जागदीश          बाबु\n",
       "18        manthan           shah   gujarati      मानथान           शाह\n",
       "19        praveen         shukla      hindi      प्रवीन        सहुकला\n",
       "20       anukampa            rao     telugu    ानुकामपा           राओ\n",
       "21          sagar        kumawat   gujarati       सागार       कुमावात\n",
       "22        pradeep         katiha  malayalam      प्रदीप        कातिहा\n",
       "23            raj          sarin     telugu         राज         सारिन\n",
       "24         taranj          singh    punjabi       तरानज          सिंह\n",
       "25              m          mouli     telugu         NaN         मोुली\n",
       "26          laxmi         mishra      hindi     लाक्समि        मिसहरा\n",
       "27           arup    purkayastha    bengali        ारुप     पुरकायसथा\n",
       "28        supriya         maydeo     telugu     सुपरिया         मेदेो\n",
       "29           azam        shariff       urdu       ाज़ाम        शारीफफ\n",
       "...           ...            ...        ...         ...           ...\n",
       "14969       madhu      singhania      hindi        मदहु     सिंहानिया\n",
       "14970    momaamed          ahmed    bengali    मोमाामेद         ाहमेद\n",
       "14971       rinku       bhiushan   gujarati       रिनकु       बहिुशान\n",
       "14972       rohit       aggarwal      hindi       रोहित      ागगारवाल\n",
       "14973       mohan          mohan      tamil        मोहन          मोहन\n",
       "14974      sanjoy          singh    punjabi      सानजॉय          सिंह\n",
       "14975      jaimin           shah   gujarati      जािमिन           शाह\n",
       "14976       junee            bal    punjabi        जुणी           बाल\n",
       "14977  shailendra         mishra      hindi  शािलेन्द्र        मिसहरा\n",
       "14978       ajeet          singh    english       ाजेेट          सिंह\n",
       "14979     lopendr          patel   gujarati     लोपेंडर          पटेल\n",
       "14980      ayesha         sayyed       urdu        ेेशा         सेयेद\n",
       "14981      pradip           paul   gujarati      प्रदीप           पौल\n",
       "14982      partha           paul    bengali      पार्था           पौल\n",
       "14983       noora         husain       urdu        नूरा        हुसािन\n",
       "14984     harmeet          singh    punjabi      हर्मीत          सिंह\n",
       "14985        bala           nair  malayalam        बाला          नािर\n",
       "14986         jai         sankar     telugu         जाि        सांकार\n",
       "14987       ehsan            ali    bengali       ेहसान           अली\n",
       "14988      prasun            ray    bengali      प्रसुन           राय\n",
       "14989        arun         pandey      hindi        ारुन         पांडी\n",
       "14990      shreya          singh      hindi        शरीा          सिंह\n",
       "14991       sagar          karwa   gujarati       सागार          करवा\n",
       "14992     kaushik      chowdhari   gujarati       कौशिक       चोवधारी\n",
       "14993     dihanta  bhattacharjee    bengali      दीहंता    भट्टाचारजी\n",
       "14994      mukesh          arora    punjabi       मुकेश         ारोरा\n",
       "14995       ankit           more    marathi       ांकित          मोरे\n",
       "14996     shivaji        gaikwad    marathi     सहिवाजी       गािकवाद\n",
       "14997        anil          singh    punjabi        ानील          सिंह\n",
       "14998       yajid         sajeed    english       याजिद         साजीद\n",
       "\n",
       "[14999 rows x 5 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('df.pickle', 'wb') as handle:\n",
    "    pickle.dump(df.to_dict(), handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or reload preprocessed dict from saved pickle\n",
    "import pickle\n",
    "with open('df.pickle', 'rb') as handle:\n",
    "    df = pd.DataFrame(pickle.load(handle))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring name bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams(words, n):\n",
    "    return [word[i:i+n]  for word in words for i in range(len(word)-n+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask_indices(mask):\n",
    "    return np.argwhere(mask).reshape(1,-1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstname_bigrams = get_ngrams(df['Name'][firstnames_mask], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "hindi_firstname_bigrams = get_ngrams(df['hindi_Name'][firstnames_mask], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sh    3661\n",
       "an    3638\n",
       "ra    2906\n",
       "ha    2417\n",
       "ar    1801\n",
       "ma    1518\n",
       "na    1375\n",
       "it    1350\n",
       "ee    1333\n",
       "as    1172\n",
       "am    1165\n",
       "nd    1157\n",
       "in    1150\n",
       "sa    1137\n",
       "hi    1114\n",
       "ta    1112\n",
       "ka    1087\n",
       "ni    1069\n",
       "al    1001\n",
       "vi     989\n",
       "es     986\n",
       "aj     901\n",
       "su     899\n",
       "ri     886\n",
       "ja     842\n",
       "is     810\n",
       "pr     796\n",
       "ya     769\n",
       "en     757\n",
       "de     744\n",
       "      ... \n",
       "wk       1\n",
       "jl       1\n",
       "zs       1\n",
       "jb       1\n",
       "kn       1\n",
       "wq       1\n",
       "xa       1\n",
       "vg       1\n",
       "hg       1\n",
       "cq       1\n",
       "fs       1\n",
       "dl       1\n",
       "hz       1\n",
       "vm       1\n",
       "vc       1\n",
       "mz       1\n",
       "cs       1\n",
       "eu       1\n",
       "bw       1\n",
       "qs       1\n",
       "gc       1\n",
       "cv       1\n",
       "fz       1\n",
       "rq       1\n",
       "gp       1\n",
       "ci       1\n",
       "gg       1\n",
       "uu       1\n",
       "vp       1\n",
       "zy       1\n",
       "Length: 484, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(firstname_bigrams).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ान    2527\n",
       "रा    1770\n",
       "ार    1546\n",
       "ित    1301\n",
       "ना    1225\n",
       "मा    1219\n",
       "वि    1046\n",
       "ाम    1033\n",
       "िन     995\n",
       "्र     967\n",
       "सा     964\n",
       "ाज     946\n",
       "ेश     919\n",
       "सु     899\n",
       "ाल     877\n",
       "का     863\n",
       "नि     832\n",
       "ता     815\n",
       "हि     809\n",
       "रि     773\n",
       "सह     710\n",
       "पा     661\n",
       "शा     646\n",
       "या     622\n",
       "दी     593\n",
       "ास     590\n",
       "िक     556\n",
       "वा     554\n",
       "ेन     546\n",
       "मि     519\n",
       "      ... \n",
       "फम       1\n",
       "फअ       1\n",
       "फट       1\n",
       "शग       1\n",
       "ईब       1\n",
       "टप       1\n",
       "ूफ       1\n",
       "भौ       1\n",
       "हप       1\n",
       "शु       1\n",
       "बं       1\n",
       "टक       1\n",
       "फश       1\n",
       "बअ       1\n",
       "चु       1\n",
       "कप       1\n",
       "यब       1\n",
       "तब       1\n",
       "लं       1\n",
       "लौ       1\n",
       "जै       1\n",
       "षर       1\n",
       "शू       1\n",
       "ीी       1\n",
       "ईर       1\n",
       "थप       1\n",
       "ॉम       1\n",
       "णी       1\n",
       "हथ       1\n",
       "थक       1\n",
       "Length: 872, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(hindi_firstname_bigrams).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY8AAAEMCAYAAAA8vjqRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzt3Xl8nWWZ//HPlZP1JE2aJqVNF5puQBfWhE1ARVGLy4COMjC/YRmXjjMw4/YbFkXlp6KMos6oDMIoo4wzqCMgtFZLRRZZCk1o6V6a7nuattl6suf6/XGelNOS5eQ0JyfL9/16Pa+ccz/3dZ4rfUGu1/3cz7lvc3dERET6Iy3VCYiIyPCj4iEiIv2m4iEiIv2m4iEiIv2m4iEiIv2m4iEiIv2m4iEiIv2m4iEiIv2m4iEiIv2WnuoEkqW4uNhLS0sTim1qaiInJyfhayte8YpX/HCMr6ysrHH38XF1dvcReZSVlXmiKioqEo5VvOIVr/jhGg9UeJx/Y3XbSkRE+k3FQ0RE+k3FQ0RE+k3FQ0RE+k3FQ0RE+k3FQ0RE+k3FQ0RE+m3EfkkwEWv31PHtpZvIbm/kwbJUZyMiMnQlbeRhZlPN7Bkz22Bm68zsM0H7XWa2x8xWBcf7Y2LuMLMqM9tkZu+LaV8QtFWZ2e3JyjmUZjz/xkFWV7cm6xIiIiNCMkce7cAX3P01MxsDVJrZsuDc99393tjOZjYXuBaYB0wC/mhmpwWn7wPeA+wGVpjZk+6+fqATnjk+j8xQGvsbO2hsaScvSwMzEZHuJG3k4e773P214HUDsAGY3EvIVcAv3b3F3bcBVcAFwVHl7lvdvRX4ZdB3wGWmpzF7Qh4AG/fVJ+MSIiIjgkWXM0nyRcxKgeeB+cDngZuAeqCC6OjkiJn9CFju7r8IYn4K/D74iAXu/smg/XrgQne/pZvrLAQWApSUlJQtWrSo37net6KOP21v4pPnjuHKWbn9jgeIRCKEw+GEYhWveMUrPlXx5eXlle5eHlfneBfBSvQA8oBK4CPB+wlAiOio527goaD9PuBvYuJ+Cvwl8DHgJzHt1wM/7Ou6iS6M+NALW33abYv9tt+8nlC8+/BeGE3xilf86I2nHwsjJvWmvpllAI8C/+3ujwXF6kDM+f8AFgdvdwNTY8KnAHuD1z21D7i5JfkArNdtKxGRHiXzaSsjOnrY4O7fi2kvien2YWBt8PpJ4FozyzKz6cBs4FVgBTDbzKabWSbRSfUnk5X3nEnR4rFxfwPtHZ3JuoyIyLCWzJHHJURvMa0xs1VB2xeB68zsHMCB7cDfAbj7OjP7NbCe6JNaN7t7B4CZ3QIsJXq76yF3X5espPOzM5iQG+LA0Q621hzltAljknUpEZFhK2nFw91fAKybU0t6ibmb6DzIie1LeosbaKVj0zlwtIP1e+tVPEREuqHlSboxfWwGAOv21qU4ExGRoUnFoxulY6MDMk2ai4h0T8WjG10jj/V767seDxYRkRgqHt0oykljbDiDI5E29tc3pzodEZEhR8WjG2b25vc99urWlYjIiVQ8eqDiISLSMxWPHsydpG+ai4j0RMWjByoeIiI9U/HoQdfeHjsORWhobkt1OiIiQ4qKRw8yQmmcNjHY22N/Q4qzEREZWlQ8eqFJcxGR7ql49ELFQ0SkeyoevZg3uQDQpLmIyIlUPHpxxsToirqbDjTQpr09RESOUfHoxZjsDKYVhWlt72TrwaOpTkdEZMhQ8ehD17yHlmcXEXmTikcfNGkuIvJWKh590DfNRUTeSsWjD7HFQ3t7iIhEJa14mNlUM3vGzDaY2Toz+0zQ/h0z22hmq83scTMbG7SXmlmTma0Kjh/HfFaZma0xsyoz+4GZdbc3elJMzM+mMJxBbaSNfXXa20NEBJI78mgHvuDuc4CLgJvNbC6wDJjv7mcBbwB3xMRscfdzguPTMe33AwuB2cGxIIl5H8fM3hx9aN5DRARIYvFw933u/lrwugHYAEx296fcvT3othyY0tvnmFkJkO/uL3v0vtHDwNXJyrs7xybNNe8hIgKADcZ9fDMrBZ4nOuKoj2lfBPzK3X8R9FlHdDRSD9zp7n82s3LgHne/Ioi5DLjN3T/YzXUWEh2hUFJSUrZo0aKE8o1EIoTD4WPvn9vRxA9erePCyVnc+rbCfsef7PUVr3jFK34w4svLyyvdvTyuzu6e1APIAyqBj5zQ/iXgcd4sYFlAUfC6DNgF5APnA3+MibsMWNTXdcvKyjxRFRUVx73fuK/ep9222C/7lz8lFH+y11e84hWv+MGIByo8zr/t6QmVpziZWQbwKPDf7v5YTPuNwAeBdwcJ4+4tQEvwutLMtgCnAbs5/tbWFGBvMvM+0YzxuWSmp7HzcIT65jbyszMG8/IiIkNOMp+2MuCnwAZ3/15M+wLgNuAv3D0S0z7ezELB6xlEJ8a3uvs+oMHMLgo+8wbgiWTl3Z2MUBqnT4iuc7Vxn/b2EBFJ5tNWlwDXA++Kefz2/cCPgDHAshMeyX07sNrMXgd+A3za3Q8H5/4e+AlQBWwBfp/EvLv15jfNtUyJiEjSblu5+wtAd9/HWNJD/0eJ3uLq7lwFMH/gsuu/eZPzoUJPXImIgL5hHjc9risi8iYVjzidERSPN/Y3am8PERn1VDzilJeVTmlRmNaOTqqqG1OdjohISql49IOWKRERiVLx6AfNe4iIRKl49INGHiIiUSoe/TC3pADQ3h4iIioe/TAhP4txuZnUNbWxV3t7iMgopuLRD2amPc1FRFDx6DfNe4iIqHj025tPXGmNKxEZvVQ8+unYyEOP64rIKKbi0U8ziqN7e+w63ERdU1uq0xERSQkVj35KD6VxxsSuvT00+hCR0UnFIwHzdOtKREY5FY8E6HFdERntVDwS0DVpvk7FQ0RGKRWPBJw+MR8z2FzdQGu79vYQkdFHxSMB0b09cmnrcO3tISKjUtKKh5lNNbNnzGyDma0zs88E7ePMbJmZbQ5+FgbtZmY/MLMqM1ttZufFfNaNQf/NZnZjsnLuDy3PLiKjWTJHHu3AF9x9DnARcLOZzQVuB55299nA08F7gCuB2cGxELgfosUG+CpwIXAB8NWugpNKWqZEREazpBUPd9/n7q8FrxuADcBk4Crg50G3nwNXB6+vAh72qOXAWDMrAd4HLHP3w+5+BFgGLEhW3vHSMiUiMpoNypyHmZUC5wKvABPcfR9ECwxwStBtMrArJmx30NZTe0rFjjy0t4eIjDaW7D98ZpYHPAfc7e6PmVmtu4+NOX/E3QvN7HfAt9z9haD9aeBW4F1Alrt/I2j/MhBx9+92c62FRG95UVJSUrZo0aKEco5EIoTD4V77uDsfX3SQ+pZO7n//eE7JDfUr/mSvr3jFK17xAx1fXl5e6e7lcXV296QdQAawFPh8TNsmoCR4XQJsCl4/AFx3Yj/gOuCBmPbj+vV0lJWVeaIqKiri6vc3P1nu025b7EvX7kso/mSvr3jFK17xAxkPVHicf9+T+bSVAT8FNrj792JOPQl0PTF1I/BETPsNwVNXFwF1Hr2ttRR4r5kVBhPl7w3aUk5PXInIaJWexM++BLgeWGNmq4K2LwL3AL82s08AO4GPBeeWAO8HqoAI8LcA7n7YzL4OrAj6fc3dDycx77jpiSsRGa2SVjw8OndhPZx+dzf9Hbi5h896CHho4LIbGBp5iMhopW+Yn4Tpxblkpaex+4j29hCR0UXF4ySkh9I4Ixh9bNDoQ0RGERWPk9R160or7IrIaKLicZI0aS4io5GKx0nSpLmIjEYqHifpjIljMIMq7e0hIqOIisdJys1KZ3qwt8fm6oZUpyMiMihUPAbAHM17iMgoo+IxADTvISKjjYrHANATVyIy2qh4DIB5MSMP194eIjIKqHgMgPFjsijOy6ShuZ3dR5pSnY6ISNKpeAwAM2OO5j1EZBRR8RggmvcQkdFExWOA6IkrERlNVDwGyDyNPERkFFHxGCDTi/PIzkhjT20Tja1apkRERjYVjwESSjPOmBgdfWyr1cZQIjKyqXgMoK5J8+217SnOREQkufpdPMys0MzOiqPfQ2ZWbWZrY9p+ZWargmO7ma0K2kvNrCnm3I9jYsrMbI2ZVZnZD8ysp33RU65r0ny7Rh4iMsLFVTzM7FkzyzezccDrwH+a2ff6CPsZsCC2wd3/yt3PcfdzgEeBx2JOb+k65+6fjmm/H1gIzA6O4z5zKOmaNF+1v1V7movIiBbvyKPA3euBjwD/6e5lwBW9Bbj788Dh7s4Fo4drgEd6+wwzKwHy3f1lj6778TBwdZw5D7qzp4ylbFohtS2d/MsfNqY6HRGRpIm3eKQHf8ivARYPwHUvAw64++aYtulmttLMnjOzy4K2ycDumD67g7YhKS3N+NZHziTd4H9e2cmK7d3WThGRYc/iWcjPzD4KfAV4wd3/wcxmAN9x97/sI64UWOzu809ovx+ocvfvBu+zgDx3P2RmZcBvgXnA6cC33P2KoN9lwK3u/qEerreQ6C0uSkpKyhYtWtTn79adSCRCOBxOKBbgv1Ye5rdVrUweE+K77ykmI9S/aZqTvb7iFa94xSeivLy80t3L4+rs7n0ewCXxtHXTpxRYe0JbOnAAmNJL3LNAOVACbIxpvw54IJ6cy8rKPFEVFRUJx7q7v/TKCr/83md82m2L/fvLNg369RWveMUrPhFAhcfx99Xd475t9cM42+JxRVAQjt2OMrPxZhYKXs8gOjG+1d33AQ1mdlEwT3ID8ESC1x00mSHjWx8+E4B/f2YLVdqeVkRGmF6Lh5ldbGZfAMab2edjjruAUB+xjwAvA6eb2W4z+0Rw6lreOlH+dmC1mb0O/Ab4tLt3TRj8PfAToArYAvw+/l8vdS6cUcR1F0yltaOTOx5bQ2en9vkQkZEjvY/zmUBe0G9MTHs98NHeAt39uh7ab+qm7VGij+52178CmN/duaHu9gVzWLa+mhXbj/DIip38nwunpTolEZEB0WvxcPfngOfM7GfuvmOQchoxCsIZ3PUXc7nlf1Zyz5KNXDFnAhPys1OdlojISYt3ziPLzB40s6fM7E9dR1IzGyE+cGYJ7z7jFBpa2rnryXWpTkdEZEDEWzz+F1gJ3An8c8whfTAzvnb1fHIzQ/x+7X6eWrc/1SmJiJy0eItHu7vf7+6vuntl15HUzEaQyWNz+L/vOx2ArzyxjoZmLV0iIsNbvMVjkZn9g5mVmNm4riOpmY0wN1xcytlTx7K/vpl7l25KdToiIicl3uJxI9HbVC8BlcFRkaykRqJQmnHPR84kPc14ePkOXtt5JNUpiYgkLK7i4e7TuzlmJDu5kWZOST6fevsM3OGOR9fQ2q4dB0VkeOrrex4AmNkN3bW7+8MDm87I95l3z2bJmn1sOtDAf/x5KzdfPivVKYmI9Fu8t63OjzkuA+4C/iJJOY1o2RkhvhksXfJvT29m68HGFGckItJ/8d62+seY41PAuUS/fS4JuGRWMX953hRa2zv54uNruhZ9FBEZNhLdwzxCdPFCSdCdH5jDuNxMlm89zP9W7O47QERkCIl3G9pFZvZkcPwO2MQwWN12KCvMzeQrH5wLwN1LNnCwoSXFGYmIxC+uCXPg3pjX7cCO2CXVJTFXnTOJx1bu4fk3DvK1xev54XXnpjolEZG4xDvn8RywkejKuoVAazKTGi3MjLuvnk9ORohFr+/lmY3VqU5JRCQu8d62ugZ4FfgY0X3MXwm2ppWTNHVcmM+/5zQA7vztWo62tKc4IxGRvsU7Yf4l4Hx3v9HdbwAuAL6cvLRGl7+9pJT5k/PZU9vE95a9kep0RET6FG/xSHP32Hsqh/oRK31ID6Vxz0fOIs3gP1/cRtVhLZwoIkNbvAXgD2a21MxuMrObgN8BS5KX1ugzf3IBn7h0Op0OP66so0Pb1orIENbXHuazzOwSd/9n4AHgLOBsonuTPzgI+Y0qn3vPaUwqyGZbbTu/rtiV6nRERHrU18jjX4EGAHd/zN0/7+6fIzrq+NfeAs3sITOrNrO1MW13mdkeM1sVHO+POXeHmVWZ2SYze19M+4KgrcrMbk/klxwuwpnp3P7+OQDcu3QTdU26fSUiQ1NfxaPU3Vef2OjuFUBpH7E/AxZ00/59dz8nOJYAmNlc4FpgXhDz72YWMrMQcB9wJTAXuC7oO2J96KwS5hRncOhoKz94enOq0xER6VZfxSO7l3M5vQW6+/PA4TjzuAr4pbu3uPs2oIroE10XAFXuvtXdW4FfBn1HLDPj4+fkYwY/f2k7VdVaOFFEhp6+iscKM/vUiY1m9gmiG0Il4hYzWx3c1ioM2iYDsTf5dwdtPbWPaDMKM7j2/Km0dzpfX7xeCyeKyJBjvf1hMrMJwONEv1HeVSzKia6o+2F339/rh5uVAovdfX7M59UADnwdKHH3j5vZfcDL7v6LoN9Pic6rpAHvc/dPBu3XAxe4+z/2cL2FwEKAkpKSskWLFvX1+3crEokQDocTih2o+La0LG75Qw2RNueLl46lrKS3QeDAX1/xilf86IsvLy+vdPfyuDq7e58HcDnwj8HxrnhigrhSYG1f54A7gDtizi0FLg6OpTHtx/Xr7SgrK/NEVVRUJBw7kPH/8fwWn3bbYn/nd57xlraOQb++4hWv+NEVD1R4nH/f413b6hl3/2Fw/CmuqtQNMyuJefthoOtJrCeBa80sy8ymE13u/VVgBTDbzKabWSbRSfUnE73+cHPj20qZOT6XbTVH+dlL21KdjojIMUn7lriZPUL0+yCnm9nuYJ7k22a2xsxWEx3NfA7A3dcBvwbWA38Abnb3DndvB24hOhLZAPw66DsqZITS+HKwbPsPnq6iuqE5xRmJiETFuyR7v7n7dd00/7SX/ncDd3fTvoRR/G32d55+Cu8+4xSe3ljNvUs38e2Pnp3qlEREtD7VcHDnB+eSETL+t3I3q3fXpjodEREVj+FgenEuH79kOu5w15Pr9OiuiKSciscwccu7ZlGcl8VrO2t5YtXeVKcjIqOciscwMSY7g1sXnA7At36/QZtGiUhKqXgMIx89bwpnTSngQH0L9z+7JdXpiMgopuIxjKSlGV/90DwAHvzzVnYeiqQ4IxEZrVQ8hpmyaYVcfc4kWts7+eaSDalOR0RGKRWPYej2K+eQkxHiD+v281JVTarTEZFRSMVjGJpYkM3Nl88E4P8tWk97R2eKMxKR0UbFY5j65GUzmFKYw6YDDfzPqztTnY6IjDIqHsNUdkaIOz8Q3bL2u0+9wZGjrSnOSERGExWPYex98ybytplF1DW18f0/vpHqdERkFFHxGMbMjK98aC5pBr9YvoON++tTnZKIjBIqHsPcGRPz+ZuLptHp8LVF2rJWRAaHiscI8LkrTqMgJ4OXthxi6boDqU5HREYBFY8RoDA3ky+89zQA7l6yntYOjT5EJLlUPEaIv77gVE6fMIZdh5t4ZG0DHZ0qICKSPCoeI0R6KI2vfii6Ze2Tb0RY8K/P89S6/ZoDEZGkUPEYQd42q5j7/vo8TgmH2FzdyML/quSjP36ZV7cdTnVqIjLCJK14mNlDZlZtZmtj2r5jZhvNbLWZPW5mY4P2UjNrMrNVwfHjmJgyM1tjZlVm9gMzs2TlPBJ84KwSfrCgmLs+NJei3Ewqdxzhmgde5uM/W6FHeUVkwCRz5PEzYMEJbcuA+e5+FvAGcEfMuS3ufk5wfDqm/X5gITA7OE78TDlBRsi46ZLpPHfr5Xz2itnkZob408Zqrvy3P/P5X61i12Et5S4iJydpxcPdnwcOn9D2lLt3bYG3HJjS22eYWQmQ7+4ve/Tm/cPA1cnIdyTKy0rns1ecxnO3Xs5NbyslPc14bOUe3vXdZ7nryXUcamxJdYoiMkylcs7j48DvY95PN7OVZvacmV0WtE0Gdsf02R20ST8U52Vx11/M409feCcfPncy7Z3Oz17aztu//Qz/+sc3aNSWtiLST5bMp3HMrBRY7O7zT2j/ElAOfMTd3cyygDx3P2RmZcBvgXnA6cC33P2KIO4y4FZ3/1AP11tI9BYXJSUlZYsWLUoo70gkQjgcTih2OMRvr23jv9c08tr+6MgjPyuNj83J5T0zw2Sk2ZDPX/GKV3xy4svLyyvdvTyuzu6etAMoBdae0HYj8DIQ7iXuWaLFpQTYGNN+HfBAPNcuKyvzRFVUVCQcO5zil2+p8avve8Gn3bbYp9222C/9l6f98dd2+ysrVgzK9RWveMUPrXigwuP8+56eUHlKkJktAG4D3uHukZj28cBhd+8wsxlEJ8a3uvthM2sws4uAV4AbgB8OZs4j2YUzinjs79/GU+sP8J2lm6iqbuSzv1oFQM4TfyCcGSI7I0Q4M3rkZIbIyQgRzkwnp6stI/Tm68x0whkhCpo6UvybiUiyJa14mNkjwDuBYjPbDXyV6NNVWcCy4Inb5R59surtwNfMrB3oAD7t7l2T7X9P9MmtHKJzJLHzJHKSzIz3zZvIFXMm8Ohru/nRn6rYeThCU1sHTW2JFYG8TGPJnAinFiU+9BaRoS1pxcPdr+um+ac99H0UeLSHcxXA/O7OycAJpRnXlE/lmvKprKioYO6Z5xBp7aCpNVpEIq3tNLV2RNvaOoLX7USC102tHUTaOlizu441e+r45MMreOwfLiEva1AHtyIySPR/trxFmhm5WenkJvCHv6G5jSu/+zRvHGjks79cxYPXl5GWpu91iow0Wp5EBtSY7Axuv7SQgpwM/rjhAN9dtinVKYlIEqh4yIAryUvnvr8+j1Cacd8zW3hi1Z5UpyQiA0zFQ5Li0tnFfPkDcwC49TerWb27NsUZichAUvGQpLnxbaVce/5UWto7WfhwJdX1zalOSUQGiIqHJI2Z8bWr5nN+aSH765tZ+F+VNCf4+K+IDC0qHpJUmelp3P83ZUwem8OqXbV88fE12qBKZARQ8ZCkK87L4sEbysjJCPHYa3v4yZ+3pTolETlJKh4yKOZNKuB715wNwLd+v4FnNlWnOCMRORkqHjJorjyzhM9eMZtOh3/6n5VUVTemOiURSZCKhwyqf3rXbK6cP5GGlnY+9XAFdZG2VKckIglQ8ZBBlZZmfPeas5lTks+2mqPc8shrtHd0pjotEeknFQ8ZdOHMdP7jhjKKcjP58+YavrlkY6pTEpF+UvGQlJhSGObH15eRETIeenEbv16xK9UpiUg/qHhIypxfOo5vXB1dbf9Lv11DxfbDfUSIyFCh4iEp9Vfnn8pNbyulrcP59C8q2VPblOqURCQOKh6Scnd+YA6XziqmprGVT/28guZ2TaCLDHXaDEpSLj2Uxo/++lyuvu9F1u+r5wtPHeWM9a9SnJdFcV4W48dkUZyXyfgxWYwP2saGMwi2MhaRFFDxkCFhbDiTn9xYzjUPLGf/0Vb2bzrYa/+MkFGU+2ZhebPIZEF9C/PaOsjOCA1S9iKjj4qHDBmzThnD87dezu+er6B4ynQONrRQ09hCTWMrBxtaONjYQk3ws6G5nf31zezvYZn3e156ivNLC7lkVjGXzRrP3En5hLQdrsiASWrxMLOHgA8C1e4+P2gbB/wKKAW2A9e4+xGL3oP4N+D9QAS4yd1fC2JuBO4MPvYb7v7zZOYtqZOXlc6scRmUzZnQa7/mto7jCktNYwsHG1qobmjmpY172VbXzotVh3ix6hDfZhNjwxlcMrOYS2YVc+msYk4tCg/SbyQyMiV75PEz4EfAwzFttwNPu/s9ZnZ78P424EpgdnBcCNwPXBgUm68C5YADlWb2pLsfSXLuMoRlZ4SYUhhmSuFbi0BlZSvTzziTl7bU8MLmGv68uYY9tU38bs0+frdmHwCnjgtHRyWzi7l4RhGFuZmD/SuIDGtJLR7u/ryZlZ7QfBXwzuD1z4FniRaPq4CHPbrZw3IzG2tmJUHfZe5+GMDMlgELgEeSmbsMb+NyM/ngWZP44FmTcHd2HIrwQlUNLwbHzsMRdr66k0de3YkZzJ9UwKWzo6MS69B+IyJ9sWRvzBMUj8Uxt61q3X1szPkj7l5oZouBe9z9haD9aaJF5Z1Atrt/I2j/MtDk7vd2c62FwEKAkpKSskWLFiWUcyQSIRxO/LaG4od2fIc724608/qBFlYfaGXjoVZinw5OM5iSn87MwgxmjE1nRmEGpWPTyU6P78n2of77K17xPSkvL6909/J4+g6lCfPuZjO9l/a3Nro/CDwIUF5e7mVlZQklUllZSaKxih8e8RcAfxW8jrS2s2L7EV6sit7m2rivnp117eysa+eZoE+awczxecyfXMC8SfmcObmAuZPyGZOdkZL8Fa/4ZMXHKxXF44CZlbj7vuC2VNeuQLuBqTH9pgB7g/Z3ntD+7CDkKaNEODOdd5w2nnecNh6Al16pIGviTNbtrWPN7jrW7q1n84EGNlc3srm6kcdX7jkWO6M4l3mTCzhzcj7zJxUwb1JBqn4NkUGViuLxJHAjcE/w84mY9lvM7JdEJ8zrggKzFPimmRUG/d4L3DHIOcsokpVulE0rpGxa4bG25rYONu1vYO3eOtbuqWPtnno27W9ga81RttYcZdHre4/1Dacb2UuWkZ5mZITSyAgZ6aG0Y68zQmlvOZcZSiM9ZKSnpdHSUM/q5m1MLQwzZVwOUwrD5GUNpZsEIsl/VPcRoqOGYjPbTfSpqXuAX5vZJ4CdwMeC7kuIPqZbRfRR3b8FcPfDZvZ1YEXQ72tdk+cigyU7I8TZU8dy9tRj03W0tnfyxoGGaDHZW8eaPfVs2FdPpL2TSHvrSV1v8eb1x70vDGcwpTDM1KCYTC3MCZ42i/7MydQXImVwJftpq+t6OPXubvo6cHMPn/MQ8NAApiZy0jLT05g/uYD5k9+8VdXe0clLr1Yy78yzaOtw2jo6aevopL2z67XT3tFJa0cn7R1vtkX7dNLW7qx+YyseHseuI03sPhJh95EmjkTaOBKpY82eum5zKc7LPFZMstsaOJi1j3mTCphSmKNlXCQpNBYWGUDpoTRyM9MoystK+DNmplVTVnbmsfednU5NY8txxWT3kQi7Dkd/7qltoqaxlZrGVlbtqgXgNxteAyA/O525k/KZWxKd6J83OZ+Z4/PICGlNVDk5Kh4iQ1xamnFKfja3tnzDAAAK20lEQVSn5GcfNw/TpaPTqW5oZveRJnYdjvD861Uc9jDr99ZR09jK8q2HWb71zTu9melpnD5hDHNLosVk3qR8zpiYT67mVaQf9F+LyDAXSjNKCnIoKcjh/NJxTPMDlJWV4e5UN7Swfm896/bWsW5vPev31bPjUIQ1e4JbYBXRzzCD6UW50cePOxrZm76X0qJcphWHye/mcWQRFQ+REcrMmJCfzYT8bC4/45Rj7fXNbWwICsm6vdFj84E3nxwDeGTdymP9x+VmMq0oHC0mJ/zU0vijl4qHyCiTn53BhTOKuHBG0bG2lvYONh9oZP3eel5cu4Xm9Dx2HIqw/dBRDh9t5fDRVlburO3ms9IpLc5lWlEupUVhphXl0lLTyoyjrVovbIRT8RARstJDx54ci07YR7+h3HXra3vN0WPFpOvn9pqj1De3s3p3Hat3H/8U2JeeWca43Exmjs9lRnEeM0/p+pnH1MIc0jVhP+ypeIhIj2JvfcWOVCBaWA4dbWXHoaNsr4mw49BRth2KsG5HNfsjfmzEsmL78QtgZ4SMaUW5zByfy8zxecwYnxctMuPzKMjR/MpwoeIhIgkxs2NbBZdNG3esvbKykvPOO4/99c1sqT7K1ppGtlQ3srXmKFuqG9lb10xVdSNV1Y3AgeM+szgvi6KsTmZsqOSUMVnRp8xif47JojCcSZo29ko5FQ8RGXBmbz4Bduns4uPORVrb2XrwKFsONh73c2tNY7DBF2w6tL/Hz84IGePzshgfU1AmHCsyWTTUt3Nup6vAJJmKh4gMqnBm+lu+mQ/RL0PurWviT6+8TmHJNA7UNwe7Q0Z3iKyub+FAfTP1ze3srWtmb133WxAD3PncU5wzdSznnlrIuaeO5ZwpYzWBP8BUPERkSEhLM6YUhpk3PpOysyf12K+5rePYlsNdBaU6KDIH6ptZv/swh5ra+XOwi2SX6cW5nDt1LOeeGi0qp08co2/anwQVDxEZVrIzQkwdF2bquO43PKqsrGTyrHms2nWElTtrWbmzltV7atlWc5RtNUd5LFhSPzsjjTMnF0RHJ8EoZWJB9mD+KsOaioeIjDgTC7JZUFDCgvklALR1dLJpfwMrdwYFZVe0mKzYfuS4p8FKCrKZHHbm7V7L5MIcJo3NYfLYHCYX5lCcm6V5lBgqHiIy4mWE3lwB+fqLo21HjkYXkly5q5aVO4+walct++qa2VcHFft2vOUzMtPTmFSQfVxBmTQ2hynB64kF2WSlj56l8VU8RGRUKszN5PIzTjm2dEtnp7PlYCNLl68me1wJe2qb2FvbxJ7aJvYEy+JvPxRh+6FIt59nBuPzssgNdVC84iVyMtMJZ4TIyYwe4YwQ4cwQ2cdep0fbM0PkBP3CmenUt3QO5j9DwlQ8RESITtjPnjCG+inZlJXNeMv5SGt7UEya2XOkiT21EfYee93E/mDiHmBb7ZG3xPfHaa88x0Uzirg4WEZm3BB8UkzFQ0QkDuHMdGadMoZZp4zp9nx7RyfVDS28UPE602bMJtLWQXNrB5HWDiJtHTS1thNp7aApaGtq63rdfux9pLWDnYcaeeNA9Hj45ejtszMmjuHimUVcNKOIi6YXURBO/TfxVTxERAZAeiiNSWNzmFmYQdkJS7n0x/JXK0gbP4OXtxzi5a01vLazlo37G9i4v4H/fHE7ZjC3JJ+LZxRx8cwizp8+LiXL5qt4iIgMIRkho2z6OC6YPo7PMJvmtg5W7qzl5a2HWL7lECt3HTm2lP5PXthGmsH8yQVcPKOIi2YWkdE2OHMmg148zOx04FcxTTOArwBjgU8BB4P2L7r7kiDmDuATQAfwT+6+dPAyFhFJneyMEBfPjI4yeA80tXZQueMIy7ce4uWth3h9V+2xlY0feH4raQa/mniY80vH9f3hJ2HQi4e7bwLOATCzELAHeBz4W+D77n5vbH8zmwtcC8wDJgF/NLPT3L1jUBMXERkCcjJDXDq7+NiaYUdb2qnYcSS4zXWI9XtqOWNi9/MyAynVt63eDWxx9x297EZ2FfBLd28BtplZFXAB8PIg5SgiMmTlZqXzjtPG847TxgPw0isVjBmEOZBUL+xyLfBIzPtbzGy1mT1kZoVB22RgV0yf3UGbiIicICt9cL4Fb+4+KBd6y4XNMoG9wDx3P2BmE4AawIGvAyXu/nEzuw942d1/EcT9FFji7o9285kLgYUAJSUlZYsWLUoot0gkQjjc/bo5ile84hU/UuPLy8sr3b08rs7unpKD6O2op3o4VwqsDV7fAdwRc24pcHFfn19WVuaJqqioSDhW8YpXvOKHazxQ4XH+DU/lbavriLllZWYlMec+DKwNXj8JXGtmWWY2HZgNvDpoWYqIyFukZMLczMLAe4C/i2n+tpmdQ/S21fauc+6+zsx+DawH2oGbXU9aiYikVEqKh7tHgKIT2q7vpf/dwN3JzktEROKT6qetRERkGFLxEBGRfkvZo7rJZmYHgbfu6BKfYqKPDSdK8YpXvOKHY/w0dx8fV894H8saTQf9eFxN8YpXvOJHUny8h25biYhIv6l4iIhIv6l4dO9BxSte8YofpfFxGbET5iIikjwaeYiISL+peIiISL+peIiISL+leifBIcHMvtJHl2p3/7HiFa94xY+0+IQNxpdJhvoBLAHygYIejt8qXvGKV/xIjE/00MgjqsPd63s6aWZ9PZKmeMUrXvHDNT4hmvOI6usfV+d1Xud1fqSeT4hGHlEZZpbfwzkDQopXvOIVP0LjE6LiEbUc+GwP5wz4veIVr3jFj9D4xCRjImW4HQzzCS/FK17xik80PtFDI4+oVE9YKV7xild8quITognzqFRPWOm8zuu8zqfqfEI08ohK9YSV4hWveMWnKj4hKh5RXRNO1sP5Pyhe8YpX/AiNT4iWZBcRkX7TnIeIiPSbioeIiPSbiodIH8zsS2a2zsxWm9kqM7swidd61szKk/X5IgNFE+YivTCzi4EPAue5e4uZFQOZKU5LJOU08hDpXQlQ4+4tAO5e4+57zewrZrbCzNaa2YNmZnBs5PB9M3vezDaY2flm9piZbTazbwR9Ss1so5n9PBjN/MbMwide2Mzea2Yvm9lrZva/ZpYXtN9jZuuD2HsH8d9C5BgVD5HePQVMNbM3zOzfzewdQfuP3P18d58P5BAdnXRpdfe3Az8GngBuBuYDN5lZUdDndOBBdz8LqAf+IfaiwQjnTuAKdz8PqAA+b2bjgA8D84LYbyThdxbpk4qHSC/cvREoAxYCB4FfmdlNwOVm9oqZrQHeBcyLCXsy+LkGWOfu+4KRy1ZganBul7u/GLz+BXDpCZe+CJgLvGhmq4AbgWlEC00z8BMz+wgQGbBfVqQfNOch0gd37wCeBZ4NisXfAWcB5e6+y8zuArJjQlqCn50xr7ved/0/d+IXrE58b8Ayd7/uxHzM7ALg3cC1wC1Ei5fIoNLIQ6QXZna6mc2OaToH2BS8rgnmIT6awEefGkzGA1wHvHDC+eXAJWY2K8gjbGanBdcrcPclRL9VfE4C1xY5aRp5iPQuD/ihmY0F2oEqorewaoneltoOrEjgczcAN5rZA8Bm4P7Yk+5+MLg99oiZZQXNdwINwBNmlk10dPK5BK4tctK0PInIIDOzUmBxMNkuMizptpWIiPSbRh4iItJvGnmIiEi/qXiIiEi/qXiIiEi/qXiIiEi/qXiIiEi/qXiIiEi//X8pzhvkag7MDAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17a84334da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fd_fnbigrams=FreqDist(hindi_firstname_bigrams)\n",
    "fd_fnbigrams.plot(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "surname_bigrams = get_ngrams(df['Surname'][surnames_mask], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "hindi_surname_bigrams = get_ngrams(df['hindi_Surname'][surnames_mask], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ha    2992\n",
       "ar    2548\n",
       "an    2276\n",
       "in    2175\n",
       "ra    2156\n",
       "sh    1753\n",
       "gh    1655\n",
       "si    1613\n",
       "ng    1566\n",
       "ma    1454\n",
       "al    1381\n",
       "at    1163\n",
       "pa    1098\n",
       "ai    1067\n",
       "wa    1034\n",
       "as    1033\n",
       "sa     945\n",
       "na     863\n",
       "ri     856\n",
       "ka     851\n",
       "ja     850\n",
       "da     834\n",
       "ch     830\n",
       "ta     794\n",
       "ah     769\n",
       "nd     679\n",
       "va     573\n",
       "er     572\n",
       "de     564\n",
       "ga     550\n",
       "      ... \n",
       "qi       1\n",
       "tg       1\n",
       "dj       1\n",
       "kn       1\n",
       "oq       1\n",
       "fu       1\n",
       "jl       1\n",
       "fz       1\n",
       "bj       1\n",
       "vt       1\n",
       "yr       1\n",
       "db       1\n",
       "wb       1\n",
       "ys       1\n",
       "td       1\n",
       "bg       1\n",
       "gs       1\n",
       "kc       1\n",
       "ci       1\n",
       "io       1\n",
       "xa       1\n",
       "bc       1\n",
       "wt       1\n",
       "cu       1\n",
       "ps       1\n",
       "dv       1\n",
       "tm       1\n",
       "vg       1\n",
       "mw       1\n",
       "jb       1\n",
       "Length: 454, dtype: int64"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(surname_bigrams).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ार    2049\n",
       "रा    1711\n",
       "सि    1625\n",
       "िं    1491\n",
       "ान    1444\n",
       "ंह    1434\n",
       "वा    1316\n",
       "मा    1259\n",
       "ाल    1242\n",
       "ाि     987\n",
       "सा     871\n",
       "जा     819\n",
       "ास     816\n",
       "ना     783\n",
       "पा     727\n",
       "ाह     697\n",
       "हा     682\n",
       "िन     636\n",
       "दा     601\n",
       "रि     531\n",
       "गा     518\n",
       "री     502\n",
       "्र     502\n",
       "या     498\n",
       "्ट     497\n",
       "ाव     495\n",
       "शा     484\n",
       "र्     462\n",
       "ला     459\n",
       "का     456\n",
       "      ... \n",
       "सद       1\n",
       "सॉ       1\n",
       "टी       1\n",
       "डस       1\n",
       "यौ       1\n",
       "दौ       1\n",
       "जल       1\n",
       "णव       1\n",
       "धह       1\n",
       "ईख       1\n",
       "णघ       1\n",
       "वब       1\n",
       "फज       1\n",
       "ैं       1\n",
       "धअ       1\n",
       "यड       1\n",
       "फद       1\n",
       "ाी       1\n",
       "णि       1\n",
       "ूय       1\n",
       "शद       1\n",
       "फड       1\n",
       "ईं       1\n",
       "वु       1\n",
       "यग       1\n",
       "डल       1\n",
       "़त       1\n",
       "डग       1\n",
       "वअ       1\n",
       "ेा       1\n",
       "Length: 763, dtype: int64"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(hindi_surname_bigrams).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17a861cd0b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fd_sntrigrams=FreqDist(hindi_surname_bigrams)\n",
    "fd_sntrigrams.plot(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting labels (a number for each community)\n",
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = le.fit_transform(df['Community'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier:\n",
    "    count = 0\n",
    "    \n",
    "    def __init__(self, name, description=\"\"):\n",
    "        self.name = name\n",
    "        self.description = description\n",
    "        self.vectorizer = CountVectorizer()\n",
    "        self.metrics = {'accuracy': accuracy_score}\n",
    "        self.scores = {}\n",
    "        Classifier.count = Classifier.count + 1\n",
    "        \n",
    "    def fit(self, x, y, clf, vectorizer=None, metrics_dict=None):\n",
    "        if vectorizer is not None:\n",
    "            self.vectorizer = vectorizer\n",
    "        if metrics_dict is not None:\n",
    "            self.metrics.update(metrics_dict)\n",
    "            \n",
    "        x = self.vectorizer.fit_transform(x)\n",
    "        self.clf = clf.fit(x,y)\n",
    "        \n",
    "        self.scores['train'] = {}\n",
    "        for metric, metric_fn in self.metrics.items():\n",
    "            self.scores['train'][metric] = metric_fn(y, self.clf.predict(x))\n",
    "        \n",
    "        self.print_scores('train')\n",
    "        \n",
    "    def predict(self, x):\n",
    "        x = self.vectorizer.transform(x)\n",
    "        return self.clf.predict(x)\n",
    "    \n",
    "    def score(self, x, y, data_set='dev', metrics_dict=None):\n",
    "        if metrics_dict is not None:\n",
    "            self.metrics.update(metrics_dict)\n",
    "        \n",
    "        self.scores[data_set] = {}\n",
    "        for metric, metric_fn in self.metrics.items():\n",
    "            self.scores[data_set][metric] = metric_fn(y, self.predict(x))\n",
    "        \n",
    "        self.print_scores(data_set)\n",
    "        \n",
    "        \n",
    "    def print_scores(self, data_set):\n",
    "        if data_set in self.scores:\n",
    "            print('Scores on',data_set)\n",
    "            scores_dict = self.scores[data_set]\n",
    "            for metric, score in scores_dict.items():\n",
    "                print(metric, \"=\", score)\n",
    "        else:\n",
    "            print('No scores available')\n",
    "        print()\n",
    "        \n",
    "    def get_scores(self):\n",
    "        for data_set in self.scores.keys():\n",
    "            self.print_scores(data_set)\n",
    "        \n",
    "    def add_method(self, name, method):\n",
    "        self.__setattr__(name, method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dict = {'f1_all':lambda y,y_pred: f1_score(y, y_pred, average=None, labels=list(range(25))), \n",
    "                'f1_micro':lambda y,y_pred: f1_score(y, y_pred, average='micro'),\n",
    "                'f1_macro':lambda y,y_pred: f1_score(y, y_pred, average='macro'),\n",
    "                'f1_weighted':lambda y,y_pred: f1_score(y, y_pred, average='weighted')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_dev, y_train, y_dev = train_test_split(df['hindi_Name'][firstnames_mask].values, labels[firstnames_mask], random_state = RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores on train\n",
      "accuracy = 0.3908812398630384\n",
      "f1_all = [0.         0.42476817 0.121673   0.         0.32299125 0.\n",
      " 0.47253353 0.26822157 0.         0.         0.         0.30117647\n",
      " 0.33060109 0.14975042 0.         0.         0.24968474 0.\n",
      " 0.         0.38410596 0.38554217 0.32794457 0.43027027 0.\n",
      " 0.48034934]\n",
      "f1_micro = 0.3908812398630384\n",
      "f1_macro = 0.1937338569432503\n",
      "f1_weighted = 0.3742133885308534\n",
      "\n",
      "Scores on dev\n",
      "accuracy = 0.3435135135135135\n",
      "f1_all = [0.         0.39788732 0.15584416 0.         0.26424242 0.\n",
      " 0.42702703 0.17741935 0.         0.         0.         0.24390244\n",
      " 0.30350195 0.04210526 0.         0.         0.15789474 0.\n",
      " 0.         0.33948339 0.20895522 0.30405405 0.38611714 0.\n",
      " 0.34693878]\n",
      "f1_micro = 0.3435135135135135\n",
      "f1_macro = 0.16327709806016957\n",
      "f1_weighted = 0.3256426162769327\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anush\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anush\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "clfs['nb_fn_2'] = Classifier('nb_fn_2', description=\"naive bayes on first name bigrams\")\n",
    "clfs['nb_fn_2'].fit(X_train, y_train, MultinomialNB(), vectorizer=CountVectorizer(analyzer=lambda x:get_ngrams([x],2)), metrics_dict=metrics_dict)\n",
    "clfs['nb_fn_2'].score(X_dev, y_dev, 'dev', metrics_dict=metrics_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores on train\n",
      "accuracy = 0.5298251937285997\n",
      "f1_all = [0.         0.6276176  0.29268293 0.         0.47623019 0.\n",
      " 0.5743366  0.41551247 0.         0.5        0.         0.4576659\n",
      " 0.40849673 0.31641791 0.         0.         0.41388175 0.\n",
      " 0.         0.51186992 0.29787234 0.47478992 0.56423841 0.\n",
      " 0.65758092]\n",
      "f1_micro = 0.5298251937285997\n",
      "f1_macro = 0.29121639964098595\n",
      "f1_weighted = 0.5153510609376355\n",
      "\n",
      "Scores on dev\n",
      "accuracy = 0.41\n",
      "f1_all = [0.         0.5045045  0.25       0.         0.32305795 0.\n",
      " 0.49558442 0.21052632 0.         0.75       0.         0.304\n",
      " 0.30901288 0.02970297 0.         0.         0.22535211 0.\n",
      " 0.         0.40038314 0.17857143 0.37082067 0.42812823 0.\n",
      " 0.49079755]\n",
      "f1_micro = 0.41\n",
      "f1_macro = 0.2291496593137238\n",
      "f1_weighted = 0.3884855810404465\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anush\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anush\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "clfs['nb_fn_3'] = Classifier('nb_fn_3', description=\"naive bayes on first name trigrams\")\n",
    "clfs['nb_fn_3'].fit(X_train, y_train, MultinomialNB(), vectorizer=CountVectorizer(analyzer=lambda x:get_ngrams([x],3)), metrics_dict=metrics_dict)\n",
    "clfs['nb_fn_3'].score(X_dev, y_dev, 'dev', metrics_dict=metrics_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anush\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anush\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores on train\n",
      "accuracy = 0.4517030095512705\n",
      "f1_all = [0.         0.503961   0.25165563 0.         0.40315908 0.\n",
      " 0.5279661  0.28871391 0.         0.47826087 0.66666667 0.41269841\n",
      " 0.34138973 0.22916667 0.         0.         0.36383208 0.\n",
      " 0.         0.43124597 0.4372093  0.37083708 0.49455041 0.\n",
      " 0.58187135]\n",
      "f1_micro = 0.4517030095512705\n",
      "f1_macro = 0.2826326770610507\n",
      "f1_weighted = 0.4418599427467069\n",
      "\n",
      "Scores on dev\n",
      "accuracy = 0.3854054054054054\n",
      "f1_all = [0.         0.44484305 0.21686747 0.         0.32685714 0.\n",
      " 0.50184049 0.21538462 0.         0.5        0.         0.28571429\n",
      " 0.2578125  0.00943396 0.         0.         0.24087591 0.\n",
      " 0.         0.37335835 0.275      0.34482759 0.42650104 0.\n",
      " 0.37837838]\n",
      "f1_micro = 0.3854054054054054\n",
      "f1_macro = 0.2085954251036278\n",
      "f1_weighted = 0.37112920133719385\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clfs['nb_fn_23'] = Classifier('nb_fn_23', description=\"naive bayes on first name bigrams and trigrams\")\n",
    "clfs['nb_fn_23'].fit(X_train, y_train, MultinomialNB(), vectorizer=CountVectorizer(analyzer=lambda x:get_ngrams([x],2)+get_ngrams([x],3)), metrics_dict=metrics_dict)\n",
    "clfs['nb_fn_23'].score(X_dev, y_dev, 'dev', metrics_dict=metrics_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_dev, y_train, y_dev = train_test_split(df['hindi_Surname'][surnames_mask].values, labels[surnames_mask], random_state = RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anush\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anush\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores on train\n",
      "accuracy = 0.5991949910554561\n",
      "f1_all = [0.21428571 0.67556403 0.28571429 0.         0.64361898 0.\n",
      " 0.52366376 0.56542056 0.         0.         0.         0.67241379\n",
      " 0.53286385 0.5192604  0.         0.         0.71596032 0.\n",
      " 0.         0.74229425 0.20512821 0.5093046  0.52808169 0.\n",
      " 0.59084195]\n",
      "f1_micro = 0.5991949910554561\n",
      "f1_macro = 0.31697665588911006\n",
      "f1_weighted = 0.5952188781433485\n",
      "\n",
      "Scores on dev\n",
      "accuracy = 0.5760665414542527\n",
      "f1_all = [0.33333333 0.64769382 0.1443299  0.         0.64563617 0.\n",
      " 0.47666906 0.47272727 0.         0.         0.         0.69090909\n",
      " 0.46212121 0.52009456 0.         0.         0.66666667 0.\n",
      " 0.         0.7645479  0.05714286 0.47588424 0.49060543 0.\n",
      " 0.51282051]\n",
      "f1_micro = 0.5760665414542527\n",
      "f1_macro = 0.36805910116380247\n",
      "f1_weighted = 0.5700921061817968\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clfs['nb_sn_2'] = Classifier('nb_sn_2', description=\"naive bayes on surname bigrams\")\n",
    "clfs['nb_sn_2'].fit(X_train, y_train, MultinomialNB(), vectorizer=CountVectorizer(analyzer=lambda x:get_ngrams([x],2)), metrics_dict=metrics_dict)\n",
    "clfs['nb_sn_2'].score(X_dev, y_dev, 'dev', metrics_dict=metrics_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores on train\n",
      "accuracy = 0.7161001788908765\n",
      "f1_all = [0.375      0.7989838  0.38795987 0.         0.78976641 0.\n",
      " 0.62158956 0.71394799 0.         0.10526316 0.         0.72689076\n",
      " 0.71239471 0.58814103 0.         0.         0.77340824 0.\n",
      " 0.         0.81599536 0.43410853 0.66530194 0.67715356 0.\n",
      " 0.76175549]\n",
      "f1_micro = 0.7161001788908765\n",
      "f1_macro = 0.397906415741577\n",
      "f1_weighted = 0.7104874011469009\n",
      "\n",
      "Scores on dev\n",
      "accuracy = 0.6485108666487792\n",
      "f1_all = [0.57142857 0.73633749 0.18604651 0.         0.7199148  0.\n",
      " 0.56373938 0.61946903 0.         0.         0.         0.66666667\n",
      " 0.5483871  0.53731343 0.         0.         0.74486804 0.\n",
      " 0.         0.80067002 0.11111111 0.51006711 0.57173448 0.\n",
      " 0.63963964]\n",
      "f1_micro = 0.6485108666487792\n",
      "f1_macro = 0.4263696682905042\n",
      "f1_weighted = 0.6397587767356016\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anush\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anush\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "clfs['nb_sn_3'] = Classifier('nb_sn_3', description=\"naive bayes on surname trigrams\")\n",
    "clfs['nb_sn_3'].fit(X_train, y_train, MultinomialNB(), vectorizer=CountVectorizer(analyzer=lambda x:get_ngrams([x],3)), metrics_dict=metrics_dict)\n",
    "clfs['nb_sn_3'].score(X_dev, y_dev, 'dev', metrics_dict=metrics_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anush\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anush\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores on train\n",
      "accuracy = 0.6820214669051878\n",
      "f1_all = [0.4375     0.77717217 0.40718563 0.         0.74971429 0.\n",
      " 0.57945835 0.68649886 0.         0.10526316 0.         0.708\n",
      " 0.64908257 0.56254519 0.         0.         0.75198588 0.\n",
      " 0.         0.8020184  0.38461538 0.60294118 0.63387157 0.\n",
      " 0.69886364]\n",
      "f1_micro = 0.6820214669051878\n",
      "f1_macro = 0.3814686506379185\n",
      "f1_weighted = 0.6773180695434098\n",
      "\n",
      "Scores on dev\n",
      "accuracy = 0.6329487523477327\n",
      "f1_all = [0.57142857 0.72458045 0.22       0.         0.7227616  0.\n",
      " 0.52745553 0.58928571 0.         0.         0.         0.65895954\n",
      " 0.52554745 0.54298643 0.         0.         0.70879121 0.\n",
      " 0.         0.79524214 0.11111111 0.48467967 0.57112069 0.\n",
      " 0.59574468]\n",
      "f1_micro = 0.6329487523477327\n",
      "f1_macro = 0.4174847385742547\n",
      "f1_weighted = 0.6259508724133239\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clfs['nb_sn_23'] = Classifier('nb_sn_23', description=\"naive bayes on surname bigrams and trigrams\")\n",
    "clfs['nb_sn_23'].fit(X_train, y_train, MultinomialNB(), vectorizer=CountVectorizer(analyzer=lambda x:get_ngrams([x],2)+get_ngrams([x],3)), metrics_dict=metrics_dict)\n",
    "clfs['nb_sn_23'].score(X_dev, y_dev, 'dev', metrics_dict=metrics_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_dev, y_train, y_dev = train_test_split(df[['hindi_Name','hindi_Surname']][fullnames_mask].values, labels[fullnames_mask], random_state = RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anush\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anush\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores on train\n",
      "accuracy = 0.7916968817984047\n",
      "f1_all = [0.51612903 0.84189853 0.47368421 0.         0.8275076  0.\n",
      " 0.76242096 0.7613941  0.         0.5        0.22222222 0.82258065\n",
      " 0.77720207 0.73746313 1.         0.2        0.7833698  1.\n",
      " 0.         0.847254   0.56934307 0.72727273 0.77923021 0.\n",
      " 0.82276423]\n",
      "f1_micro = 0.7916968817984047\n",
      "f1_macro = 0.5588694613444164\n",
      "f1_weighted = 0.7875756496972992\n",
      "\n",
      "Scores on dev\n",
      "accuracy = 0.7047308319738989\n",
      "f1_all = [0.         0.78952123 0.34042553 0.         0.71175166 0.\n",
      " 0.71606476 0.625      0.         0.22222222 0.         0.7114094\n",
      " 0.65502183 0.6038961  0.         0.         0.67105263 0.\n",
      " 0.         0.792      0.32       0.58666667 0.66368715 0.\n",
      " 0.70247934]\n",
      "f1_micro = 0.7047308319738989\n",
      "f1_macro = 0.43386659641292996\n",
      "f1_weighted = 0.6974210959822423\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clfs['nb_fsn_3'] = Classifier('nb_fsn_3', description=\"naive bayes on first name and surname trigrams\")\n",
    "clfs['nb_fsn_3'].fit(X_train, y_train, MultinomialNB(), vectorizer=CountVectorizer(analyzer=lambda x:get_ngrams(x,3)), metrics_dict=metrics_dict)\n",
    "clfs['nb_fsn_3'].score(X_dev, y_dev, 'dev', metrics_dict=metrics_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anush\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores on train\n",
      "accuracy = 0.7483683828861494\n",
      "f1_all = [0.35714286 0.80027036 0.40549828 0.         0.79038317 0.\n",
      " 0.70409134 0.73404255 0.         0.2        0.         0.76612903\n",
      " 0.73358116 0.66965889 0.         0.2        0.76612903 0.\n",
      " 0.         0.82641292 0.54545455 0.67779961 0.73653984 0.\n",
      " 0.7562777 ]\n",
      "f1_micro = 0.7483683828861495\n",
      "f1_macro = 0.42677645161982064\n",
      "f1_weighted = 0.7436884449333901\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anush\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores on dev\n",
      "accuracy = 0.6810766721044046\n",
      "f1_all = [0.         0.77209302 0.28888889 0.         0.69441402 0.\n",
      " 0.67722772 0.5890411  0.         0.         0.         0.69333333\n",
      " 0.61290323 0.59130435 0.         0.         0.68639053 0.\n",
      " 0.         0.77744209 0.29787234 0.55016181 0.63565891 0.\n",
      " 0.71814672]\n",
      "f1_micro = 0.6810766721044046\n",
      "f1_macro = 0.4088037176330558\n",
      "f1_weighted = 0.6745353670596259\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clfs['nb_fsn_23'] = Classifier('nb_fsn_23', description=\"naive bayes on first name and surname bigrams trigrams\")\n",
    "clfs['nb_fsn_23'].fit(X_train, y_train, MultinomialNB(), vectorizer=CountVectorizer(analyzer=lambda x:get_ngrams(x,2) + get_ngrams(x,3)), metrics_dict=metrics_dict)\n",
    "clfs['nb_fsn_23'].score(X_dev, y_dev, 'dev', metrics_dict=metrics_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "overall, using hindi script gives improvement of 2 to 5 % in accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assamese\n",
      "['ंकट' 'ाजप' 'ाजन' 'ाजद' 'ाजट' 'ाजज' 'ाजग' 'ाजभ' 'ाजक' 'ाचो']\n",
      "bengali\n",
      "['लोव' 'सनं' 'सनी' 'सनु' 'सन्' 'सपा' 'सपे' 'सबा' 'सबि' 'सबी']\n",
      "english\n",
      "['ंकट' 'ांध' 'ाईं' 'ाईख' 'ाईन' 'ाईफ' 'ाईल' 'ाईस' 'ाओं' 'ाओन']\n",
      "garhwali\n",
      "['ंकट' 'ातम' 'ातब' 'ातप' 'ातन' 'ातग' 'ातक' 'ातय' 'ाडो' 'ाडर']\n",
      "gujarati\n",
      "['ंकट' 'सोो' 'सौव' 'स्व' 'हंक' 'हंट' 'हकु' 'हगा' 'हज़' 'हजा']\n",
      "haryanavi\n",
      "['ंकट' 'ातब' 'ातप' 'ातन' 'ातग' 'ातक' 'ाडो' 'ातम' 'ाडी' 'ाट्']\n",
      "hindi\n",
      "['ंकट' 'हनी' 'हनु' 'हने' 'हफु' 'हबा' 'हबू' 'हमद' 'हमा' 'हमि']\n",
      "kannada\n",
      "['ंकट' '़ेफ' '़ेब' '़ॉय' '़ोड' '़ोप' '़ोह' 'ांध' 'ाईं' 'ाईख']\n",
      "kashmiri\n",
      "['ंकट' 'ातय' 'ातम' 'ातब' 'ातप' 'ातन' 'ातग' 'ातर' 'ातक' 'ाडी']\n",
      "konkani\n",
      "['ंकट' 'ाजो' 'ाजॉ' 'ाजे' 'ाजु' 'ाजी' 'ाजि' 'ाटो' 'ाजा' 'ाजस']\n",
      "kumaoni\n",
      "['ंकट' 'ातग' 'ातक' 'ाडो' 'ाडी' 'ाडर' 'ाट्' 'ातन' 'ाटो' 'ाजॉ']\n",
      "malayalam\n",
      "['ंकट' '़ार' '़ाल' '़ाव' '़ाह' '़ाि' '़िज' '़िद' '़िन' '़िम']\n",
      "marathi\n",
      "['लोव' '़ार' '़ाल' '़ाव' '़ाह' '़ाि' '़िज' '़िद' '़िन' '़िम']\n",
      "marwari\n",
      "['ंकट' 'हेत' 'हेद' 'हेब' 'हेल' 'हेव' 'हेश' 'हेस' 'हॉय' 'हों']\n",
      "mizo\n",
      "['ंकट' 'ातर' 'ातय' 'ातम' 'ातब' 'ातप' 'ातन' 'ातल' 'ातग' 'ाडो']\n",
      "nepali\n",
      "['ंकट' 'ाडी' 'ाडर' 'ाट्' 'ाटो' 'ाजो' 'ाजॉ' 'ाडो' 'ाजे' 'ाजी']\n",
      "oriya\n",
      "['ंकट' 'हूद' 'हून' 'हूल' 'हेग' 'हेज' 'हेट' 'हेड' 'हेत' 'हेद']\n",
      "pashto\n",
      "['ंकट' 'ातय' 'ातम' 'ातब' 'ातप' 'ातन' 'ातग' 'ातर' 'ातक' 'ाडी']\n",
      "persian\n",
      "['ंकट' 'ातम' 'ातब' 'ातप' 'ातन' 'ातग' 'ातक' 'ातय' 'ाडो' 'ाडर']\n",
      "punjabi\n",
      "['ंकट' 'सोल' 'सोू' 'सौम' 'सौव' 'स्व' 'हंक' 'हंट' 'हकल' 'हकु']\n",
      "sindhi\n",
      "['ंकट' 'ागत' 'ागद' 'ागन' 'ागब' 'ागम' 'ागय' 'ागर' 'ागल' 'ागव']\n",
      "tamil\n",
      "['लोव' 'हेग' 'हेज' 'हेट' 'हेड' 'हेत' 'हेद' 'हेन' 'हेर' 'हेल']\n",
      "telugu\n",
      "['लोव' 'हंत' 'हअल' 'हकल' 'हकु' 'हगा' 'हचा' 'हज़' 'हजा' 'हटक']\n",
      "tulu\n",
      "['ंकट' 'ातय' 'ातम' 'ातब' 'ातप' 'ातन' 'ातग' 'ातर' 'ातक' 'ाडी']\n",
      "urdu\n",
      "['ंकट' '़ोह' 'ांक' 'ांग' 'ांट' 'ांड' 'ांध' 'ाईं' 'ाईल' 'ाईस']\n"
     ]
    }
   ],
   "source": [
    "# 10 most impt features for each class\n",
    "for i, class_idx in enumerate(clfs['nb_fsn_3'].clf.classes_):\n",
    "    print(le.classes_[class_idx])\n",
    "    class_prob_sorted = clfs['nb_fsn_3'].clf.feature_log_prob_[i, :].argsort()\n",
    "    print(np.take(clfs['nb_fsn_3'].vectorizer.get_feature_names(), class_prob_sorted[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train and dev f1 scores for each community as well as the total number of samples available for the community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs['nb_fsn_3'].scores_df = pd.DataFrame({'f1_train':clfs['nb_fsn_3'].scores['train']['f1_all'], 'f1_test':clfs['nb_fsn_3'].scores['dev']['f1_all']}, index=le.classes_).join(pd.DataFrame(df['Community'].value_counts().rename('value_counts')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1_test</th>\n",
       "      <th>f1_train</th>\n",
       "      <th>value_counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mizo</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pashto</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>persian</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kashmiri</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tulu</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>garhwali</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>haryanavi</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kumaoni</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nepali</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>assamese</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.516129</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>konkani</th>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sindhi</th>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.569343</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>english</th>\n",
       "      <td>0.340426</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tamil</th>\n",
       "      <td>0.586667</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>marwari</th>\n",
       "      <td>0.603896</td>\n",
       "      <td>0.737463</td>\n",
       "      <td>674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kannada</th>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.761394</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>marathi</th>\n",
       "      <td>0.655022</td>\n",
       "      <td>0.777202</td>\n",
       "      <td>545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>telugu</th>\n",
       "      <td>0.663687</td>\n",
       "      <td>0.779230</td>\n",
       "      <td>1770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oriya</th>\n",
       "      <td>0.671053</td>\n",
       "      <td>0.783370</td>\n",
       "      <td>672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>urdu</th>\n",
       "      <td>0.702479</td>\n",
       "      <td>0.822764</td>\n",
       "      <td>446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>malayalam</th>\n",
       "      <td>0.711409</td>\n",
       "      <td>0.822581</td>\n",
       "      <td>364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gujarati</th>\n",
       "      <td>0.711752</td>\n",
       "      <td>0.827508</td>\n",
       "      <td>1760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hindi</th>\n",
       "      <td>0.716065</td>\n",
       "      <td>0.762421</td>\n",
       "      <td>2935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bengali</th>\n",
       "      <td>0.789521</td>\n",
       "      <td>0.841899</td>\n",
       "      <td>2073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>punjabi</th>\n",
       "      <td>0.792000</td>\n",
       "      <td>0.847254</td>\n",
       "      <td>2208</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            f1_test  f1_train  value_counts\n",
       "mizo       0.000000  1.000000             1\n",
       "pashto     0.000000  1.000000             1\n",
       "persian    0.000000  0.000000             1\n",
       "kashmiri   0.000000  0.000000             2\n",
       "tulu       0.000000  0.000000             4\n",
       "garhwali   0.000000  0.000000             6\n",
       "haryanavi  0.000000  0.000000             6\n",
       "kumaoni    0.000000  0.222222            10\n",
       "nepali     0.000000  0.200000            13\n",
       "assamese   0.000000  0.516129            30\n",
       "konkani    0.222222  0.500000            26\n",
       "sindhi     0.320000  0.569343           131\n",
       "english    0.340426  0.473684           302\n",
       "tamil      0.586667  0.727273           661\n",
       "marwari    0.603896  0.737463           674\n",
       "kannada    0.625000  0.761394           358\n",
       "marathi    0.655022  0.777202           545\n",
       "telugu     0.663687  0.779230          1770\n",
       "oriya      0.671053  0.783370           672\n",
       "urdu       0.702479  0.822764           446\n",
       "malayalam  0.711409  0.822581           364\n",
       "gujarati   0.711752  0.827508          1760\n",
       "hindi      0.716065  0.762421          2935\n",
       "bengali    0.789521  0.841899          2073\n",
       "punjabi    0.792000  0.847254          2208"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfs['nb_fsn_3'].scores_df.sort_values(by=['f1_test','value_counts'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB classifier on first name and last name trigrams does relatively well on Punjabi and Bengali names.\n",
    "\n",
    "Overall, the greater number of available examples for a class, the better the performance (dev)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Hindi script shows significant dev score improvement for virtually all communities (especially for communities with more than 100 samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores on train\n",
      "accuracy = 0.9849528643944888\n",
      "f1_all = [1.         0.98405316 0.95035461 1.         0.98743814 1.\n",
      " 0.98256081 0.98130841 1.         1.         1.         0.99261993\n",
      " 0.98540146 0.9786802  1.         1.         0.98480243 1.\n",
      " 1.         0.98780125 0.98924731 0.99071207 0.98664632 1.\n",
      " 0.98827471]\n",
      "f1_micro = 0.9849528643944889\n",
      "f1_macro = 0.9907960323561631\n",
      "f1_weighted = 0.9849195144379089\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anush\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anush\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores on dev\n",
      "accuracy = 0.7058183795541055\n",
      "f1_all = [0.44444444 0.75544794 0.43636364 0.5        0.72666667 0.\n",
      " 0.70104439 0.68874172 0.         0.2        0.         0.72049689\n",
      " 0.63247863 0.47019868 0.         0.         0.75516224 0.\n",
      " 0.         0.81036269 0.5974026  0.60150376 0.6912114  0.\n",
      " 0.65700483]\n",
      "f1_micro = 0.7058183795541055\n",
      "f1_macro = 0.49469192977897514\n",
      "f1_weighted = 0.7007762713572684\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clfs['rf_fsn_3'] = Classifier('rf_fsn_3', description=\"random forest on first name and surname trigrams\")\n",
    "clfs['rf_fsn_3'].fit(X_train, y_train, RandomForestClassifier(), vectorizer=CountVectorizer(analyzer=lambda x:get_ngrams(x,3)), metrics_dict=metrics_dict)\n",
    "clfs['rf_fsn_3'].score(X_dev, y_dev, 'dev', metrics_dict=metrics_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No improvement with Hindi script.\n",
    "\n",
    "Trying Search over hyperparameters to reduce variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters currently in use:\n",
      "\n",
      "{'bootstrap': True,\n",
      " 'class_weight': None,\n",
      " 'criterion': 'gini',\n",
      " 'max_depth': None,\n",
      " 'max_features': 'auto',\n",
      " 'max_leaf_nodes': None,\n",
      " 'min_impurity_decrease': 0.0,\n",
      " 'min_impurity_split': None,\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 2,\n",
      " 'min_weight_fraction_leaf': 0.0,\n",
      " 'n_estimators': 10,\n",
      " 'n_jobs': 1,\n",
      " 'oob_score': False,\n",
      " 'random_state': None,\n",
      " 'verbose': 0,\n",
      " 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "# Look at parameters used by our current forest\n",
    "print('Parameters currently in use:\\n')\n",
    "pprint(clfs['rf_fsn_3'].clf.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_grid = \\\n",
    "{'bootstrap': [True, False],\n",
    " 'max_depth': [None]+list(range(5,50,5)),\n",
    " 'max_features': ['auto', 'sqrt'],\n",
    " 'min_samples_leaf': list(range(1,12)),\n",
    " 'min_samples_split': list(range(2,21)),\n",
    " 'n_estimators': list(range(1,25))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 100 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anush\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:605: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=4.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    8.6s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   18.8s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:   34.2s\n",
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed:   39.5s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=4, error_score='raise',\n",
       "          estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "          fit_params=None, iid=True, n_iter=100, n_jobs=-1,\n",
       "          param_distributions={'bootstrap': [True, False], 'max_depth': [None, 5, 10, 15, 20, 25, 30, 35, 40, 45], 'max_features': ['auto', 'sqrt'], 'min_samples_leaf': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], 'min_samples_split': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], 'n_estimators': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]},\n",
       "          pre_dispatch='2*n_jobs', random_state=0, refit=True,\n",
       "          return_train_score='warn', scoring=None, verbose=2)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_v = clfs['rf_fsn_3'].vectorizer.transform(X_train)\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 4, verbose=2, random_state=RANDOM_STATE, n_jobs = -1)\n",
    "\n",
    "rf_random.fit(X_train_v, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': False,\n",
       " 'max_depth': None,\n",
       " 'max_features': 'auto',\n",
       " 'min_samples_leaf': 2,\n",
       " 'min_samples_split': 19,\n",
       " 'n_estimators': 18}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anush\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores on train\n",
      "accuracy = 0.7694887599709934\n",
      "f1_all = [0.60606061 0.81588903 0.44745763 0.         0.81773023 0.\n",
      " 0.73121132 0.70422535 0.         0.51851852 0.         0.76447876\n",
      " 0.74123989 0.55247286 0.         0.36363636 0.77608916 0.\n",
      " 0.         0.86030268 0.625      0.74190178 0.7724235  0.\n",
      " 0.73033708]\n",
      "f1_micro = 0.7694887599709934\n",
      "f1_macro = 0.4627589897115514\n",
      "f1_weighted = 0.7634127753499784\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anush\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores on dev\n",
      "accuracy = 0.697389885807504\n",
      "f1_all = [0.44444444 0.77029361 0.32967033 0.         0.73755656 0.\n",
      " 0.68214055 0.67123288 0.         0.18181818 0.         0.67096774\n",
      " 0.63636364 0.45692884 0.         0.         0.71597633 0.\n",
      " 0.         0.81665015 0.45945946 0.62372881 0.6631016  0.\n",
      " 0.61083744]\n",
      "f1_micro = 0.697389885807504\n",
      "f1_macro = 0.4510081224227622\n",
      "f1_weighted = 0.6895931580006398\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clfs['rf_fsn_3_randsearch'] = Classifier('rf_fsn_3_randsearch', description=\"random forest on first name and surname trigrams, min_samples_split=10\")\n",
    "clfs['rf_fsn_3_randsearch'].fit(X_train, y_train, RandomForestClassifier().set_params(**rf_random.best_params_), vectorizer=CountVectorizer(analyzer=lambda x:get_ngrams(x,3)), metrics_dict=metrics_dict)\n",
    "clfs['rf_fsn_3_randsearch'].score(X_dev, y_dev, 'dev', metrics_dict=metrics_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lower scores on dev.. ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying a grid search on parameter ranges that seem promising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_grid = \\\n",
    "{'min_samples_split': list(range(2,7))+list(range(15,21)),\n",
    " 'n_estimators': list(range(8,12))+list(range(18,21))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'min_samples_split': [2, 3, 4, 5, 6, 15, 16, 17, 18, 19, 20],\n",
       " 'n_estimators': [8, 9, 10, 11, 18, 19, 20]}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 77 candidates, totalling 308 fits\n",
      "[CV] min_samples_split=2, n_estimators=8 .............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anush\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:605: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=4.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .............. min_samples_split=2, n_estimators=8, total=   2.8s\n",
      "[CV] min_samples_split=2, n_estimators=8 .............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.9s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .............. min_samples_split=2, n_estimators=8, total=   2.8s\n",
      "[CV] min_samples_split=2, n_estimators=8 .............................\n",
      "[CV] .............. min_samples_split=2, n_estimators=8, total=   2.9s\n",
      "[CV] min_samples_split=2, n_estimators=8 .............................\n",
      "[CV] .............. min_samples_split=2, n_estimators=8, total=   3.2s\n",
      "[CV] min_samples_split=2, n_estimators=9 .............................\n",
      "[CV] .............. min_samples_split=2, n_estimators=9, total=   3.4s\n",
      "[CV] min_samples_split=2, n_estimators=9 .............................\n",
      "[CV] .............. min_samples_split=2, n_estimators=9, total=   3.3s\n",
      "[CV] min_samples_split=2, n_estimators=9 .............................\n",
      "[CV] .............. min_samples_split=2, n_estimators=9, total=   3.1s\n",
      "[CV] min_samples_split=2, n_estimators=9 .............................\n",
      "[CV] .............. min_samples_split=2, n_estimators=9, total=   3.1s\n",
      "[CV] min_samples_split=2, n_estimators=10 ............................\n",
      "[CV] ............. min_samples_split=2, n_estimators=10, total=   3.5s\n",
      "[CV] min_samples_split=2, n_estimators=10 ............................\n",
      "[CV] ............. min_samples_split=2, n_estimators=10, total=   3.8s\n",
      "[CV] min_samples_split=2, n_estimators=10 ............................\n",
      "[CV] ............. min_samples_split=2, n_estimators=10, total=   3.5s\n",
      "[CV] min_samples_split=2, n_estimators=10 ............................\n",
      "[CV] ............. min_samples_split=2, n_estimators=10, total=   3.4s\n",
      "[CV] min_samples_split=2, n_estimators=11 ............................\n",
      "[CV] ............. min_samples_split=2, n_estimators=11, total=   4.6s\n",
      "[CV] min_samples_split=2, n_estimators=11 ............................\n",
      "[CV] ............. min_samples_split=2, n_estimators=11, total=   4.4s\n",
      "[CV] min_samples_split=2, n_estimators=11 ............................\n",
      "[CV] ............. min_samples_split=2, n_estimators=11, total=   4.9s\n",
      "[CV] min_samples_split=2, n_estimators=11 ............................\n",
      "[CV] ............. min_samples_split=2, n_estimators=11, total=   4.6s\n",
      "[CV] min_samples_split=2, n_estimators=18 ............................\n",
      "[CV] ............. min_samples_split=2, n_estimators=18, total=   7.5s\n",
      "[CV] min_samples_split=2, n_estimators=18 ............................\n",
      "[CV] ............. min_samples_split=2, n_estimators=18, total=   7.2s\n",
      "[CV] min_samples_split=2, n_estimators=18 ............................\n",
      "[CV] ............. min_samples_split=2, n_estimators=18, total=   6.4s\n",
      "[CV] min_samples_split=2, n_estimators=18 ............................\n",
      "[CV] ............. min_samples_split=2, n_estimators=18, total=   6.2s\n",
      "[CV] min_samples_split=2, n_estimators=19 ............................\n",
      "[CV] ............. min_samples_split=2, n_estimators=19, total=   6.5s\n",
      "[CV] min_samples_split=2, n_estimators=19 ............................\n",
      "[CV] ............. min_samples_split=2, n_estimators=19, total=   6.5s\n",
      "[CV] min_samples_split=2, n_estimators=19 ............................\n",
      "[CV] ............. min_samples_split=2, n_estimators=19, total=   6.0s\n",
      "[CV] min_samples_split=2, n_estimators=19 ............................\n",
      "[CV] ............. min_samples_split=2, n_estimators=19, total=   6.5s\n",
      "[CV] min_samples_split=2, n_estimators=20 ............................\n",
      "[CV] ............. min_samples_split=2, n_estimators=20, total=   6.7s\n",
      "[CV] min_samples_split=2, n_estimators=20 ............................\n",
      "[CV] ............. min_samples_split=2, n_estimators=20, total=   7.1s\n",
      "[CV] min_samples_split=2, n_estimators=20 ............................\n",
      "[CV] ............. min_samples_split=2, n_estimators=20, total=   7.2s\n",
      "[CV] min_samples_split=2, n_estimators=20 ............................\n",
      "[CV] ............. min_samples_split=2, n_estimators=20, total=   7.8s\n",
      "[CV] min_samples_split=3, n_estimators=8 .............................\n",
      "[CV] .............. min_samples_split=3, n_estimators=8, total=   2.5s\n",
      "[CV] min_samples_split=3, n_estimators=8 .............................\n",
      "[CV] .............. min_samples_split=3, n_estimators=8, total=   2.8s\n",
      "[CV] min_samples_split=3, n_estimators=8 .............................\n",
      "[CV] .............. min_samples_split=3, n_estimators=8, total=   2.2s\n",
      "[CV] min_samples_split=3, n_estimators=8 .............................\n",
      "[CV] .............. min_samples_split=3, n_estimators=8, total=   2.2s\n",
      "[CV] min_samples_split=3, n_estimators=9 .............................\n",
      "[CV] .............. min_samples_split=3, n_estimators=9, total=   2.7s\n",
      "[CV] min_samples_split=3, n_estimators=9 .............................\n",
      "[CV] .............. min_samples_split=3, n_estimators=9, total=   2.7s\n",
      "[CV] min_samples_split=3, n_estimators=9 .............................\n",
      "[CV] .............. min_samples_split=3, n_estimators=9, total=   2.6s\n",
      "[CV] min_samples_split=3, n_estimators=9 .............................\n",
      "[CV] .............. min_samples_split=3, n_estimators=9, total=   2.6s\n",
      "[CV] min_samples_split=3, n_estimators=10 ............................\n",
      "[CV] ............. min_samples_split=3, n_estimators=10, total=   2.8s\n",
      "[CV] min_samples_split=3, n_estimators=10 ............................\n",
      "[CV] ............. min_samples_split=3, n_estimators=10, total=   2.9s\n",
      "[CV] min_samples_split=3, n_estimators=10 ............................\n",
      "[CV] ............. min_samples_split=3, n_estimators=10, total=   2.9s\n",
      "[CV] min_samples_split=3, n_estimators=10 ............................\n",
      "[CV] ............. min_samples_split=3, n_estimators=10, total=   2.9s\n",
      "[CV] min_samples_split=3, n_estimators=11 ............................\n",
      "[CV] ............. min_samples_split=3, n_estimators=11, total=   3.1s\n",
      "[CV] min_samples_split=3, n_estimators=11 ............................\n",
      "[CV] ............. min_samples_split=3, n_estimators=11, total=   3.1s\n",
      "[CV] min_samples_split=3, n_estimators=11 ............................\n",
      "[CV] ............. min_samples_split=3, n_estimators=11, total=   2.5s\n",
      "[CV] min_samples_split=3, n_estimators=11 ............................\n",
      "[CV] ............. min_samples_split=3, n_estimators=11, total=   3.1s\n",
      "[CV] min_samples_split=3, n_estimators=18 ............................\n",
      "[CV] ............. min_samples_split=3, n_estimators=18, total=   4.5s\n",
      "[CV] min_samples_split=3, n_estimators=18 ............................\n",
      "[CV] ............. min_samples_split=3, n_estimators=18, total=   5.0s\n",
      "[CV] min_samples_split=3, n_estimators=18 ............................\n",
      "[CV] ............. min_samples_split=3, n_estimators=18, total=   5.2s\n",
      "[CV] min_samples_split=3, n_estimators=18 ............................\n",
      "[CV] ............. min_samples_split=3, n_estimators=18, total=   5.3s\n",
      "[CV] min_samples_split=3, n_estimators=19 ............................\n",
      "[CV] ............. min_samples_split=3, n_estimators=19, total=   5.5s\n",
      "[CV] min_samples_split=3, n_estimators=19 ............................\n",
      "[CV] ............. min_samples_split=3, n_estimators=19, total=   5.6s\n",
      "[CV] min_samples_split=3, n_estimators=19 ............................\n",
      "[CV] ............. min_samples_split=3, n_estimators=19, total=   5.9s\n",
      "[CV] min_samples_split=3, n_estimators=19 ............................\n",
      "[CV] ............. min_samples_split=3, n_estimators=19, total=   5.6s\n",
      "[CV] min_samples_split=3, n_estimators=20 ............................\n",
      "[CV] ............. min_samples_split=3, n_estimators=20, total=   6.0s\n",
      "[CV] min_samples_split=3, n_estimators=20 ............................\n",
      "[CV] ............. min_samples_split=3, n_estimators=20, total=   5.8s\n",
      "[CV] min_samples_split=3, n_estimators=20 ............................\n",
      "[CV] ............. min_samples_split=3, n_estimators=20, total=   5.9s\n",
      "[CV] min_samples_split=3, n_estimators=20 ............................\n",
      "[CV] ............. min_samples_split=3, n_estimators=20, total=   5.3s\n",
      "[CV] min_samples_split=4, n_estimators=8 .............................\n",
      "[CV] .............. min_samples_split=4, n_estimators=8, total=   1.7s\n",
      "[CV] min_samples_split=4, n_estimators=8 .............................\n",
      "[CV] .............. min_samples_split=4, n_estimators=8, total=   2.0s\n",
      "[CV] min_samples_split=4, n_estimators=8 .............................\n",
      "[CV] .............. min_samples_split=4, n_estimators=8, total=   2.0s\n",
      "[CV] min_samples_split=4, n_estimators=8 .............................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .............. min_samples_split=4, n_estimators=8, total=   2.1s\n",
      "[CV] min_samples_split=4, n_estimators=9 .............................\n",
      "[CV] .............. min_samples_split=4, n_estimators=9, total=   2.5s\n",
      "[CV] min_samples_split=4, n_estimators=9 .............................\n",
      "[CV] .............. min_samples_split=4, n_estimators=9, total=   2.5s\n",
      "[CV] min_samples_split=4, n_estimators=9 .............................\n",
      "[CV] .............. min_samples_split=4, n_estimators=9, total=   2.9s\n",
      "[CV] min_samples_split=4, n_estimators=9 .............................\n",
      "[CV] .............. min_samples_split=4, n_estimators=9, total=   2.5s\n",
      "[CV] min_samples_split=4, n_estimators=10 ............................\n",
      "[CV] ............. min_samples_split=4, n_estimators=10, total=   2.5s\n",
      "[CV] min_samples_split=4, n_estimators=10 ............................\n",
      "[CV] ............. min_samples_split=4, n_estimators=10, total=   2.5s\n",
      "[CV] min_samples_split=4, n_estimators=10 ............................\n",
      "[CV] ............. min_samples_split=4, n_estimators=10, total=   2.8s\n",
      "[CV] min_samples_split=4, n_estimators=10 ............................\n",
      "[CV] ............. min_samples_split=4, n_estimators=10, total=   3.1s\n",
      "[CV] min_samples_split=4, n_estimators=11 ............................\n",
      "[CV] ............. min_samples_split=4, n_estimators=11, total=   2.9s\n",
      "[CV] min_samples_split=4, n_estimators=11 ............................\n",
      "[CV] ............. min_samples_split=4, n_estimators=11, total=   3.0s\n",
      "[CV] min_samples_split=4, n_estimators=11 ............................\n",
      "[CV] ............. min_samples_split=4, n_estimators=11, total=   2.7s\n",
      "[CV] min_samples_split=4, n_estimators=11 ............................\n",
      "[CV] ............. min_samples_split=4, n_estimators=11, total=   2.7s\n",
      "[CV] min_samples_split=4, n_estimators=18 ............................\n",
      "[CV] ............. min_samples_split=4, n_estimators=18, total=   4.7s\n",
      "[CV] min_samples_split=4, n_estimators=18 ............................\n",
      "[CV] ............. min_samples_split=4, n_estimators=18, total=   4.6s\n",
      "[CV] min_samples_split=4, n_estimators=18 ............................\n",
      "[CV] ............. min_samples_split=4, n_estimators=18, total=   4.9s\n",
      "[CV] min_samples_split=4, n_estimators=18 ............................\n",
      "[CV] ............. min_samples_split=4, n_estimators=18, total=   4.6s\n",
      "[CV] min_samples_split=4, n_estimators=19 ............................\n",
      "[CV] ............. min_samples_split=4, n_estimators=19, total=   4.9s\n",
      "[CV] min_samples_split=4, n_estimators=19 ............................\n",
      "[CV] ............. min_samples_split=4, n_estimators=19, total=   5.0s\n",
      "[CV] min_samples_split=4, n_estimators=19 ............................\n",
      "[CV] ............. min_samples_split=4, n_estimators=19, total=   6.4s\n",
      "[CV] min_samples_split=4, n_estimators=19 ............................\n",
      "[CV] ............. min_samples_split=4, n_estimators=19, total=   5.0s\n",
      "[CV] min_samples_split=4, n_estimators=20 ............................\n",
      "[CV] ............. min_samples_split=4, n_estimators=20, total=   5.0s\n",
      "[CV] min_samples_split=4, n_estimators=20 ............................\n",
      "[CV] ............. min_samples_split=4, n_estimators=20, total=   4.9s\n",
      "[CV] min_samples_split=4, n_estimators=20 ............................\n",
      "[CV] ............. min_samples_split=4, n_estimators=20, total=   6.5s\n",
      "[CV] min_samples_split=4, n_estimators=20 ............................\n",
      "[CV] ............. min_samples_split=4, n_estimators=20, total=   6.9s\n",
      "[CV] min_samples_split=5, n_estimators=8 .............................\n",
      "[CV] .............. min_samples_split=5, n_estimators=8, total=   1.8s\n",
      "[CV] min_samples_split=5, n_estimators=8 .............................\n",
      "[CV] .............. min_samples_split=5, n_estimators=8, total=   1.8s\n",
      "[CV] min_samples_split=5, n_estimators=8 .............................\n",
      "[CV] .............. min_samples_split=5, n_estimators=8, total=   2.1s\n",
      "[CV] min_samples_split=5, n_estimators=8 .............................\n",
      "[CV] .............. min_samples_split=5, n_estimators=8, total=   1.8s\n",
      "[CV] min_samples_split=5, n_estimators=9 .............................\n",
      "[CV] .............. min_samples_split=5, n_estimators=9, total=   2.1s\n",
      "[CV] min_samples_split=5, n_estimators=9 .............................\n",
      "[CV] .............. min_samples_split=5, n_estimators=9, total=   2.0s\n",
      "[CV] min_samples_split=5, n_estimators=9 .............................\n",
      "[CV] .............. min_samples_split=5, n_estimators=9, total=   2.3s\n",
      "[CV] min_samples_split=5, n_estimators=9 .............................\n",
      "[CV] .............. min_samples_split=5, n_estimators=9, total=   2.1s\n",
      "[CV] min_samples_split=5, n_estimators=10 ............................\n",
      "[CV] ............. min_samples_split=5, n_estimators=10, total=   2.5s\n",
      "[CV] min_samples_split=5, n_estimators=10 ............................\n",
      "[CV] ............. min_samples_split=5, n_estimators=10, total=   2.3s\n",
      "[CV] min_samples_split=5, n_estimators=10 ............................\n",
      "[CV] ............. min_samples_split=5, n_estimators=10, total=   2.3s\n",
      "[CV] min_samples_split=5, n_estimators=10 ............................\n",
      "[CV] ............. min_samples_split=5, n_estimators=10, total=   2.3s\n",
      "[CV] min_samples_split=5, n_estimators=11 ............................\n",
      "[CV] ............. min_samples_split=5, n_estimators=11, total=   2.5s\n",
      "[CV] min_samples_split=5, n_estimators=11 ............................\n",
      "[CV] ............. min_samples_split=5, n_estimators=11, total=   2.5s\n",
      "[CV] min_samples_split=5, n_estimators=11 ............................\n",
      "[CV] ............. min_samples_split=5, n_estimators=11, total=   2.2s\n",
      "[CV] min_samples_split=5, n_estimators=11 ............................\n",
      "[CV] ............. min_samples_split=5, n_estimators=11, total=   2.5s\n",
      "[CV] min_samples_split=5, n_estimators=18 ............................\n",
      "[CV] ............. min_samples_split=5, n_estimators=18, total=   4.1s\n",
      "[CV] min_samples_split=5, n_estimators=18 ............................\n",
      "[CV] ............. min_samples_split=5, n_estimators=18, total=   4.2s\n",
      "[CV] min_samples_split=5, n_estimators=18 ............................\n",
      "[CV] ............. min_samples_split=5, n_estimators=18, total=   4.9s\n",
      "[CV] min_samples_split=5, n_estimators=18 ............................\n",
      "[CV] ............. min_samples_split=5, n_estimators=18, total=   5.5s\n",
      "[CV] min_samples_split=5, n_estimators=19 ............................\n",
      "[CV] ............. min_samples_split=5, n_estimators=19, total=   5.5s\n",
      "[CV] min_samples_split=5, n_estimators=19 ............................\n",
      "[CV] ............. min_samples_split=5, n_estimators=19, total=   5.0s\n",
      "[CV] min_samples_split=5, n_estimators=19 ............................\n",
      "[CV] ............. min_samples_split=5, n_estimators=19, total=   4.9s\n",
      "[CV] min_samples_split=5, n_estimators=19 ............................\n",
      "[CV] ............. min_samples_split=5, n_estimators=19, total=   6.1s\n",
      "[CV] min_samples_split=5, n_estimators=20 ............................\n",
      "[CV] ............. min_samples_split=5, n_estimators=20, total=   6.4s\n",
      "[CV] min_samples_split=5, n_estimators=20 ............................\n",
      "[CV] ............. min_samples_split=5, n_estimators=20, total=   6.8s\n",
      "[CV] min_samples_split=5, n_estimators=20 ............................\n",
      "[CV] ............. min_samples_split=5, n_estimators=20, total=   6.5s\n",
      "[CV] min_samples_split=5, n_estimators=20 ............................\n",
      "[CV] ............. min_samples_split=5, n_estimators=20, total=   6.8s\n",
      "[CV] min_samples_split=6, n_estimators=8 .............................\n",
      "[CV] .............. min_samples_split=6, n_estimators=8, total=   2.5s\n",
      "[CV] min_samples_split=6, n_estimators=8 .............................\n",
      "[CV] .............. min_samples_split=6, n_estimators=8, total=   2.2s\n",
      "[CV] min_samples_split=6, n_estimators=8 .............................\n",
      "[CV] .............. min_samples_split=6, n_estimators=8, total=   2.0s\n",
      "[CV] min_samples_split=6, n_estimators=8 .............................\n",
      "[CV] .............. min_samples_split=6, n_estimators=8, total=   2.3s\n",
      "[CV] min_samples_split=6, n_estimators=9 .............................\n",
      "[CV] .............. min_samples_split=6, n_estimators=9, total=   2.6s\n",
      "[CV] min_samples_split=6, n_estimators=9 .............................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .............. min_samples_split=6, n_estimators=9, total=   2.7s\n",
      "[CV] min_samples_split=6, n_estimators=9 .............................\n",
      "[CV] .............. min_samples_split=6, n_estimators=9, total=   2.5s\n",
      "[CV] min_samples_split=6, n_estimators=9 .............................\n",
      "[CV] .............. min_samples_split=6, n_estimators=9, total=   2.5s\n",
      "[CV] min_samples_split=6, n_estimators=10 ............................\n",
      "[CV] ............. min_samples_split=6, n_estimators=10, total=   3.1s\n",
      "[CV] min_samples_split=6, n_estimators=10 ............................\n",
      "[CV] ............. min_samples_split=6, n_estimators=10, total=   3.0s\n",
      "[CV] min_samples_split=6, n_estimators=10 ............................\n",
      "[CV] ............. min_samples_split=6, n_estimators=10, total=   2.7s\n",
      "[CV] min_samples_split=6, n_estimators=10 ............................\n",
      "[CV] ............. min_samples_split=6, n_estimators=10, total=   2.7s\n",
      "[CV] min_samples_split=6, n_estimators=11 ............................\n",
      "[CV] ............. min_samples_split=6, n_estimators=11, total=   3.0s\n",
      "[CV] min_samples_split=6, n_estimators=11 ............................\n",
      "[CV] ............. min_samples_split=6, n_estimators=11, total=   3.1s\n",
      "[CV] min_samples_split=6, n_estimators=11 ............................\n",
      "[CV] ............. min_samples_split=6, n_estimators=11, total=   3.2s\n",
      "[CV] min_samples_split=6, n_estimators=11 ............................\n",
      "[CV] ............. min_samples_split=6, n_estimators=11, total=   3.4s\n",
      "[CV] min_samples_split=6, n_estimators=18 ............................\n",
      "[CV] ............. min_samples_split=6, n_estimators=18, total=   5.6s\n",
      "[CV] min_samples_split=6, n_estimators=18 ............................\n",
      "[CV] ............. min_samples_split=6, n_estimators=18, total=   5.2s\n",
      "[CV] min_samples_split=6, n_estimators=18 ............................\n",
      "[CV] ............. min_samples_split=6, n_estimators=18, total=   5.8s\n",
      "[CV] min_samples_split=6, n_estimators=18 ............................\n",
      "[CV] ............. min_samples_split=6, n_estimators=18, total=   4.8s\n",
      "[CV] min_samples_split=6, n_estimators=19 ............................\n",
      "[CV] ............. min_samples_split=6, n_estimators=19, total=   5.2s\n",
      "[CV] min_samples_split=6, n_estimators=19 ............................\n",
      "[CV] ............. min_samples_split=6, n_estimators=19, total=   5.1s\n",
      "[CV] min_samples_split=6, n_estimators=19 ............................\n",
      "[CV] ............. min_samples_split=6, n_estimators=19, total=   4.7s\n",
      "[CV] min_samples_split=6, n_estimators=19 ............................\n",
      "[CV] ............. min_samples_split=6, n_estimators=19, total=   5.0s\n",
      "[CV] min_samples_split=6, n_estimators=20 ............................\n",
      "[CV] ............. min_samples_split=6, n_estimators=20, total=   4.5s\n",
      "[CV] min_samples_split=6, n_estimators=20 ............................\n",
      "[CV] ............. min_samples_split=6, n_estimators=20, total=   4.4s\n",
      "[CV] min_samples_split=6, n_estimators=20 ............................\n",
      "[CV] ............. min_samples_split=6, n_estimators=20, total=   4.5s\n",
      "[CV] min_samples_split=6, n_estimators=20 ............................\n",
      "[CV] ............. min_samples_split=6, n_estimators=20, total=   4.4s\n",
      "[CV] min_samples_split=15, n_estimators=8 ............................\n",
      "[CV] ............. min_samples_split=15, n_estimators=8, total=   1.2s\n",
      "[CV] min_samples_split=15, n_estimators=8 ............................\n",
      "[CV] ............. min_samples_split=15, n_estimators=8, total=   1.2s\n",
      "[CV] min_samples_split=15, n_estimators=8 ............................\n",
      "[CV] ............. min_samples_split=15, n_estimators=8, total=   1.3s\n",
      "[CV] min_samples_split=15, n_estimators=8 ............................\n",
      "[CV] ............. min_samples_split=15, n_estimators=8, total=   1.4s\n",
      "[CV] min_samples_split=15, n_estimators=9 ............................\n",
      "[CV] ............. min_samples_split=15, n_estimators=9, total=   1.6s\n",
      "[CV] min_samples_split=15, n_estimators=9 ............................\n",
      "[CV] ............. min_samples_split=15, n_estimators=9, total=   1.6s\n",
      "[CV] min_samples_split=15, n_estimators=9 ............................\n",
      "[CV] ............. min_samples_split=15, n_estimators=9, total=   1.8s\n",
      "[CV] min_samples_split=15, n_estimators=9 ............................\n",
      "[CV] ............. min_samples_split=15, n_estimators=9, total=   1.9s\n",
      "[CV] min_samples_split=15, n_estimators=10 ...........................\n",
      "[CV] ............ min_samples_split=15, n_estimators=10, total=   2.1s\n",
      "[CV] min_samples_split=15, n_estimators=10 ...........................\n",
      "[CV] ............ min_samples_split=15, n_estimators=10, total=   1.6s\n",
      "[CV] min_samples_split=15, n_estimators=10 ...........................\n",
      "[CV] ............ min_samples_split=15, n_estimators=10, total=   1.6s\n",
      "[CV] min_samples_split=15, n_estimators=10 ...........................\n",
      "[CV] ............ min_samples_split=15, n_estimators=10, total=   1.6s\n",
      "[CV] min_samples_split=15, n_estimators=11 ...........................\n",
      "[CV] ............ min_samples_split=15, n_estimators=11, total=   2.0s\n",
      "[CV] min_samples_split=15, n_estimators=11 ...........................\n",
      "[CV] ............ min_samples_split=15, n_estimators=11, total=   2.4s\n",
      "[CV] min_samples_split=15, n_estimators=11 ...........................\n",
      "[CV] ............ min_samples_split=15, n_estimators=11, total=   1.8s\n",
      "[CV] min_samples_split=15, n_estimators=11 ...........................\n",
      "[CV] ............ min_samples_split=15, n_estimators=11, total=   2.2s\n",
      "[CV] min_samples_split=15, n_estimators=18 ...........................\n",
      "[CV] ............ min_samples_split=15, n_estimators=18, total=   3.7s\n",
      "[CV] min_samples_split=15, n_estimators=18 ...........................\n",
      "[CV] ............ min_samples_split=15, n_estimators=18, total=   3.6s\n",
      "[CV] min_samples_split=15, n_estimators=18 ...........................\n",
      "[CV] ............ min_samples_split=15, n_estimators=18, total=   3.1s\n",
      "[CV] min_samples_split=15, n_estimators=18 ...........................\n",
      "[CV] ............ min_samples_split=15, n_estimators=18, total=   2.9s\n",
      "[CV] min_samples_split=15, n_estimators=19 ...........................\n",
      "[CV] ............ min_samples_split=15, n_estimators=19, total=   3.0s\n",
      "[CV] min_samples_split=15, n_estimators=19 ...........................\n",
      "[CV] ............ min_samples_split=15, n_estimators=19, total=   2.6s\n",
      "[CV] min_samples_split=15, n_estimators=19 ...........................\n",
      "[CV] ............ min_samples_split=15, n_estimators=19, total=   3.0s\n",
      "[CV] min_samples_split=15, n_estimators=19 ...........................\n",
      "[CV] ............ min_samples_split=15, n_estimators=19, total=   3.1s\n",
      "[CV] min_samples_split=15, n_estimators=20 ...........................\n",
      "[CV] ............ min_samples_split=15, n_estimators=20, total=   3.3s\n",
      "[CV] min_samples_split=15, n_estimators=20 ...........................\n",
      "[CV] ............ min_samples_split=15, n_estimators=20, total=   3.2s\n",
      "[CV] min_samples_split=15, n_estimators=20 ...........................\n",
      "[CV] ............ min_samples_split=15, n_estimators=20, total=   3.4s\n",
      "[CV] min_samples_split=15, n_estimators=20 ...........................\n",
      "[CV] ............ min_samples_split=15, n_estimators=20, total=   3.4s\n",
      "[CV] min_samples_split=16, n_estimators=8 ............................\n",
      "[CV] ............. min_samples_split=16, n_estimators=8, total=   1.3s\n",
      "[CV] min_samples_split=16, n_estimators=8 ............................\n",
      "[CV] ............. min_samples_split=16, n_estimators=8, total=   1.2s\n",
      "[CV] min_samples_split=16, n_estimators=8 ............................\n",
      "[CV] ............. min_samples_split=16, n_estimators=8, total=   1.4s\n",
      "[CV] min_samples_split=16, n_estimators=8 ............................\n",
      "[CV] ............. min_samples_split=16, n_estimators=8, total=   1.8s\n",
      "[CV] min_samples_split=16, n_estimators=9 ............................\n",
      "[CV] ............. min_samples_split=16, n_estimators=9, total=   2.1s\n",
      "[CV] min_samples_split=16, n_estimators=9 ............................\n",
      "[CV] ............. min_samples_split=16, n_estimators=9, total=   1.9s\n",
      "[CV] min_samples_split=16, n_estimators=9 ............................\n",
      "[CV] ............. min_samples_split=16, n_estimators=9, total=   1.8s\n",
      "[CV] min_samples_split=16, n_estimators=9 ............................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............. min_samples_split=16, n_estimators=9, total=   2.0s\n",
      "[CV] min_samples_split=16, n_estimators=10 ...........................\n",
      "[CV] ............ min_samples_split=16, n_estimators=10, total=   2.3s\n",
      "[CV] min_samples_split=16, n_estimators=10 ...........................\n",
      "[CV] ............ min_samples_split=16, n_estimators=10, total=   2.1s\n",
      "[CV] min_samples_split=16, n_estimators=10 ...........................\n",
      "[CV] ............ min_samples_split=16, n_estimators=10, total=   2.0s\n",
      "[CV] min_samples_split=16, n_estimators=10 ...........................\n",
      "[CV] ............ min_samples_split=16, n_estimators=10, total=   1.8s\n",
      "[CV] min_samples_split=16, n_estimators=11 ...........................\n",
      "[CV] ............ min_samples_split=16, n_estimators=11, total=   1.8s\n",
      "[CV] min_samples_split=16, n_estimators=11 ...........................\n",
      "[CV] ............ min_samples_split=16, n_estimators=11, total=   2.2s\n",
      "[CV] min_samples_split=16, n_estimators=11 ...........................\n",
      "[CV] ............ min_samples_split=16, n_estimators=11, total=   1.9s\n",
      "[CV] min_samples_split=16, n_estimators=11 ...........................\n",
      "[CV] ............ min_samples_split=16, n_estimators=11, total=   1.7s\n",
      "[CV] min_samples_split=16, n_estimators=18 ...........................\n",
      "[CV] ............ min_samples_split=16, n_estimators=18, total=   3.0s\n",
      "[CV] min_samples_split=16, n_estimators=18 ...........................\n",
      "[CV] ............ min_samples_split=16, n_estimators=18, total=   2.9s\n",
      "[CV] min_samples_split=16, n_estimators=18 ...........................\n",
      "[CV] ............ min_samples_split=16, n_estimators=18, total=   3.0s\n",
      "[CV] min_samples_split=16, n_estimators=18 ...........................\n",
      "[CV] ............ min_samples_split=16, n_estimators=18, total=   2.8s\n",
      "[CV] min_samples_split=16, n_estimators=19 ...........................\n",
      "[CV] ............ min_samples_split=16, n_estimators=19, total=   3.0s\n",
      "[CV] min_samples_split=16, n_estimators=19 ...........................\n",
      "[CV] ............ min_samples_split=16, n_estimators=19, total=   3.0s\n",
      "[CV] min_samples_split=16, n_estimators=19 ...........................\n",
      "[CV] ............ min_samples_split=16, n_estimators=19, total=   3.0s\n",
      "[CV] min_samples_split=16, n_estimators=19 ...........................\n",
      "[CV] ............ min_samples_split=16, n_estimators=19, total=   3.0s\n",
      "[CV] min_samples_split=16, n_estimators=20 ...........................\n",
      "[CV] ............ min_samples_split=16, n_estimators=20, total=   3.2s\n",
      "[CV] min_samples_split=16, n_estimators=20 ...........................\n",
      "[CV] ............ min_samples_split=16, n_estimators=20, total=   3.2s\n",
      "[CV] min_samples_split=16, n_estimators=20 ...........................\n",
      "[CV] ............ min_samples_split=16, n_estimators=20, total=   3.2s\n",
      "[CV] min_samples_split=16, n_estimators=20 ...........................\n",
      "[CV] ............ min_samples_split=16, n_estimators=20, total=   3.2s\n",
      "[CV] min_samples_split=17, n_estimators=8 ............................\n",
      "[CV] ............. min_samples_split=17, n_estimators=8, total=   1.2s\n",
      "[CV] min_samples_split=17, n_estimators=8 ............................\n",
      "[CV] ............. min_samples_split=17, n_estimators=8, total=   1.2s\n",
      "[CV] min_samples_split=17, n_estimators=8 ............................\n",
      "[CV] ............. min_samples_split=17, n_estimators=8, total=   1.3s\n",
      "[CV] min_samples_split=17, n_estimators=8 ............................\n",
      "[CV] ............. min_samples_split=17, n_estimators=8, total=   1.3s\n",
      "[CV] min_samples_split=17, n_estimators=9 ............................\n",
      "[CV] ............. min_samples_split=17, n_estimators=9, total=   1.3s\n",
      "[CV] min_samples_split=17, n_estimators=9 ............................\n",
      "[CV] ............. min_samples_split=17, n_estimators=9, total=   1.6s\n",
      "[CV] min_samples_split=17, n_estimators=9 ............................\n",
      "[CV] ............. min_samples_split=17, n_estimators=9, total=   1.5s\n",
      "[CV] min_samples_split=17, n_estimators=9 ............................\n",
      "[CV] ............. min_samples_split=17, n_estimators=9, total=   1.4s\n",
      "[CV] min_samples_split=17, n_estimators=10 ...........................\n",
      "[CV] ............ min_samples_split=17, n_estimators=10, total=   1.5s\n",
      "[CV] min_samples_split=17, n_estimators=10 ...........................\n",
      "[CV] ............ min_samples_split=17, n_estimators=10, total=   1.5s\n",
      "[CV] min_samples_split=17, n_estimators=10 ...........................\n",
      "[CV] ............ min_samples_split=17, n_estimators=10, total=   1.6s\n",
      "[CV] min_samples_split=17, n_estimators=10 ...........................\n",
      "[CV] ............ min_samples_split=17, n_estimators=10, total=   1.5s\n",
      "[CV] min_samples_split=17, n_estimators=11 ...........................\n",
      "[CV] ............ min_samples_split=17, n_estimators=11, total=   1.6s\n",
      "[CV] min_samples_split=17, n_estimators=11 ...........................\n",
      "[CV] ............ min_samples_split=17, n_estimators=11, total=   1.6s\n",
      "[CV] min_samples_split=17, n_estimators=11 ...........................\n",
      "[CV] ............ min_samples_split=17, n_estimators=11, total=   1.9s\n",
      "[CV] min_samples_split=17, n_estimators=11 ...........................\n",
      "[CV] ............ min_samples_split=17, n_estimators=11, total=   1.8s\n",
      "[CV] min_samples_split=17, n_estimators=18 ...........................\n",
      "[CV] ............ min_samples_split=17, n_estimators=18, total=   2.8s\n",
      "[CV] min_samples_split=17, n_estimators=18 ...........................\n",
      "[CV] ............ min_samples_split=17, n_estimators=18, total=   2.8s\n",
      "[CV] min_samples_split=17, n_estimators=18 ...........................\n",
      "[CV] ............ min_samples_split=17, n_estimators=18, total=   3.1s\n",
      "[CV] min_samples_split=17, n_estimators=18 ...........................\n",
      "[CV] ............ min_samples_split=17, n_estimators=18, total=   3.1s\n",
      "[CV] min_samples_split=17, n_estimators=19 ...........................\n",
      "[CV] ............ min_samples_split=17, n_estimators=19, total=   4.0s\n",
      "[CV] min_samples_split=17, n_estimators=19 ...........................\n",
      "[CV] ............ min_samples_split=17, n_estimators=19, total=   4.0s\n",
      "[CV] min_samples_split=17, n_estimators=19 ...........................\n",
      "[CV] ............ min_samples_split=17, n_estimators=19, total=   4.1s\n",
      "[CV] min_samples_split=17, n_estimators=19 ...........................\n",
      "[CV] ............ min_samples_split=17, n_estimators=19, total=   4.0s\n",
      "[CV] min_samples_split=17, n_estimators=20 ...........................\n",
      "[CV] ............ min_samples_split=17, n_estimators=20, total=   4.4s\n",
      "[CV] min_samples_split=17, n_estimators=20 ...........................\n",
      "[CV] ............ min_samples_split=17, n_estimators=20, total=   4.5s\n",
      "[CV] min_samples_split=17, n_estimators=20 ...........................\n",
      "[CV] ............ min_samples_split=17, n_estimators=20, total=   3.7s\n",
      "[CV] min_samples_split=17, n_estimators=20 ...........................\n",
      "[CV] ............ min_samples_split=17, n_estimators=20, total=   3.8s\n",
      "[CV] min_samples_split=18, n_estimators=8 ............................\n",
      "[CV] ............. min_samples_split=18, n_estimators=8, total=   1.2s\n",
      "[CV] min_samples_split=18, n_estimators=8 ............................\n",
      "[CV] ............. min_samples_split=18, n_estimators=8, total=   1.2s\n",
      "[CV] min_samples_split=18, n_estimators=8 ............................\n",
      "[CV] ............. min_samples_split=18, n_estimators=8, total=   1.2s\n",
      "[CV] min_samples_split=18, n_estimators=8 ............................\n",
      "[CV] ............. min_samples_split=18, n_estimators=8, total=   1.2s\n",
      "[CV] min_samples_split=18, n_estimators=9 ............................\n",
      "[CV] ............. min_samples_split=18, n_estimators=9, total=   1.6s\n",
      "[CV] min_samples_split=18, n_estimators=9 ............................\n",
      "[CV] ............. min_samples_split=18, n_estimators=9, total=   1.5s\n",
      "[CV] min_samples_split=18, n_estimators=9 ............................\n",
      "[CV] ............. min_samples_split=18, n_estimators=9, total=   1.6s\n",
      "[CV] min_samples_split=18, n_estimators=9 ............................\n",
      "[CV] ............. min_samples_split=18, n_estimators=9, total=   1.7s\n",
      "[CV] min_samples_split=18, n_estimators=10 ...........................\n",
      "[CV] ............ min_samples_split=18, n_estimators=10, total=   2.2s\n",
      "[CV] min_samples_split=18, n_estimators=10 ...........................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............ min_samples_split=18, n_estimators=10, total=   2.0s\n",
      "[CV] min_samples_split=18, n_estimators=10 ...........................\n",
      "[CV] ............ min_samples_split=18, n_estimators=10, total=   1.5s\n",
      "[CV] min_samples_split=18, n_estimators=10 ...........................\n",
      "[CV] ............ min_samples_split=18, n_estimators=10, total=   1.6s\n",
      "[CV] min_samples_split=18, n_estimators=11 ...........................\n",
      "[CV] ............ min_samples_split=18, n_estimators=11, total=   1.6s\n",
      "[CV] min_samples_split=18, n_estimators=11 ...........................\n",
      "[CV] ............ min_samples_split=18, n_estimators=11, total=   1.6s\n",
      "[CV] min_samples_split=18, n_estimators=11 ...........................\n",
      "[CV] ............ min_samples_split=18, n_estimators=11, total=   1.6s\n",
      "[CV] min_samples_split=18, n_estimators=11 ...........................\n",
      "[CV] ............ min_samples_split=18, n_estimators=11, total=   1.7s\n",
      "[CV] min_samples_split=18, n_estimators=18 ...........................\n",
      "[CV] ............ min_samples_split=18, n_estimators=18, total=   2.7s\n",
      "[CV] min_samples_split=18, n_estimators=18 ...........................\n",
      "[CV] ............ min_samples_split=18, n_estimators=18, total=   3.2s\n",
      "[CV] min_samples_split=18, n_estimators=18 ...........................\n",
      "[CV] ............ min_samples_split=18, n_estimators=18, total=   3.0s\n",
      "[CV] min_samples_split=18, n_estimators=18 ...........................\n",
      "[CV] ............ min_samples_split=18, n_estimators=18, total=   2.7s\n",
      "[CV] min_samples_split=18, n_estimators=19 ...........................\n",
      "[CV] ............ min_samples_split=18, n_estimators=19, total=   2.9s\n",
      "[CV] min_samples_split=18, n_estimators=19 ...........................\n",
      "[CV] ............ min_samples_split=18, n_estimators=19, total=   3.5s\n",
      "[CV] min_samples_split=18, n_estimators=19 ...........................\n",
      "[CV] ............ min_samples_split=18, n_estimators=19, total=   3.5s\n",
      "[CV] min_samples_split=18, n_estimators=19 ...........................\n",
      "[CV] ............ min_samples_split=18, n_estimators=19, total=   3.0s\n",
      "[CV] min_samples_split=18, n_estimators=20 ...........................\n",
      "[CV] ............ min_samples_split=18, n_estimators=20, total=   3.5s\n",
      "[CV] min_samples_split=18, n_estimators=20 ...........................\n",
      "[CV] ............ min_samples_split=18, n_estimators=20, total=   3.1s\n",
      "[CV] min_samples_split=18, n_estimators=20 ...........................\n",
      "[CV] ............ min_samples_split=18, n_estimators=20, total=   3.7s\n",
      "[CV] min_samples_split=18, n_estimators=20 ...........................\n",
      "[CV] ............ min_samples_split=18, n_estimators=20, total=   3.1s\n",
      "[CV] min_samples_split=19, n_estimators=8 ............................\n",
      "[CV] ............. min_samples_split=19, n_estimators=8, total=   1.1s\n",
      "[CV] min_samples_split=19, n_estimators=8 ............................\n",
      "[CV] ............. min_samples_split=19, n_estimators=8, total=   1.1s\n",
      "[CV] min_samples_split=19, n_estimators=8 ............................\n",
      "[CV] ............. min_samples_split=19, n_estimators=8, total=   1.3s\n",
      "[CV] min_samples_split=19, n_estimators=8 ............................\n",
      "[CV] ............. min_samples_split=19, n_estimators=8, total=   1.2s\n",
      "[CV] min_samples_split=19, n_estimators=9 ............................\n",
      "[CV] ............. min_samples_split=19, n_estimators=9, total=   1.3s\n",
      "[CV] min_samples_split=19, n_estimators=9 ............................\n",
      "[CV] ............. min_samples_split=19, n_estimators=9, total=   1.3s\n",
      "[CV] min_samples_split=19, n_estimators=9 ............................\n",
      "[CV] ............. min_samples_split=19, n_estimators=9, total=   1.3s\n",
      "[CV] min_samples_split=19, n_estimators=9 ............................\n",
      "[CV] ............. min_samples_split=19, n_estimators=9, total=   1.3s\n",
      "[CV] min_samples_split=19, n_estimators=10 ...........................\n",
      "[CV] ............ min_samples_split=19, n_estimators=10, total=   1.5s\n",
      "[CV] min_samples_split=19, n_estimators=10 ...........................\n",
      "[CV] ............ min_samples_split=19, n_estimators=10, total=   1.5s\n",
      "[CV] min_samples_split=19, n_estimators=10 ...........................\n",
      "[CV] ............ min_samples_split=19, n_estimators=10, total=   1.7s\n",
      "[CV] min_samples_split=19, n_estimators=10 ...........................\n",
      "[CV] ............ min_samples_split=19, n_estimators=10, total=   1.6s\n",
      "[CV] min_samples_split=19, n_estimators=11 ...........................\n",
      "[CV] ............ min_samples_split=19, n_estimators=11, total=   1.6s\n",
      "[CV] min_samples_split=19, n_estimators=11 ...........................\n",
      "[CV] ............ min_samples_split=19, n_estimators=11, total=   1.6s\n",
      "[CV] min_samples_split=19, n_estimators=11 ...........................\n",
      "[CV] ............ min_samples_split=19, n_estimators=11, total=   1.6s\n",
      "[CV] min_samples_split=19, n_estimators=11 ...........................\n",
      "[CV] ............ min_samples_split=19, n_estimators=11, total=   1.7s\n",
      "[CV] min_samples_split=19, n_estimators=18 ...........................\n",
      "[CV] ............ min_samples_split=19, n_estimators=18, total=   2.9s\n",
      "[CV] min_samples_split=19, n_estimators=18 ...........................\n",
      "[CV] ............ min_samples_split=19, n_estimators=18, total=   2.9s\n",
      "[CV] min_samples_split=19, n_estimators=18 ...........................\n",
      "[CV] ............ min_samples_split=19, n_estimators=18, total=   2.8s\n",
      "[CV] min_samples_split=19, n_estimators=18 ...........................\n",
      "[CV] ............ min_samples_split=19, n_estimators=18, total=   2.8s\n",
      "[CV] min_samples_split=19, n_estimators=19 ...........................\n",
      "[CV] ............ min_samples_split=19, n_estimators=19, total=   2.9s\n",
      "[CV] min_samples_split=19, n_estimators=19 ...........................\n",
      "[CV] ............ min_samples_split=19, n_estimators=19, total=   2.8s\n",
      "[CV] min_samples_split=19, n_estimators=19 ...........................\n",
      "[CV] ............ min_samples_split=19, n_estimators=19, total=   2.8s\n",
      "[CV] min_samples_split=19, n_estimators=19 ...........................\n",
      "[CV] ............ min_samples_split=19, n_estimators=19, total=   2.8s\n",
      "[CV] min_samples_split=19, n_estimators=20 ...........................\n",
      "[CV] ............ min_samples_split=19, n_estimators=20, total=   3.1s\n",
      "[CV] min_samples_split=19, n_estimators=20 ...........................\n",
      "[CV] ............ min_samples_split=19, n_estimators=20, total=   3.3s\n",
      "[CV] min_samples_split=19, n_estimators=20 ...........................\n",
      "[CV] ............ min_samples_split=19, n_estimators=20, total=   3.0s\n",
      "[CV] min_samples_split=19, n_estimators=20 ...........................\n",
      "[CV] ............ min_samples_split=19, n_estimators=20, total=   3.0s\n",
      "[CV] min_samples_split=20, n_estimators=8 ............................\n",
      "[CV] ............. min_samples_split=20, n_estimators=8, total=   1.1s\n",
      "[CV] min_samples_split=20, n_estimators=8 ............................\n",
      "[CV] ............. min_samples_split=20, n_estimators=8, total=   1.1s\n",
      "[CV] min_samples_split=20, n_estimators=8 ............................\n",
      "[CV] ............. min_samples_split=20, n_estimators=8, total=   1.2s\n",
      "[CV] min_samples_split=20, n_estimators=8 ............................\n",
      "[CV] ............. min_samples_split=20, n_estimators=8, total=   1.4s\n",
      "[CV] min_samples_split=20, n_estimators=9 ............................\n",
      "[CV] ............. min_samples_split=20, n_estimators=9, total=   1.7s\n",
      "[CV] min_samples_split=20, n_estimators=9 ............................\n",
      "[CV] ............. min_samples_split=20, n_estimators=9, total=   1.7s\n",
      "[CV] min_samples_split=20, n_estimators=9 ............................\n",
      "[CV] ............. min_samples_split=20, n_estimators=9, total=   1.7s\n",
      "[CV] min_samples_split=20, n_estimators=9 ............................\n",
      "[CV] ............. min_samples_split=20, n_estimators=9, total=   1.7s\n",
      "[CV] min_samples_split=20, n_estimators=10 ...........................\n",
      "[CV] ............ min_samples_split=20, n_estimators=10, total=   1.9s\n",
      "[CV] min_samples_split=20, n_estimators=10 ...........................\n",
      "[CV] ............ min_samples_split=20, n_estimators=10, total=   1.4s\n",
      "[CV] min_samples_split=20, n_estimators=10 ...........................\n",
      "[CV] ............ min_samples_split=20, n_estimators=10, total=   1.7s\n",
      "[CV] min_samples_split=20, n_estimators=10 ...........................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............ min_samples_split=20, n_estimators=10, total=   1.4s\n",
      "[CV] min_samples_split=20, n_estimators=11 ...........................\n",
      "[CV] ............ min_samples_split=20, n_estimators=11, total=   1.6s\n",
      "[CV] min_samples_split=20, n_estimators=11 ...........................\n",
      "[CV] ............ min_samples_split=20, n_estimators=11, total=   1.6s\n",
      "[CV] min_samples_split=20, n_estimators=11 ...........................\n",
      "[CV] ............ min_samples_split=20, n_estimators=11, total=   1.6s\n",
      "[CV] min_samples_split=20, n_estimators=11 ...........................\n",
      "[CV] ............ min_samples_split=20, n_estimators=11, total=   1.6s\n",
      "[CV] min_samples_split=20, n_estimators=18 ...........................\n",
      "[CV] ............ min_samples_split=20, n_estimators=18, total=   2.6s\n",
      "[CV] min_samples_split=20, n_estimators=18 ...........................\n",
      "[CV] ............ min_samples_split=20, n_estimators=18, total=   2.6s\n",
      "[CV] min_samples_split=20, n_estimators=18 ...........................\n",
      "[CV] ............ min_samples_split=20, n_estimators=18, total=   2.6s\n",
      "[CV] min_samples_split=20, n_estimators=18 ...........................\n",
      "[CV] ............ min_samples_split=20, n_estimators=18, total=   2.3s\n",
      "[CV] min_samples_split=20, n_estimators=19 ...........................\n",
      "[CV] ............ min_samples_split=20, n_estimators=19, total=   3.0s\n",
      "[CV] min_samples_split=20, n_estimators=19 ...........................\n",
      "[CV] ............ min_samples_split=20, n_estimators=19, total=   3.3s\n",
      "[CV] min_samples_split=20, n_estimators=19 ...........................\n",
      "[CV] ............ min_samples_split=20, n_estimators=19, total=   3.3s\n",
      "[CV] min_samples_split=20, n_estimators=19 ...........................\n",
      "[CV] ............ min_samples_split=20, n_estimators=19, total=   4.1s\n",
      "[CV] min_samples_split=20, n_estimators=20 ...........................\n",
      "[CV] ............ min_samples_split=20, n_estimators=20, total=   3.4s\n",
      "[CV] min_samples_split=20, n_estimators=20 ...........................\n",
      "[CV] ............ min_samples_split=20, n_estimators=20, total=   3.5s\n",
      "[CV] min_samples_split=20, n_estimators=20 ...........................\n",
      "[CV] ............ min_samples_split=20, n_estimators=20, total=   3.2s\n",
      "[CV] min_samples_split=20, n_estimators=20 ...........................\n",
      "[CV] ............ min_samples_split=20, n_estimators=20, total=   3.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 308 out of 308 | elapsed: 17.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=4, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'min_samples_split': [2, 3, 4, 5, 6, 15, 16, 17, 18, 19, 20], 'n_estimators': [8, 9, 10, 11, 18, 19, 20]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=2)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_v = clfs['rf_fsn_3'].vectorizer.transform(X_train)\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "rf_grid = GridSearchCV(estimator = rf, param_grid = params_grid, cv = 4, verbose=2)\n",
    "\n",
    "rf_grid.fit(X_train_v, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'min_samples_split': 4, 'n_estimators': 19}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores on train\n",
      "accuracy = 0.9847715736040609\n",
      "f1_all = [0.95454545 0.98726542 0.94588235 0.5        0.98855835 0.8\n",
      " 0.98114075 0.98839907 1.         1.         1.         0.99267399\n",
      " 0.98543689 0.98489426 1.         1.         0.98787879 1.\n",
      " 1.         0.98261126 0.97802198 0.98757764 0.98969072 0.8\n",
      " 0.99      ]\n",
      "f1_micro = 0.9847715736040609\n",
      "f1_macro = 0.9529830773282516\n",
      "f1_weighted = 0.9846670540750933\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anush\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anush\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores on dev\n",
      "accuracy = 0.7438825448613376\n",
      "f1_all = [0.         0.82704126 0.46601942 0.         0.77904328 0.\n",
      " 0.72519084 0.7027027  0.         0.22222222 0.         0.70440252\n",
      " 0.67826087 0.52054795 0.         0.         0.76506024 0.\n",
      " 0.         0.83534137 0.53521127 0.6910299  0.72747497 0.\n",
      " 0.69856459]\n",
      "f1_micro = 0.7438825448613375\n",
      "f1_macro = 0.44900515440430594\n",
      "f1_weighted = 0.7379200553674303\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clfs['rf_fsn_3_gridsearch'] = Classifier('rf_fsn_3_gridsearch', description=\"random forest on first name and surname trigrams, min_samples_split=10\")\n",
    "clfs['rf_fsn_3_gridsearch'].fit(X_train, y_train, RandomForestClassifier().set_params(**rf_grid.best_params_), vectorizer=CountVectorizer(analyzer=lambda x:get_ngrams(x,3)), metrics_dict=metrics_dict)\n",
    "clfs['rf_fsn_3_gridsearch'].score(X_dev, y_dev, 'dev', metrics_dict=metrics_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anush\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores on train\n",
      "accuracy = 0.25761421319796957\n",
      "f1_all = [0.         0.         0.         0.         0.         0.\n",
      " 0.32788779 0.         0.         0.         0.         0.\n",
      " 0.         0.         1.         0.         0.         1.\n",
      " 1.         0.61494045 0.         0.         0.         0.\n",
      " 0.        ]\n",
      "f1_micro = 0.25761421319796957\n",
      "f1_macro = 0.15771312973953477\n",
      "f1_weighted = 0.15882208342633414\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anush\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores on dev\n",
      "accuracy = 0.2544861337683524\n",
      "f1_all = [0.         0.         0.         0.         0.         0.\n",
      " 0.33325189 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.62561576 0.         0.         0.         0.\n",
      " 0.        ]\n",
      "f1_micro = 0.2544861337683524\n",
      "f1_macro = 0.045660364620165025\n",
      "f1_weighted = 0.1497271751712573\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clfs['ab_fsn_3'] = Classifier('ab_fsn_3', description=\"Adaboost on first name and surname trigrams\")\n",
    "clfs['ab_fsn_3'].fit(X_train, y_train, AdaBoostClassifier(), vectorizer=CountVectorizer(analyzer=lambda x:get_ngrams(x,3)), metrics_dict=metrics_dict)\n",
    "clfs['ab_fsn_3'].score(X_dev, y_dev, 'dev', metrics_dict=metrics_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anush\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores on train\n",
      "accuracy = 0.25761421319796957\n",
      "f1_all = [0.         0.         0.         0.         0.         0.\n",
      " 0.32788779 0.         0.         0.         0.         0.\n",
      " 0.         0.         1.         0.         0.         1.\n",
      " 1.         0.61494045 0.         0.         0.         0.\n",
      " 0.        ]\n",
      "f1_micro = 0.25761421319796957\n",
      "f1_macro = 0.15771312973953477\n",
      "f1_weighted = 0.15882208342633414\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anush\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores on dev\n",
      "accuracy = 0.2544861337683524\n",
      "f1_all = [0.         0.         0.         0.         0.         0.\n",
      " 0.33325189 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.62561576 0.         0.         0.         0.\n",
      " 0.        ]\n",
      "f1_micro = 0.2544861337683524\n",
      "f1_macro = 0.045660364620165025\n",
      "f1_weighted = 0.1497271751712573\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clfs['ab_fsn_23'] = Classifier('ab_fsn_23', description=\"Adaboost on first name and surname bigrams and trigrams\")\n",
    "clfs['ab_fsn_23'].fit(X_train, y_train, AdaBoostClassifier(), vectorizer=CountVectorizer(analyzer=lambda x:get_ngrams(x,2) + get_ngrams(x,3)), metrics_dict=metrics_dict)\n",
    "clfs['ab_fsn_23'].score(X_dev, y_dev, 'dev', metrics_dict=metrics_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default AdaBoost classifier does very poorly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hindi script gives similar performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anush\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anush\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores on train\n",
      "accuracy = 0.9006526468455403\n",
      "f1_all = [0.93023256 0.92439185 0.69767442 0.         0.91522881 0.8\n",
      " 0.87581395 0.91044776 0.         0.83870968 0.76923077 0.95849057\n",
      " 0.90773067 0.86524823 1.         0.8        0.89378758 1.\n",
      " 1.         0.91264368 0.88505747 0.9092827  0.90542056 0.8\n",
      " 0.92699491]\n",
      "f1_micro = 0.9006526468455401\n",
      "f1_macro = 0.8170554460567466\n",
      "f1_weighted = 0.8993465545561002\n",
      "\n",
      "Scores on dev\n",
      "accuracy = 0.7737901033170201\n",
      "f1_all = [0.25       0.83668122 0.56363636 0.         0.78587196 0.\n",
      " 0.76694642 0.80254777 0.         0.22222222 0.         0.75496689\n",
      " 0.71681416 0.69677419 0.         0.         0.77710843 0.\n",
      " 0.         0.85714286 0.47619048 0.67375887 0.75141884 0.\n",
      " 0.7037037 ]\n",
      "f1_micro = 0.77379010331702\n",
      "f1_macro = 0.5064659228329715\n",
      "f1_weighted = 0.7688277373721886\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clfs['lr_fsn_3'] = Classifier('lr_fsn_3', description=\"logistic regression on first name and surname trigrams\")\n",
    "clfs['lr_fsn_3'].fit(X_train, y_train, LogisticRegression(), vectorizer=CountVectorizer(analyzer=lambda x:get_ngrams(x,3)), metrics_dict=metrics_dict)\n",
    "clfs['lr_fsn_3'].score(X_dev, y_dev, 'dev', metrics_dict=metrics_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anush\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores on train\n",
      "accuracy = 0.9195068890500363\n",
      "f1_all = [0.97777778 0.94420601 0.75482094 0.         0.92192192 0.90909091\n",
      " 0.89427108 0.92682927 1.         0.94444444 0.85714286 0.96629213\n",
      " 0.93742331 0.89001009 1.         0.875      0.91508492 1.\n",
      " 1.         0.92401392 0.94382022 0.94129979 0.92911011 1.\n",
      " 0.93918919]\n",
      "f1_micro = 0.9195068890500363\n",
      "f1_macro = 0.895669955375581\n",
      "f1_weighted = 0.9186783405114948\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anush\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores on dev\n",
      "accuracy = 0.7827623708537248\n",
      "f1_all = [0.44444444 0.84628975 0.61016949 0.         0.79603087 0.\n",
      " 0.78534031 0.78527607 0.         0.22222222 0.         0.78709677\n",
      " 0.73640167 0.69508197 0.         0.         0.79761905 0.\n",
      " 0.         0.85856574 0.44067797 0.65263158 0.75822928 0.\n",
      " 0.7037037 ]\n",
      "f1_micro = 0.7827623708537249\n",
      "f1_macro = 0.5199895668083472\n",
      "f1_weighted = 0.7781565041344567\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clfs['lr_fsn_23'] = Classifier('lr_fsn_23', description=\"logistic regression on first name and surname bigrams and trigrams\")\n",
    "clfs['lr_fsn_23'].fit(X_train, y_train, LogisticRegression(), vectorizer=CountVectorizer(analyzer=lambda x:get_ngrams(x,2) + get_ngrams(x,3)), metrics_dict=metrics_dict)\n",
    "clfs['lr_fsn_23'].score(X_dev, y_dev, 'dev', metrics_dict=metrics_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Significant improvement in dev scores when using Hindi script (about 3%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anush\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores on train\n",
      "accuracy = 0.9689992748368383\n",
      "f1_all = [1.         0.97923644 0.9451074  0.         0.97299353 1.\n",
      " 0.95187658 0.99074074 1.         1.         0.93333333 0.99082569\n",
      " 0.97799511 0.95381526 1.         1.         0.96871847 1.\n",
      " 1.         0.9664311  0.99459459 0.99171843 0.97322112 1.\n",
      " 0.97      ]\n",
      "f1_micro = 0.9689992748368383\n",
      "f1_macro = 0.9424243113967435\n",
      "f1_weighted = 0.9688338581608651\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anush\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores on dev\n",
      "accuracy = 0.7669929309407286\n",
      "f1_all = [0.72727273 0.82249561 0.58394161 0.         0.7783903  0.\n",
      " 0.75703325 0.81176471 0.         0.4        0.66666667 0.72258065\n",
      " 0.74688797 0.64       0.         0.         0.77151335 0.\n",
      " 0.         0.84662577 0.47761194 0.70547945 0.75761267 0.\n",
      " 0.70046083]\n",
      "f1_micro = 0.7669929309407286\n",
      "f1_macro = 0.5674446418587048\n",
      "f1_weighted = 0.7641049300048491\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clfs['svml_fsn_3'] = Classifier('svml_fsn_3', description=\"SVM with linear kernel on first name and surname trigrams\")\n",
    "clfs['svml_fsn_3'].fit(X_train, y_train, SVC(kernel='linear'), vectorizer=CountVectorizer(analyzer=lambda x:get_ngrams(x,3)), metrics_dict=metrics_dict)\n",
    "clfs['svml_fsn_3'].score(X_dev, y_dev, 'dev', metrics_dict=metrics_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores on train\n",
      "accuracy = 0.9796047860768673\n",
      "f1_all = [1.         0.98727395 0.97209302 0.8        0.98434517 1.\n",
      " 0.96383322 0.99770115 1.         0.97297297 1.         0.996337\n",
      " 0.99151515 0.97487437 1.         1.         0.98580122 1.\n",
      " 1.         0.97230407 0.99459459 0.99585921 0.98466258 1.\n",
      " 0.98      ]\n",
      "f1_micro = 0.9796047860768673\n",
      "f1_macro = 0.9821667066430726\n",
      "f1_weighted = 0.9795892899162478\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anush\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anush\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores on dev\n",
      "accuracy = 0.7626427406199021\n",
      "f1_all = [0.6        0.83079678 0.60273973 0.5        0.76756757 0.\n",
      " 0.76485788 0.77011494 0.         0.4        0.66666667 0.78527607\n",
      " 0.70445344 0.62666667 0.         0.         0.75301205 0.\n",
      " 0.         0.83943089 0.48484848 0.68292683 0.73762376 0.\n",
      " 0.71428571]\n",
      "f1_micro = 0.762642740619902\n",
      "f1_macro = 0.5824413083748131\n",
      "f1_weighted = 0.7600692488417674\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clfs['svml_fsn_23'] = Classifier('svlm_fsn_23', description=\"SVM with linear kernel on first name and surname bigrams and trigrams\")\n",
    "clfs['svml_fsn_23'].fit(X_train, y_train, SVC(kernel='linear'), vectorizer=CountVectorizer(analyzer=lambda x:get_ngrams(x,2) + get_ngrams(x,3)), metrics_dict=metrics_dict)\n",
    "clfs['svml_fsn_23'].score(X_dev, y_dev, 'dev', metrics_dict=metrics_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Hindi scripts gives approximately a 2% performance improvement for SVMs with Linear Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anush\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores on train\n",
      "accuracy = 0.7572516316171138\n",
      "f1_all = [0.52941176 0.82551595 0.56923077 0.         0.73979758 0.66666667\n",
      " 0.75659317 0.69041096 0.         0.64285714 0.54545455 0.71331828\n",
      " 0.68238558 0.61371841 0.         0.         0.69745958 0.\n",
      " 0.         0.80904818 0.55555556 0.76129032 0.77137014 0.\n",
      " 0.80074488]\n",
      "f1_micro = 0.757251631617114\n",
      "f1_macro = 0.4948331793246627\n",
      "f1_weighted = 0.7531787822031586\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anush\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores on dev\n",
      "accuracy = 0.6283306144643828\n",
      "f1_all = [0.         0.74891399 0.47540984 0.         0.60688406 0.\n",
      " 0.67518248 0.52777778 0.         0.2        0.         0.54961832\n",
      " 0.50961538 0.39370079 0.         0.         0.51178451 0.\n",
      " 0.         0.68924303 0.25531915 0.54681648 0.58167331 0.\n",
      " 0.62686567]\n",
      "f1_micro = 0.6283306144643828\n",
      "f1_macro = 0.3761335609740514\n",
      "f1_weighted = 0.6174132457814512\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clfs['knn_fsn_3'] = Classifier('knn_fsn_3', description=\"kNN on first name and surname trigrams\")\n",
    "clfs['knn_fsn_3'].fit(X_train, y_train, KNeighborsClassifier(), vectorizer=CountVectorizer(analyzer=lambda x:get_ngrams(x,3)), metrics_dict=metrics_dict)\n",
    "clfs['knn_fsn_3'].score(X_dev, y_dev, 'dev', metrics_dict=metrics_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anush\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores on train\n",
      "accuracy = 0.7687635968092821\n",
      "f1_all = [0.61111111 0.83844709 0.58375635 0.         0.77219546 0.66666667\n",
      " 0.7602843  0.68698061 0.         0.62068966 0.54545455 0.69603524\n",
      " 0.67934783 0.61395349 0.         0.         0.72278339 0.\n",
      " 0.         0.82905484 0.59722222 0.76273023 0.7717303  0.\n",
      " 0.81560284]\n",
      "f1_micro = 0.7687635968092821\n",
      "f1_macro = 0.5029618461509006\n",
      "f1_weighted = 0.764358164377214\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anush\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores on dev\n",
      "accuracy = 0.6446438281674823\n",
      "f1_all = [0.         0.7624683  0.37795276 0.         0.63907603 0.\n",
      " 0.66584615 0.57142857 0.         0.2        0.66666667 0.58394161\n",
      " 0.53811659 0.38554217 0.         0.         0.51351351 0.\n",
      " 0.         0.72382851 0.13953488 0.58113208 0.61953728 0.\n",
      " 0.66981132]\n",
      "f1_micro = 0.6446438281674823\n",
      "f1_macro = 0.41135221108807457\n",
      "f1_weighted = 0.6329370249301715\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clfs['knn_fsn_23'] = Classifier('knn_fsn_23', description=\"kNN on first name and surname bigrams and trigrams\")\n",
    "clfs['knn_fsn_23'].fit(X_train, y_train, KNeighborsClassifier(), vectorizer=CountVectorizer(analyzer=lambda x:get_ngrams(x,2) + get_ngrams(x,3)), metrics_dict=metrics_dict)\n",
    "clfs['knn_fsn_23'].score(X_dev, y_dev, 'dev', metrics_dict=metrics_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Hindi scripts gives similar performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying out embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anush\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Embedding, Input, GRU\n",
    "from keras.models import Model\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_vocab = ord('ॿ') - ord('ऀ')  +1\n",
    "n_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reference site: https://www.utf8-chartable.de/unicode-utf8-table.pl?start=2304&number=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get label/id for a character\n",
    "def get_char_val(ch, n_vocab=n_vocab):\n",
    "    n = ord(ch) - ord('ऀ')\n",
    "    if n>=n_vocab or n<0:\n",
    "        print(ch)\n",
    "        n = n_vocab\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get np array of labels/ids corresponding to characters in a name\n",
    "def get_name_val(name):\n",
    "    return np.array([get_char_val(c) for c in name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert sequence of names to sequence of label arrays\n",
    "def get_names_vals(names):\n",
    "    return np.array([get_name_val(name) for name in names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 5 #embedding vector dimensions for a character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = df['Community'].unique().shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN over characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many to one sequence model\n",
    "\n",
    "With varying sequence lengths (stochastic / 1 name per batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Over last names only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_generator(X,y):\n",
    "    while True:\n",
    "        for x_i, y_i in zip(X,y):\n",
    "            yield (get_name_val(x_i).reshape(1,-1), to_categorical(y_i, num_classes=n_classes).reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_predict(names, model, argmax=True, x_processor=None):\n",
    "    if x_processor is None:\n",
    "        x_processor = lambda x: get_name_val(x).reshape(1,-1)\n",
    "        \n",
    "    predictions = []\n",
    "    for name in names:\n",
    "        result = model.predict( x_processor(name) )\n",
    "        if argmax:\n",
    "            result = np.argmax(result)\n",
    "        predictions += [result]\n",
    "    return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_dev, y_train, y_dev = train_test_split(df['hindi_Surname'][surnames_mask].values, labels[surnames_mask], random_state = RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(batch_shape=(1,None,))\n",
    "x = Embedding(n_vocab, EMBEDDING_DIM, trainable=True)(inp)\n",
    "x = GRU(64)(x)\n",
    "out = Dense(n_classes, activation='softmax')(x)\n",
    "model = Model(inputs=inp, outputs=out)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (1, None)                 0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (1, None, 5)              640       \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (1, 64)                   13440     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (1, 25)                   1625      \n",
      "=================================================================\n",
      "Total params: 15,705\n",
      "Trainable params: 15,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "11180/11180 [==============================] - 51s 5ms/step - loss: 2.0630 - acc: 0.3470 - val_loss: 1.7831 - val_acc: 0.4457\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.78312, saving model to hindi_checkpoints\\model_rnn_lastname.hdf5\n",
      "Epoch 2/20\n",
      "11180/11180 [==============================] - 54s 5ms/step - loss: 1.6042 - acc: 0.5208 - val_loss: 1.5074 - val_acc: 0.5707\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.78312 to 1.50736, saving model to hindi_checkpoints\\model_rnn_lastname.hdf5\n",
      "Epoch 3/20\n",
      "11180/11180 [==============================] - 39s 4ms/step - loss: 1.3586 - acc: 0.6021 - val_loss: 1.3726 - val_acc: 0.6101\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.50736 to 1.37258, saving model to hindi_checkpoints\\model_rnn_lastname.hdf5\n",
      "Epoch 4/20\n",
      "11180/11180 [==============================] - 35s 3ms/step - loss: 1.2156 - acc: 0.6389 - val_loss: 1.3116 - val_acc: 0.6230\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.37258 to 1.31156, saving model to hindi_checkpoints\\model_rnn_lastname.hdf5\n",
      "Epoch 5/20\n",
      "11180/11180 [==============================] - 35s 3ms/step - loss: 1.1217 - acc: 0.6655 - val_loss: 1.2812 - val_acc: 0.6338\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.31156 to 1.28121, saving model to hindi_checkpoints\\model_rnn_lastname.hdf5\n",
      "Epoch 6/20\n",
      "11180/11180 [==============================] - 35s 3ms/step - loss: 1.0478 - acc: 0.6852 - val_loss: 1.2595 - val_acc: 0.6437\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.28121 to 1.25952, saving model to hindi_checkpoints\\model_rnn_lastname.hdf5\n",
      "Epoch 7/20\n",
      "11180/11180 [==============================] - 36s 3ms/step - loss: 0.9904 - acc: 0.6971 - val_loss: 1.2685 - val_acc: 0.6466\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.25952\n",
      "Epoch 8/20\n",
      "11180/11180 [==============================] - 36s 3ms/step - loss: 0.9444 - acc: 0.7055 - val_loss: 1.2768 - val_acc: 0.6466\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.25952\n",
      "Epoch 9/20\n",
      "11180/11180 [==============================] - 32s 3ms/step - loss: 0.9134 - acc: 0.7122 - val_loss: 1.2591 - val_acc: 0.6568\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.25952 to 1.25909, saving model to hindi_checkpoints\\model_rnn_lastname.hdf5\n",
      "Epoch 10/20\n",
      "11180/11180 [==============================] - 31s 3ms/step - loss: 0.8841 - acc: 0.7192 - val_loss: 1.2654 - val_acc: 0.6531\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.25909\n",
      "Epoch 11/20\n",
      "11180/11180 [==============================] - 33s 3ms/step - loss: 0.8623 - acc: 0.7278 - val_loss: 1.2696 - val_acc: 0.6464\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.25909\n",
      "Epoch 12/20\n",
      "11180/11180 [==============================] - 61s 5ms/step - loss: 0.8451 - acc: 0.7309 - val_loss: 1.2843 - val_acc: 0.6579\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.25909\n",
      "Epoch 13/20\n",
      "11180/11180 [==============================] - 41s 4ms/step - loss: 0.8340 - acc: 0.7296 - val_loss: 1.2994 - val_acc: 0.6539\n",
      "Restoring model weights from the end of the best epoch\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.25909\n",
      "Epoch 00013: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17aa4caed68>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_point = ModelCheckpoint('hindi_checkpoints\\\\model_rnn_lastname.hdf5', verbose=True, save_best_only=True)\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=4, verbose=True, restore_best_weights=True)\n",
    "model.fit_generator(rnn_generator(X_train,y_train), validation_data=rnn_generator(X_dev,y_dev), steps_per_epoch=len(X_train), validation_steps=len(X_dev), epochs=n_epochs, verbose=1, callbacks=[early_stop,check_point])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dict['accuracy'] = accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train..\n",
      "f1_all = [0.64       0.82803374 0.39181287 0.         0.7746891  0.\n",
      " 0.67598416 0.67405765 0.         0.28571429 0.22222222 0.69546436\n",
      " 0.72679045 0.51890756 1.         0.66666667 0.79065888 1.\n",
      " 0.         0.81864696 0.22033898 0.66798811 0.6624156  0.\n",
      " 0.72450533]\n",
      "f1_micro = 0.7245974955277281\n",
      "f1_macro = 0.5193958773403653\n",
      "f1_weighted = 0.7178798816619029\n",
      "accuracy = 0.7245974955277281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anush\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "yhat_train = rnn_predict(X_train, model)\n",
    "print('Train..')\n",
    "for metric, metric_fn in metrics_dict.items():\n",
    "    print(metric,\"=\", metric_fn(y_train, yhat_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev..\n",
      "f1_all = [0.33333333 0.7578125  0.18867925 0.         0.71576763 0.\n",
      " 0.61384725 0.61157025 0.         0.         0.         0.6835443\n",
      " 0.56880734 0.41830065 0.         0.         0.72207792 0.\n",
      " 0.         0.79313164 0.16216216 0.51265823 0.59235669 0.\n",
      " 0.61818182]\n",
      "f1_micro = 0.656828548430373\n",
      "f1_macro = 0.4146115485938678\n",
      "f1_weighted = 0.6477165387409132\n",
      "accuracy = 0.656828548430373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anush\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anush\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "yhat_dev = rnn_predict(X_dev, model)\n",
    "print('Dev..')\n",
    "for metric, metric_fn in metrics_dict.items():\n",
    "    print(metric,\"=\", metric_fn(y_dev, yhat_dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring name representations (last GRU state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_layer = Model(inputs=model.input, outputs=model.layers[2].output)\n",
    "name_vecs = rnn_predict(df['hindi_Surname'][surnames_mask].values, gru_layer, argmax=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "name_vecs = np.squeeze(name_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=n_classes, random_state=RANDOM_STATE).fit(name_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_vecs_clusters = km.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17054124721842603"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#how well do name vector clusters correspond with community\n",
    "adjusted_rand_score(name_vecs_clusters, labels[surnames_mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2% improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Over full names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Concatenate, BatchNormalization, Dropout\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_generator2(X,y):\n",
    "    while True:\n",
    "        for (x_fn_i, x_sn_i), y_i in zip(X,y):\n",
    "            yield ({'fn':get_name_val(x_fn_i).reshape(1,-1), 'sn':get_name_val(x_sn_i).reshape(1,-1)}, to_categorical(y_i, num_classes=n_classes).reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_dev, y_train, y_dev = train_test_split(df[['hindi_Name','hindi_Surname']][fullnames_mask].values, labels[fullnames_mask], random_state = RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considerations: Should first names and surnames use same or different RNNs? \n",
    "    \n",
    "There would be fewer parameters with the same RNN - beneficial for limited data, but first names and surnames seem to be fundamentally different sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Different GRUs for first and last names\n",
    "embedding_layer = Embedding(n_vocab, EMBEDDING_DIM, trainable=True)\n",
    "fn_inp = Input(batch_shape=(1,None,), name='fn')\n",
    "fn_x = embedding_layer(fn_inp)\n",
    "fn_x = GRU(32)(fn_x)\n",
    "sn_inp = Input(batch_shape=(1,None,), name='sn')\n",
    "sn_x = embedding_layer(sn_inp)\n",
    "sn_x = GRU(32)(sn_x)\n",
    "x = Concatenate()([fn_x, sn_x])\n",
    "out = Dense(n_classes, activation='softmax')(x)\n",
    "model = Model(inputs=[fn_inp, sn_inp], outputs=out)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "fn (InputLayer)                 (1, None)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sn (InputLayer)                 (1, None)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (1, None, 5)         640         fn[0][0]                         \n",
      "                                                                 sn[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "gru_5 (GRU)                     (1, 32)              3648        embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "gru_6 (GRU)                     (1, 32)              3648        embedding_5[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (1, 64)              0           gru_5[0][0]                      \n",
      "                                                                 gru_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (1, 25)              1625        concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 9,561\n",
      "Trainable params: 9,561\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "11032/11032 [==============================] - 68s 6ms/step - loss: 2.0672 - acc: 0.3495 - val_loss: 1.8732 - val_acc: 0.4190\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.87322, saving model to hindi_checkpoints\\model_rnn_fullname2.hdf5\n",
      "Epoch 2/20\n",
      "11032/11032 [==============================] - 61s 5ms/step - loss: 1.7000 - acc: 0.4810 - val_loss: 1.6043 - val_acc: 0.5160\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.87322 to 1.60431, saving model to hindi_checkpoints\\model_rnn_fullname2.hdf5\n",
      "Epoch 3/20\n",
      "11032/11032 [==============================] - 59s 5ms/step - loss: 1.4668 - acc: 0.5585 - val_loss: 1.4398 - val_acc: 0.5729\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.60431 to 1.43983, saving model to hindi_checkpoints\\model_rnn_fullname2.hdf5\n",
      "Epoch 4/20\n",
      "11032/11032 [==============================] - 60s 5ms/step - loss: 1.2988 - acc: 0.6069 - val_loss: 1.3321 - val_acc: 0.6001\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.43983 to 1.33215, saving model to hindi_checkpoints\\model_rnn_fullname2.hdf5\n",
      "Epoch 5/20\n",
      "11032/11032 [==============================] - 61s 6ms/step - loss: 1.1684 - acc: 0.6460 - val_loss: 1.2503 - val_acc: 0.6349\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.33215 to 1.25029, saving model to hindi_checkpoints\\model_rnn_fullname2.hdf5\n",
      "Epoch 6/20\n",
      "11032/11032 [==============================] - 60s 5ms/step - loss: 1.0695 - acc: 0.6743 - val_loss: 1.1914 - val_acc: 0.6533\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.25029 to 1.19144, saving model to hindi_checkpoints\\model_rnn_fullname2.hdf5\n",
      "Epoch 7/20\n",
      "11032/11032 [==============================] - 60s 5ms/step - loss: 0.9834 - acc: 0.6992 - val_loss: 1.1398 - val_acc: 0.6735\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.19144 to 1.13978, saving model to hindi_checkpoints\\model_rnn_fullname2.hdf5\n",
      "Epoch 8/20\n",
      "11032/11032 [==============================] - 68s 6ms/step - loss: 0.9111 - acc: 0.7189 - val_loss: 1.1022 - val_acc: 0.6830\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.13978 to 1.10216, saving model to hindi_checkpoints\\model_rnn_fullname2.hdf5\n",
      "Epoch 9/20\n",
      "11032/11032 [==============================] - 61s 6ms/step - loss: 0.8470 - acc: 0.7379 - val_loss: 1.0791 - val_acc: 0.6895\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.10216 to 1.07907, saving model to hindi_checkpoints\\model_rnn_fullname2.hdf5\n",
      "Epoch 10/20\n",
      "11032/11032 [==============================] - 51s 5ms/step - loss: 0.7962 - acc: 0.7554 - val_loss: 1.0725 - val_acc: 0.6873\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.07907 to 1.07251, saving model to hindi_checkpoints\\model_rnn_fullname2.hdf5\n",
      "Epoch 11/20\n",
      "11032/11032 [==============================] - 49s 4ms/step - loss: 0.7497 - acc: 0.7722 - val_loss: 1.0689 - val_acc: 0.6960\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.07251 to 1.06890, saving model to hindi_checkpoints\\model_rnn_fullname2.hdf5\n",
      "Epoch 12/20\n",
      "11032/11032 [==============================] - 43s 4ms/step - loss: 0.7104 - acc: 0.7821 - val_loss: 1.0900 - val_acc: 0.6900\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.06890\n",
      "Epoch 13/20\n",
      "11032/11032 [==============================] - 40s 4ms/step - loss: 0.6767 - acc: 0.7906 - val_loss: 1.0662 - val_acc: 0.7042\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.06890 to 1.06620, saving model to hindi_checkpoints\\model_rnn_fullname2.hdf5\n",
      "Epoch 14/20\n",
      "11032/11032 [==============================] - 40s 4ms/step - loss: 0.6467 - acc: 0.7973 - val_loss: 1.0748 - val_acc: 0.6996\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.06620\n",
      "Epoch 15/20\n",
      "11032/11032 [==============================] - 40s 4ms/step - loss: 0.6224 - acc: 0.8085 - val_loss: 1.0863 - val_acc: 0.6968\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.06620\n",
      "Epoch 16/20\n",
      "11032/11032 [==============================] - 40s 4ms/step - loss: 0.5985 - acc: 0.8135 - val_loss: 1.0921 - val_acc: 0.7083\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 1.06620\n",
      "Epoch 17/20\n",
      "11032/11032 [==============================] - 40s 4ms/step - loss: 0.5819 - acc: 0.8160 - val_loss: 1.1064 - val_acc: 0.7096\n",
      "Restoring model weights from the end of the best epoch\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.06620\n",
      "Epoch 00017: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17aa73e16a0>"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_point = ModelCheckpoint('hindi_checkpoints\\\\model_rnn_fullname2.hdf5', verbose=True, save_best_only=True)\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=4, verbose=True, restore_best_weights=True)\n",
    "model.fit_generator(rnn_generator2(X_train,y_train), validation_data=rnn_generator2(X_dev,y_dev), steps_per_epoch=len(X_train), validation_steps=len(X_dev), epochs=n_epochs, verbose=1, callbacks=[early_stop,check_point])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "insignificant improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same GRU for first and last names\n",
    "embedding_layer = Embedding(n_vocab, EMBEDDING_DIM, trainable=True)\n",
    "gru = GRU(48)\n",
    "fn_inp = Input(batch_shape=(1,None,), name='fn')\n",
    "fn_x = embedding_layer(fn_inp)\n",
    "fn_x = gru(fn_x)\n",
    "sn_inp = Input(batch_shape=(1,None,), name='sn')\n",
    "sn_x = embedding_layer(sn_inp)\n",
    "sn_x = gru(sn_x)\n",
    "x = Concatenate()([fn_x, sn_x])\n",
    "out = Dense(n_classes, activation='softmax')(x)\n",
    "model = Model(inputs=[fn_inp, sn_inp], outputs=out)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "11032/11032 [==============================] - 75s 7ms/step - loss: 2.0999 - acc: 0.3234 - val_loss: 1.8994 - val_acc: 0.4144\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.89942, saving model to hindi_checkpoints\\model_rnn_fullname2ii.hdf5\n",
      "Epoch 2/20\n",
      "11032/11032 [==============================] - 58s 5ms/step - loss: 1.7008 - acc: 0.4781 - val_loss: 1.5729 - val_acc: 0.5215\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.89942 to 1.57287, saving model to hindi_checkpoints\\model_rnn_fullname2ii.hdf5\n",
      "Epoch 3/20\n",
      "11032/11032 [==============================] - 38s 3ms/step - loss: 1.4035 - acc: 0.5762 - val_loss: 1.3399 - val_acc: 0.6030\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.57287 to 1.33991, saving model to hindi_checkpoints\\model_rnn_fullname2ii.hdf5\n",
      "Epoch 4/20\n",
      "11032/11032 [==============================] - 36s 3ms/step - loss: 1.1937 - acc: 0.6380 - val_loss: 1.2192 - val_acc: 0.6397\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.33991 to 1.21919, saving model to hindi_checkpoints\\model_rnn_fullname2ii.hdf5\n",
      "Epoch 5/20\n",
      "11032/11032 [==============================] - 35s 3ms/step - loss: 1.0433 - acc: 0.6846 - val_loss: 1.1584 - val_acc: 0.6533\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.21919 to 1.15836, saving model to hindi_checkpoints\\model_rnn_fullname2ii.hdf5\n",
      "Epoch 6/20\n",
      "11032/11032 [==============================] - 35s 3ms/step - loss: 0.9339 - acc: 0.7121 - val_loss: 1.1146 - val_acc: 0.6732\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.15836 to 1.11460, saving model to hindi_checkpoints\\model_rnn_fullname2ii.hdf5\n",
      "Epoch 7/20\n",
      "11032/11032 [==============================] - 36s 3ms/step - loss: 0.8471 - acc: 0.7404 - val_loss: 1.0836 - val_acc: 0.6884\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.11460 to 1.08360, saving model to hindi_checkpoints\\model_rnn_fullname2ii.hdf5\n",
      "Epoch 8/20\n",
      "11032/11032 [==============================] - 36s 3ms/step - loss: 0.7805 - acc: 0.7582 - val_loss: 1.0673 - val_acc: 0.6917\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.08360 to 1.06732, saving model to hindi_checkpoints\\model_rnn_fullname2ii.hdf5\n",
      "Epoch 9/20\n",
      "11032/11032 [==============================] - 35s 3ms/step - loss: 0.7245 - acc: 0.7736 - val_loss: 1.0651 - val_acc: 0.6977\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.06732 to 1.06506, saving model to hindi_checkpoints\\model_rnn_fullname2ii.hdf5\n",
      "Epoch 10/20\n",
      "11032/11032 [==============================] - 32s 3ms/step - loss: 0.6879 - acc: 0.7863 - val_loss: 1.0484 - val_acc: 0.7028\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.06506 to 1.04842, saving model to hindi_checkpoints\\model_rnn_fullname2ii.hdf5\n",
      "Epoch 11/20\n",
      "11032/11032 [==============================] - 31s 3ms/step - loss: 0.6484 - acc: 0.7958 - val_loss: 1.0570 - val_acc: 0.6985\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.04842\n",
      "Epoch 12/20\n",
      "11032/11032 [==============================] - 31s 3ms/step - loss: 0.6176 - acc: 0.8046 - val_loss: 1.0712 - val_acc: 0.7036\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.04842\n",
      "Epoch 13/20\n",
      "11032/11032 [==============================] - 35s 3ms/step - loss: 0.5870 - acc: 0.8142 - val_loss: 1.0771 - val_acc: 0.7072\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.04842\n",
      "Epoch 14/20\n",
      "11032/11032 [==============================] - 34s 3ms/step - loss: 0.5695 - acc: 0.8185 - val_loss: 1.0686 - val_acc: 0.7045\n",
      "Restoring model weights from the end of the best epoch\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.04842\n",
      "Epoch 00014: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17aa73e1d30>"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_point = ModelCheckpoint('hindi_checkpoints\\\\model_rnn_fullname2ii.hdf5', verbose=True, save_best_only=True)\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=4, verbose=True, restore_best_weights=True)\n",
    "model.fit_generator(rnn_generator2(X_train,y_train), validation_data=rnn_generator2(X_dev,y_dev), steps_per_epoch=len(X_train), validation_steps=len(X_dev), epochs=n_epochs, verbose=1, callbacks=[early_stop,check_point])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fullname_processor(fullname):\n",
    "    return {'fn':get_name_val(fullname[0]).reshape(1,-1),'sn':get_name_val(fullname[1]).reshape(1,-1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_train2 = rnn_predict(X_train, model, x_processor=fullname_processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_dev2 = rnn_predict(X_dev, model, x_processor=fullname_processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train..\n",
      "f1_all = [0.86363636 0.91271989 0.73181818 0.         0.89618321 0.83333333\n",
      " 0.87557392 0.85977011 1.         0.94117647 0.66666667 0.8778626\n",
      " 0.84185493 0.83561644 1.         0.8        0.89719626 1.\n",
      " 1.         0.91545894 0.76836158 0.83953033 0.89099167 0.75\n",
      " 0.87702265]\n",
      "f1_micro = 0.8827955039883975\n",
      "f1_macro = 0.8349909423369144\n",
      "f1_weighted = 0.8828711675769418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anush\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print('Train..')\n",
    "for metric, metric_fn in metrics_dict.items():\n",
    "    print(metric,\"=\", metric_fn(y_train, yhat_train2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev..\n",
      "f1_all = [0.         0.83333333 0.51851852 0.         0.74387528 0.\n",
      " 0.75879718 0.71005917 0.         0.22222222 0.66666667 0.66666667\n",
      " 0.66914498 0.65396825 0.         0.8        0.77639752 0.\n",
      " 0.         0.82539683 0.60869565 0.61538462 0.71981243 0.\n",
      " 0.70588235]\n",
      "f1_micro = 0.7457857531266993\n",
      "f1_macro = 0.5473724602779464\n",
      "f1_weighted = 0.744617141378752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anush\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anush\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print('Dev..')\n",
    "for metric, metric_fn in metrics_dict.items():\n",
    "    print(metric,\"=\", metric_fn(y_dev, yhat_dev2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring name representations (last GRU state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_layer1 = Model(inputs=model.input, outputs=model.layers[3].output)\n",
    "gru_layer2 = Model(inputs=model.input, outputs=model.layers[4].output)\n",
    "#name_vecs = rnn_predict(df['Surname'][surnames_mask].values, gru_layer, argmax=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gru_vecs1 = np.squeeze(rnn_predict(df[['Name','Surname']][fullnames_mask].values, gru_layer1, argmax=False, x_processor=fullname_processor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_vecs2 = np.squeeze(rnn_predict(df[['Name','Surname']][fullnames_mask].values, gru_layer2, argmax=False, x_processor=fullname_processor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_vecs2 = np.hstack([gru_vecs1, gru_vecs2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "km2 = KMeans(n_clusters=n_classes, random_state=RANDOM_STATE).fit(name_vecs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_vecs2_clusters = km2.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13661862081640017"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#how well do name vector clusters correspond with community\n",
    "adjusted_rand_score(name_vecs2_clusters, labels[fullnames_mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(n_vocab, EMBEDDING_DIM, trainable=True)\n",
    "\n",
    "fn_inp = Input(batch_shape=(1,None,), name='fn')\n",
    "fn_x = embedding_layer(fn_inp)\n",
    "fn_x = GRU(64, return_sequences=True)(fn_x)\n",
    "fn_x = Dropout(0.2)(fn_x)\n",
    "fn_x = GRU(64)(fn_x)\n",
    "\n",
    "sn_inp = Input(batch_shape=(1,None,), name='sn')\n",
    "sn_x = embedding_layer(sn_inp)\n",
    "sn_x = GRU(64, return_sequences=True)(sn_x)\n",
    "sn_x = Dropout(0.2)(sn_x)\n",
    "sn_x = GRU(64)(sn_x)\n",
    "\n",
    "x = Concatenate()([fn_x, sn_x])\n",
    "out = Dense(n_classes, activation='softmax')(x)\n",
    "model = Model(inputs=[fn_inp, sn_inp], outputs=out)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "11032/11032 [==============================] - 71s 6ms/step - loss: 2.0100 - acc: 0.3619 - val_loss: 1.7658 - val_acc: 0.4546\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.76582, saving model to hindi_checkpoints\\model_rnn_fullname3.hdf5\n",
      "Epoch 2/20\n",
      "11032/11032 [==============================] - 76s 7ms/step - loss: 1.5487 - acc: 0.5237 - val_loss: 1.4641 - val_acc: 0.5571\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.76582 to 1.46413, saving model to hindi_checkpoints\\model_rnn_fullname3.hdf5\n",
      "Epoch 3/20\n",
      "11032/11032 [==============================] - 58s 5ms/step - loss: 1.2614 - acc: 0.6177 - val_loss: 1.2593 - val_acc: 0.6136\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.46413 to 1.25926, saving model to hindi_checkpoints\\model_rnn_fullname3.hdf5\n",
      "Epoch 4/20\n",
      "11032/11032 [==============================] - 58s 5ms/step - loss: 1.0462 - acc: 0.6778 - val_loss: 1.1126 - val_acc: 0.6691\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.25926 to 1.11263, saving model to hindi_checkpoints\\model_rnn_fullname3.hdf5\n",
      "Epoch 5/20\n",
      "11032/11032 [==============================] - 57s 5ms/step - loss: 0.9051 - acc: 0.7182 - val_loss: 1.0422 - val_acc: 0.6947\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.11263 to 1.04221, saving model to hindi_checkpoints\\model_rnn_fullname3.hdf5\n",
      "Epoch 6/20\n",
      "11032/11032 [==============================] - 58s 5ms/step - loss: 0.7829 - acc: 0.7548 - val_loss: 1.0243 - val_acc: 0.7042\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.04221 to 1.02429, saving model to hindi_checkpoints\\model_rnn_fullname3.hdf5\n",
      "Epoch 7/20\n",
      "11032/11032 [==============================] - 55s 5ms/step - loss: 0.7026 - acc: 0.7747 - val_loss: 1.0146 - val_acc: 0.7083\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.02429 to 1.01458, saving model to hindi_checkpoints\\model_rnn_fullname3.hdf5\n",
      "Epoch 8/20\n",
      "11032/11032 [==============================] - 56s 5ms/step - loss: 0.6420 - acc: 0.7944 - val_loss: 0.9773 - val_acc: 0.7178\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.01458 to 0.97731, saving model to hindi_checkpoints\\model_rnn_fullname3.hdf5\n",
      "Epoch 9/20\n",
      "11032/11032 [==============================] - 56s 5ms/step - loss: 0.6019 - acc: 0.8060 - val_loss: 0.9946 - val_acc: 0.7254\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.97731\n",
      "Epoch 10/20\n",
      "11032/11032 [==============================] - 55s 5ms/step - loss: 0.5542 - acc: 0.8185 - val_loss: 0.9814 - val_acc: 0.7289\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.97731\n",
      "Epoch 11/20\n",
      "11032/11032 [==============================] - 76s 7ms/step - loss: 0.5322 - acc: 0.8256 - val_loss: 0.9729 - val_acc: 0.7376\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.97731 to 0.97287, saving model to hindi_checkpoints\\model_rnn_fullname3.hdf5\n",
      "Epoch 12/20\n",
      "11032/11032 [==============================] - 89s 8ms/step - loss: 0.5015 - acc: 0.8346 - val_loss: 0.9725 - val_acc: 0.7458\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.97287 to 0.97252, saving model to hindi_checkpoints\\model_rnn_fullname3.hdf5\n",
      "Epoch 13/20\n",
      "11032/11032 [==============================] - 63s 6ms/step - loss: 0.4827 - acc: 0.8374 - val_loss: 1.0151 - val_acc: 0.7382\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.97252\n",
      "Epoch 14/20\n",
      "11032/11032 [==============================] - 63s 6ms/step - loss: 0.4708 - acc: 0.8422 - val_loss: 1.0123 - val_acc: 0.7287\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.97252\n",
      "Epoch 15/20\n",
      "11032/11032 [==============================] - 67s 6ms/step - loss: 0.4484 - acc: 0.8485 - val_loss: 1.0116 - val_acc: 0.7425\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.97252\n",
      "Epoch 16/20\n",
      "11032/11032 [==============================] - 67s 6ms/step - loss: 0.4530 - acc: 0.8513 - val_loss: 1.0518 - val_acc: 0.7314\n",
      "Restoring model weights from the end of the best epoch\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.97252\n",
      "Epoch 00016: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1bc5d4b4400>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_point = ModelCheckpoint('hindi_checkpoints\\\\model_rnn_fullname3.hdf5', verbose=True, save_best_only=True)\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=4, verbose=True, restore_best_weights=True)\n",
    "model.fit_generator(rnn_generator2(X_train,y_train), validation_data=rnn_generator2(X_dev,y_dev), steps_per_epoch=len(X_train), validation_steps=len(X_dev), epochs=n_epochs, verbose=1, callbacks=[early_stop,check_point])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "from keras.layers import concatenate, Conv1D, GlobalMaxPooling1D, Lambda, Flatten\n",
    "from keras import backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_vocab = n_vocab + 1\n",
    "n_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Over last names only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_dev, y_train, y_dev = train_test_split(df['hindi_Surname'][surnames_mask].values, labels[surnames_mask], random_state = RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_name_processor(name, const_val=n_vocab-1, pad=2):\n",
    "    return np.pad(get_name_val(name), pad_width=pad, mode='constant', constant_values=const_val).reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding of 2 so minimum word length is 5\n",
    "def cnn_generator(X, y, one_iter=False, x_processor=None):\n",
    "    if x_processor is None:\n",
    "        x_processor = cnn_name_processor\n",
    "        \n",
    "    while True:\n",
    "        for x_i, y_i in zip(X,y):\n",
    "            yield (x_processor(x_i), to_categorical(y_i, num_classes=n_classes).reshape(1,-1))\n",
    "    \n",
    "        if one_iter:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[128 128  56  62  57  66 128 128]] [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]]\n",
      "[[128 128  23  65  42  77  36 128 128]] [[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]]\n",
      "[[128 128  42  31  77  31  40  62  63  21 128 128]] [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]]\n",
      "[[128 128  56  62  57  62 128 128]] [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]]\n",
      "[[128 128  45  31  77  31  62 128 128]] [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]]\n"
     ]
    }
   ],
   "source": [
    "for a,b in cnn_generator(X_train[:5],y_train[:5], one_iter=True):\n",
    "    print(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_predict(names, model, argmax=True, x_processor=None):\n",
    "    if x_processor is None:\n",
    "        x_processor = cnn_name_processor\n",
    "        \n",
    "    predictions = []\n",
    "    for name in names:\n",
    "        result = model.predict( x_processor(name) )\n",
    "        if argmax:\n",
    "            result = np.argmax(result)\n",
    "        predictions += [result]\n",
    "    return np.array(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1D Convolution / convolution over time with filter sizes from 1 to 5 followed by max pooling over time (giving one element for each filter). Output is stacked into a single vector followed by a softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layers = []\n",
    "for i in range(1,6):\n",
    "    conv_layers.append( Conv1D(filters=10, kernel_size=(i,), strides=(1,)) )\n",
    "    \n",
    "inp = Input(shape=(None,))\n",
    "x = Embedding(n_vocab, EMBEDDING_DIM, trainable=True)(inp)\n",
    "conv_outputs = []\n",
    "for conv_layer in conv_layers:\n",
    "    conv_outputs.append( GlobalMaxPooling1D() (conv_layer(x)) )\n",
    "x = Lambda(lambda x: backend.stack(x, axis=1))(conv_outputs)\n",
    "x = Flatten()(x) \n",
    "out = Dense(n_classes, activation='softmax')(x)\n",
    "model = Model(inputs=inp, outputs=out)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 5)      645         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, None, 10)     60          embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, None, 10)     110         embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, None, 10)     160         embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, None, 10)     210         embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, None, 10)     260         embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 10)           0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 10)           0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 10)           0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 10)           0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalM (None, 10)           0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 5, 10)        0           global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "                                                                 global_max_pooling1d_4[0][0]     \n",
      "                                                                 global_max_pooling1d_5[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 50)           0           lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 25)           1275        flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,720\n",
      "Trainable params: 2,720\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "11180/11180 [==============================] - 41s 4ms/step - loss: 1.9875 - acc: 0.3868 - val_loss: 1.7321 - val_acc: 0.4832\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.73211, saving model to hindi_checkpoints\\model_cnn_surname.hdf5\n",
      "Epoch 2/20\n",
      "11180/11180 [==============================] - 41s 4ms/step - loss: 1.6457 - acc: 0.5137 - val_loss: 1.5946 - val_acc: 0.5466\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.73211 to 1.59461, saving model to hindi_checkpoints\\model_cnn_surname.hdf5\n",
      "Epoch 3/20\n",
      "11180/11180 [==============================] - 39s 3ms/step - loss: 1.5237 - acc: 0.5564 - val_loss: 1.5383 - val_acc: 0.5661\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.59461 to 1.53833, saving model to hindi_checkpoints\\model_cnn_surname.hdf5\n",
      "Epoch 4/20\n",
      "11180/11180 [==============================] - 39s 4ms/step - loss: 1.4568 - acc: 0.5776 - val_loss: 1.5065 - val_acc: 0.5793\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.53833 to 1.50647, saving model to hindi_checkpoints\\model_cnn_surname.hdf5\n",
      "Epoch 5/20\n",
      "11180/11180 [==============================] - 41s 4ms/step - loss: 1.4130 - acc: 0.5915 - val_loss: 1.4835 - val_acc: 0.5948\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.50647 to 1.48352, saving model to hindi_checkpoints\\model_cnn_surname.hdf5\n",
      "Epoch 6/20\n",
      "11180/11180 [==============================] - 40s 4ms/step - loss: 1.3768 - acc: 0.6030 - val_loss: 1.4681 - val_acc: 0.5967\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.48352 to 1.46812, saving model to hindi_checkpoints\\model_cnn_surname.hdf5\n",
      "Epoch 7/20\n",
      "11180/11180 [==============================] - 42s 4ms/step - loss: 1.3490 - acc: 0.6086 - val_loss: 1.4552 - val_acc: 0.5999\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.46812 to 1.45515, saving model to hindi_checkpoints\\model_cnn_surname.hdf5\n",
      "Epoch 8/20\n",
      "11180/11180 [==============================] - 39s 4ms/step - loss: 1.3253 - acc: 0.6170 - val_loss: 1.4394 - val_acc: 0.6037\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.45515 to 1.43944, saving model to hindi_checkpoints\\model_cnn_surname.hdf5\n",
      "Epoch 9/20\n",
      "11180/11180 [==============================] - 40s 4ms/step - loss: 1.3052 - acc: 0.6233 - val_loss: 1.4325 - val_acc: 0.6050\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.43944 to 1.43247, saving model to hindi_checkpoints\\model_cnn_surname.hdf5\n",
      "Epoch 10/20\n",
      "11180/11180 [==============================] - 34s 3ms/step - loss: 1.2882 - acc: 0.6285 - val_loss: 1.4246 - val_acc: 0.6058\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.43247 to 1.42456, saving model to hindi_checkpoints\\model_cnn_surname.hdf5\n",
      "Epoch 11/20\n",
      "11180/11180 [==============================] - 39s 4ms/step - loss: 1.2745 - acc: 0.6318 - val_loss: 1.4170 - val_acc: 0.6067\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.42456 to 1.41698, saving model to hindi_checkpoints\\model_cnn_surname.hdf5\n",
      "Epoch 12/20\n",
      "11180/11180 [==============================] - 48s 4ms/step - loss: 1.2620 - acc: 0.6352 - val_loss: 1.4126 - val_acc: 0.6099\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.41698 to 1.41261, saving model to hindi_checkpoints\\model_cnn_surname.hdf5\n",
      "Epoch 13/20\n",
      "11180/11180 [==============================] - 46s 4ms/step - loss: 1.2521 - acc: 0.6383 - val_loss: 1.4106 - val_acc: 0.6134\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.41261 to 1.41056, saving model to hindi_checkpoints\\model_cnn_surname.hdf5\n",
      "Epoch 14/20\n",
      "11180/11180 [==============================] - 45s 4ms/step - loss: 1.2442 - acc: 0.6411 - val_loss: 1.4083 - val_acc: 0.6134\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.41056 to 1.40828, saving model to hindi_checkpoints\\model_cnn_surname.hdf5\n",
      "Epoch 15/20\n",
      "11180/11180 [==============================] - 46s 4ms/step - loss: 1.2370 - acc: 0.6408 - val_loss: 1.4041 - val_acc: 0.6190\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.40828 to 1.40415, saving model to hindi_checkpoints\\model_cnn_surname.hdf5\n",
      "Epoch 16/20\n",
      "11180/11180 [==============================] - 48s 4ms/step - loss: 1.2298 - acc: 0.6430 - val_loss: 1.4023 - val_acc: 0.6195\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.40415 to 1.40226, saving model to hindi_checkpoints\\model_cnn_surname.hdf5\n",
      "Epoch 17/20\n",
      "11180/11180 [==============================] - 43s 4ms/step - loss: 1.2226 - acc: 0.6442 - val_loss: 1.4002 - val_acc: 0.6236\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.40226 to 1.40017, saving model to hindi_checkpoints\\model_cnn_surname.hdf5\n",
      "Epoch 18/20\n",
      "11180/11180 [==============================] - 39s 3ms/step - loss: 1.2167 - acc: 0.6458 - val_loss: 1.3975 - val_acc: 0.6230\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.40017 to 1.39748, saving model to hindi_checkpoints\\model_cnn_surname.hdf5\n",
      "Epoch 19/20\n",
      "11180/11180 [==============================] - 39s 4ms/step - loss: 1.2119 - acc: 0.6467 - val_loss: 1.3982 - val_acc: 0.6211\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.39748\n",
      "Epoch 20/20\n",
      "11180/11180 [==============================] - 39s 3ms/step - loss: 1.2071 - acc: 0.6488 - val_loss: 1.3955 - val_acc: 0.6201\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.39748 to 1.39553, saving model to hindi_checkpoints\\model_cnn_surname.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b5b8355470>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_point = ModelCheckpoint('hindi_checkpoints\\\\model_cnn_surname.hdf5', verbose=True, save_best_only=True)\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=4, verbose=True, restore_best_weights=True)\n",
    "model.fit_generator(cnn_generator(X_train,y_train), validation_data=cnn_generator(X_dev,y_dev), steps_per_epoch=len(X_train), validation_steps=len(X_dev), epochs=n_epochs, verbose=1, callbacks=[early_stop,check_point])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "similar performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dict['accuracy'] = accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train..\n",
      "f1_all = [0.32258065 0.77608222 0.25958702 0.         0.68745763 0.\n",
      " 0.62754871 0.62884161 0.         0.         0.         0.57907543\n",
      " 0.62418726 0.48587571 0.         0.15384615 0.74550129 1.\n",
      " 1.         0.78493387 0.         0.50378788 0.5511811  0.\n",
      " 0.59938838]\n",
      "f1_micro = 0.6557245080500894\n",
      "f1_macro = 0.41319499537186727\n",
      "f1_weighted = 0.6444963897446352\n",
      "accuracy = 0.6557245080500894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anush\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "yhat_train = cnn_predict(X_train, model)\n",
    "print('Train..')\n",
    "for metric, metric_fn in metrics_dict.items():\n",
    "    print(metric,\"=\", metric_fn(y_train, yhat_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev..\n",
      "f1_all = [0.         0.7456979  0.14814815 0.         0.66533066 0.\n",
      " 0.57784011 0.51162791 0.         0.         0.         0.66242038\n",
      " 0.49765258 0.40531561 0.         0.         0.70588235 0.\n",
      " 0.         0.77317881 0.         0.43067847 0.53117783 0.\n",
      " 0.5787234 ]\n",
      "f1_micro = 0.6200697612020392\n",
      "f1_macro = 0.3616837082330856\n",
      "f1_weighted = 0.6089883362655147\n",
      "accuracy = 0.6200697612020392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anush\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anush\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "yhat_dev = cnn_predict(X_dev, model)\n",
    "print('Dev..')\n",
    "for metric, metric_fn in metrics_dict.items():\n",
    "    print(metric,\"=\", metric_fn(y_dev, yhat_dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring name representations (last GRU state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjusted Random Index is a metric which gives the similarity between 2 different sets of clusters.\n",
    "\n",
    "Here we cluster name vector representations using KMeans and compare them with the actual clusters of communities to get an idea of how close name vectors from the same community are to each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_output = Model(inputs=model.input, outputs=model.layers[-2].output)\n",
    "name_vecs = cnn_predict(df['hindi_Surname'][surnames_mask].values, layer_output, argmax=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "name_vecs = np.squeeze(name_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=n_classes, random_state=RANDOM_STATE).fit(name_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_vecs_clusters = km.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10286560817081361"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#how well do name vector clusters correspond with community\n",
    "adjusted_rand_score(name_vecs_clusters, labels[surnames_mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Over full names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_dev, y_train, y_dev = train_test_split(df[['hindi_Name','hindi_Surname']][fullnames_mask].values, labels[fullnames_mask], random_state = RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding of 2 so minimum word length is 5\n",
    "def cnn_generator2(X, y, one_iter=False, x_processor=None):\n",
    "    if x_processor is None:\n",
    "        x_processor = cnn_name_processor\n",
    "        \n",
    "    while True:\n",
    "        for x_i, y_i in zip(X,y):\n",
    "            yield ({'fn': x_processor(x_i[0]), 'sn':x_processor(x_i[1])}, to_categorical(y_i, num_classes=n_classes).reshape(1,-1))\n",
    "    \n",
    "        if one_iter:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(n_vocab, EMBEDDING_DIM, trainable=True)\n",
    "fn_inp = Input(shape=(None,), name='fn')\n",
    "fn_x = embedding_layer(fn_inp)\n",
    "\n",
    "sn_inp = Input(shape=(None,), name='sn')\n",
    "sn_x = embedding_layer(sn_inp)\n",
    "\n",
    "conv_layers = {'fn':[], 'sn':[]}\n",
    "for inp_x in ['fn','sn']:\n",
    "    for i in range(1,6):\n",
    "        conv_layers[inp_x].append( Conv1D(filters=10, kernel_size=(i,), strides=(1,)) )\n",
    "    \n",
    "conv_outputs = []\n",
    "for conv_layer in conv_layers['fn']:\n",
    "    conv_outputs.append( GlobalMaxPooling1D() (conv_layer(fn_x)) )\n",
    "for conv_layer in conv_layers['sn']:\n",
    "    conv_outputs.append( GlobalMaxPooling1D() (conv_layer(sn_x)) )\n",
    "    \n",
    "x = Lambda(lambda x: backend.stack(x, axis=1))(conv_outputs)\n",
    "x = Flatten()(x) \n",
    "out = Dense(n_classes, activation='softmax')(x)\n",
    "model = Model(inputs=[fn_inp, sn_inp], outputs=out)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "fn (InputLayer)                 (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sn (InputLayer)                 (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, None, 5)      645         fn[0][0]                         \n",
      "                                                                 sn[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, None, 10)     60          embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, None, 10)     110         embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, None, 10)     160         embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, None, 10)     210         embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, None, 10)     260         embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, None, 10)     60          embedding_3[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, None, 10)     110         embedding_3[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, None, 10)     160         embedding_3[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, None, 10)     210         embedding_3[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, None, 10)     260         embedding_3[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_11 (Global (None, 10)           0           conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_12 (Global (None, 10)           0           conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_13 (Global (None, 10)           0           conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_14 (Global (None, 10)           0           conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_15 (Global (None, 10)           0           conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_16 (Global (None, 10)           0           conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_17 (Global (None, 10)           0           conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_18 (Global (None, 10)           0           conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_19 (Global (None, 10)           0           conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_20 (Global (None, 10)           0           conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 10, 10)       0           global_max_pooling1d_11[0][0]    \n",
      "                                                                 global_max_pooling1d_12[0][0]    \n",
      "                                                                 global_max_pooling1d_13[0][0]    \n",
      "                                                                 global_max_pooling1d_14[0][0]    \n",
      "                                                                 global_max_pooling1d_15[0][0]    \n",
      "                                                                 global_max_pooling1d_16[0][0]    \n",
      "                                                                 global_max_pooling1d_17[0][0]    \n",
      "                                                                 global_max_pooling1d_18[0][0]    \n",
      "                                                                 global_max_pooling1d_19[0][0]    \n",
      "                                                                 global_max_pooling1d_20[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 100)          0           lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 25)           2525        flatten_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,770\n",
      "Trainable params: 4,770\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "11032/11032 [==============================] - 61s 6ms/step - loss: 1.9509 - acc: 0.3921 - val_loss: 1.6931 - val_acc: 0.4894\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.69313, saving model to hindi_checkpoints\\model_cnn_fullname.hdf5\n",
      "Epoch 2/20\n",
      "11032/11032 [==============================] - 61s 6ms/step - loss: 1.5247 - acc: 0.5354 - val_loss: 1.4692 - val_acc: 0.5563\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.69313 to 1.46922, saving model to hindi_checkpoints\\model_cnn_fullname.hdf5\n",
      "Epoch 3/20\n",
      "11032/11032 [==============================] - 55s 5ms/step - loss: 1.3541 - acc: 0.5858 - val_loss: 1.3730 - val_acc: 0.5881\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.46922 to 1.37298, saving model to hindi_checkpoints\\model_cnn_fullname.hdf5\n",
      "Epoch 4/20\n",
      "11032/11032 [==============================] - 59s 5ms/step - loss: 1.2595 - acc: 0.6135 - val_loss: 1.3233 - val_acc: 0.6093\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.37298 to 1.32328, saving model to hindi_checkpoints\\model_cnn_fullname.hdf5\n",
      "Epoch 5/20\n",
      "11032/11032 [==============================] - 58s 5ms/step - loss: 1.1988 - acc: 0.6296 - val_loss: 1.2952 - val_acc: 0.6101\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.32328 to 1.29515, saving model to hindi_checkpoints\\model_cnn_fullname.hdf5\n",
      "Epoch 6/20\n",
      "11032/11032 [==============================] - 51s 5ms/step - loss: 1.1537 - acc: 0.6465 - val_loss: 1.2779 - val_acc: 0.6115\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.29515 to 1.27786, saving model to hindi_checkpoints\\model_cnn_fullname.hdf5\n",
      "Epoch 7/20\n",
      "11032/11032 [==============================] - 54s 5ms/step - loss: 1.1198 - acc: 0.6555 - val_loss: 1.2646 - val_acc: 0.6215\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.27786 to 1.26456, saving model to hindi_checkpoints\\model_cnn_fullname.hdf5\n",
      "Epoch 8/20\n",
      "11032/11032 [==============================] - 50s 5ms/step - loss: 1.0936 - acc: 0.6648 - val_loss: 1.2572 - val_acc: 0.6262\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.26456 to 1.25724, saving model to hindi_checkpoints\\model_cnn_fullname.hdf5\n",
      "Epoch 9/20\n",
      "11032/11032 [==============================] - 53s 5ms/step - loss: 1.0728 - acc: 0.6698 - val_loss: 1.2489 - val_acc: 0.6321\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.25724 to 1.24889, saving model to hindi_checkpoints\\model_cnn_fullname.hdf5\n",
      "Epoch 10/20\n",
      "11032/11032 [==============================] - 51s 5ms/step - loss: 1.0539 - acc: 0.6752 - val_loss: 1.2455 - val_acc: 0.6340\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.24889 to 1.24554, saving model to hindi_checkpoints\\model_cnn_fullname.hdf5\n",
      "Epoch 11/20\n",
      "11032/11032 [==============================] - 54s 5ms/step - loss: 1.0377 - acc: 0.6821 - val_loss: 1.2437 - val_acc: 0.6351\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.24554 to 1.24371, saving model to hindi_checkpoints\\model_cnn_fullname.hdf5\n",
      "Epoch 12/20\n",
      "11032/11032 [==============================] - 55s 5ms/step - loss: 1.0248 - acc: 0.6863 - val_loss: 1.2408 - val_acc: 0.6378\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.24371 to 1.24079, saving model to hindi_checkpoints\\model_cnn_fullname.hdf5\n",
      "Epoch 13/20\n",
      "11032/11032 [==============================] - 53s 5ms/step - loss: 1.0132 - acc: 0.6877 - val_loss: 1.2404 - val_acc: 0.6384\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.24079 to 1.24044, saving model to hindi_checkpoints\\model_cnn_fullname.hdf5\n",
      "Epoch 14/20\n",
      "11032/11032 [==============================] - 60s 5ms/step - loss: 1.0015 - acc: 0.6916 - val_loss: 1.2362 - val_acc: 0.6397\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.24044 to 1.23616, saving model to hindi_checkpoints\\model_cnn_fullname.hdf5\n",
      "Epoch 15/20\n",
      "11032/11032 [==============================] - 54s 5ms/step - loss: 0.9905 - acc: 0.6933 - val_loss: 1.2305 - val_acc: 0.6449\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.23616 to 1.23050, saving model to hindi_checkpoints\\model_cnn_fullname.hdf5\n",
      "Epoch 16/20\n",
      "11032/11032 [==============================] - 62s 6ms/step - loss: 0.9809 - acc: 0.6972 - val_loss: 1.2248 - val_acc: 0.6493\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.23050 to 1.22477, saving model to hindi_checkpoints\\model_cnn_fullname.hdf5\n",
      "Epoch 17/20\n",
      "11032/11032 [==============================] - 58s 5ms/step - loss: 0.9724 - acc: 0.6993 - val_loss: 1.2252 - val_acc: 0.6482\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.22477\n",
      "Epoch 18/20\n",
      "11032/11032 [==============================] - 51s 5ms/step - loss: 0.9649 - acc: 0.7036 - val_loss: 1.2231 - val_acc: 0.6517\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.22477 to 1.22308, saving model to hindi_checkpoints\\model_cnn_fullname.hdf5\n",
      "Epoch 19/20\n",
      "11032/11032 [==============================] - 49s 4ms/step - loss: 0.9571 - acc: 0.7049 - val_loss: 1.2214 - val_acc: 0.6506\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.22308 to 1.22143, saving model to hindi_checkpoints\\model_cnn_fullname.hdf5\n",
      "Epoch 20/20\n",
      "11032/11032 [==============================] - 52s 5ms/step - loss: 0.9508 - acc: 0.7078 - val_loss: 1.2219 - val_acc: 0.6512\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 1.22143\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b5bad4da20>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_point = ModelCheckpoint('hindi_checkpoints\\\\model_cnn_fullname.hdf5', verbose=True, save_best_only=True)\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=4, verbose=True, restore_best_weights=True)\n",
    "model.fit_generator(cnn_generator2(X_train,y_train), validation_data=cnn_generator2(X_dev,y_dev), steps_per_epoch=len(X_train), validation_steps=len(X_dev), epochs=n_epochs, verbose=1, callbacks=[early_stop,check_point])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "similar performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train..\n",
      "f1_all = [0.54545455 0.79038024 0.41089109 0.         0.74700657 0.66666667\n",
      " 0.69492255 0.60846561 1.         0.83870968 0.54545455 0.69333333\n",
      " 0.6556962  0.62673611 0.         0.36363636 0.7755102  1.\n",
      " 1.         0.80519481 0.55629139 0.63565891 0.7265437  0.8\n",
      " 0.70588235]\n",
      "f1_micro = 0.7226250906453953\n",
      "f1_macro = 0.6476973947926626\n",
      "f1_weighted = 0.7220639464373831\n",
      "accuracy = 0.7226250906453953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anush\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "yhat_train = cnn_predict(X_train, model, x_processor=lambda x:{'fn': cnn_name_processor(x[0]), 'sn':cnn_name_processor(x[1])})\n",
    "print('Train..')\n",
    "for metric, metric_fn in metrics_dict.items():\n",
    "    print(metric,\"=\", metric_fn(y_train, yhat_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev..\n",
      "f1_all = [0.         0.75917637 0.3030303  0.         0.69624573 0.\n",
      " 0.65147783 0.55       0.         0.36363636 0.66666667 0.50617284\n",
      " 0.57254902 0.52010724 0.         0.         0.72340426 0.\n",
      " 0.         0.75704989 0.42857143 0.44688645 0.60628931 0.\n",
      " 0.61538462]\n",
      "f1_micro = 0.6511691136487221\n",
      "f1_macro = 0.43650706230933\n",
      "f1_weighted = 0.6486790912431675\n",
      "accuracy = 0.6511691136487221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anush\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anush\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "yhat_dev = cnn_predict(X_dev, model, x_processor=lambda x:{'fn': cnn_name_processor(x[0]), 'sn':cnn_name_processor(x[1])})\n",
    "print('Dev..')\n",
    "for metric, metric_fn in metrics_dict.items():\n",
    "    print(metric,\"=\", metric_fn(y_dev, yhat_dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring name representations (last GRU state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_output = Model(inputs=model.input, outputs=model.layers[-2].output)\n",
    "name_vecs = cnn_predict(df[['hindi_Name','hindi_Surname']][fullnames_mask].values, layer_output, argmax=False, x_processor=lambda x:{'fn': cnn_name_processor(x[0]), 'sn':cnn_name_processor(x[1])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "name_vecs = np.squeeze(name_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14710, 100)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=n_classes, random_state=RANDOM_STATE).fit(name_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_vecs_clusters = km.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07553715012844152"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#how well do name vector clusters correspond with community\n",
    "adjusted_rand_score(name_vecs_clusters, labels[fullnames_mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning Conclusion\n",
    "\n",
    "With Hindi script also, Logistic Regression seems to be the best option, taking both dev scores and speed of training and predicting into consideration.\n",
    "\n",
    "The neural network models may have better performance with more data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from keras.layers import Lambda, RepeatVector, Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_vocab = ord('ॿ') - ord('ऀ')  +1\n",
    "n_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_dev, y_train, y_dev = train_test_split(df[['hindi_Name','hindi_Surname']][fullnames_mask].values, labels[fullnames_mask], random_state = RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoencoder_generator(X, one_iter=False, x_processor=None, n_vocab=n_vocab):\n",
    "    if x_processor is None:\n",
    "        x_processor = lambda x: to_categorical(get_name_val(x).reshape(1,-1), num_classes=n_vocab)\n",
    "        \n",
    "    while True:\n",
    "        for x_i in X:\n",
    "            yield (x_processor(x_i), x_processor(x_i))\n",
    "    \n",
    "        if one_iter:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoencoder_generator2(X, one_iter=False, x_processor=None):\n",
    "    while True:\n",
    "        for i in range(X.shape[-1]):\n",
    "            for x_i, y_i  in autoencoder_generator(X[:,i], one_iter=True, x_processor=x_processor):\n",
    "                yield (x_i, y_i)\n",
    "                \n",
    "        if one_iter:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_vector(args):\n",
    "        layer_to_repeat = args[0]\n",
    "        sequence_layer = args[1]\n",
    "        return RepeatVector(K.shape(sequence_layer)[1])(layer_to_repeat)\n",
    "\n",
    "inp = Input(batch_shape=(1,None,n_vocab))\n",
    "x = Bidirectional(GRU(32, return_sequences=True))(inp)\n",
    "x = Bidirectional(GRU(32, return_sequences=False))(x)\n",
    "x = Dense(4)(x)\n",
    "x = Lambda(repeat_vector, output_shape=(None,4))([x, inp])\n",
    "x = GRU(64, return_sequences=True)(x)\n",
    "x = GRU(64, return_sequences=True)(x)\n",
    "out = Dense(n_vocab, activation='softmax')(x)\n",
    "model = Model(inputs=inp, outputs=out)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (1, None, 128)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (1, None, 64)        30912       input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (1, 64)              18624       bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (1, 4)               260         bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (1, None, 4)         0           dense_4[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "gru_3 (GRU)                     (1, None, 64)        13248       lambda_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "gru_4 (GRU)                     (1, None, 64)        24768       gru_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (1, None, 128)       8320        gru_4[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 96,132\n",
      "Trainable params: 96,132\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "22064/22064 [==============================] - 178s 8ms/step - loss: 1.8382 - acc: 0.4982 - val_loss: 1.6561 - val_acc: 0.5547\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.65612, saving model to hindi_checkpoints\\model_autoencoder2.hdf5\n",
      "Epoch 2/20\n",
      "22064/22064 [==============================] - 180s 8ms/step - loss: 1.2672 - acc: 0.6527 - val_loss: 1.3754 - val_acc: 0.6388\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.65612 to 1.37539, saving model to hindi_checkpoints\\model_autoencoder2.hdf5\n",
      "Epoch 3/20\n",
      "22064/22064 [==============================] - 175s 8ms/step - loss: 1.0954 - acc: 0.6987 - val_loss: 1.2827 - val_acc: 0.6563\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.37539 to 1.28275, saving model to hindi_checkpoints\\model_autoencoder2.hdf5\n",
      "Epoch 4/20\n",
      "22064/22064 [==============================] - 176s 8ms/step - loss: 1.0018 - acc: 0.7268 - val_loss: 1.1560 - val_acc: 0.6917\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.28275 to 1.15601, saving model to hindi_checkpoints\\model_autoencoder2.hdf5\n",
      "Epoch 5/20\n",
      "22064/22064 [==============================] - 182s 8ms/step - loss: 0.9435 - acc: 0.7421 - val_loss: 1.1176 - val_acc: 0.7028\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.15601 to 1.11761, saving model to hindi_checkpoints\\model_autoencoder2.hdf5\n",
      "Epoch 6/20\n",
      "22064/22064 [==============================] - 181s 8ms/step - loss: 0.9011 - acc: 0.7548 - val_loss: 1.0855 - val_acc: 0.7146\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.11761 to 1.08547, saving model to hindi_checkpoints\\model_autoencoder2.hdf5\n",
      "Epoch 7/20\n",
      "22064/22064 [==============================] - 179s 8ms/step - loss: 0.8683 - acc: 0.7653 - val_loss: 1.0741 - val_acc: 0.7175\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.08547 to 1.07406, saving model to hindi_checkpoints\\model_autoencoder2.hdf5\n",
      "Epoch 8/20\n",
      "22064/22064 [==============================] - 178s 8ms/step - loss: 0.8423 - acc: 0.7730 - val_loss: 1.0796 - val_acc: 0.7137\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.07406\n",
      "Epoch 9/20\n",
      "22064/22064 [==============================] - 180s 8ms/step - loss: 0.8280 - acc: 0.7784 - val_loss: 1.0305 - val_acc: 0.7275\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.07406 to 1.03055, saving model to hindi_checkpoints\\model_autoencoder2.hdf5\n",
      "Epoch 10/20\n",
      "22064/22064 [==============================] - 179s 8ms/step - loss: 0.8064 - acc: 0.7838 - val_loss: 0.9854 - val_acc: 0.7405\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.03055 to 0.98537, saving model to hindi_checkpoints\\model_autoencoder2.hdf5\n",
      "Epoch 11/20\n",
      "22064/22064 [==============================] - 181s 8ms/step - loss: 0.7892 - acc: 0.7895 - val_loss: 0.9768 - val_acc: 0.7477\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.98537 to 0.97679, saving model to hindi_checkpoints\\model_autoencoder2.hdf5\n",
      "Epoch 12/20\n",
      "22064/22064 [==============================] - 185s 8ms/step - loss: 0.7905 - acc: 0.7887 - val_loss: 0.9673 - val_acc: 0.7473\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.97679 to 0.96730, saving model to hindi_checkpoints\\model_autoencoder2.hdf5\n",
      "Epoch 13/20\n",
      "22064/22064 [==============================] - 183s 8ms/step - loss: 0.7807 - acc: 0.7926 - val_loss: 0.9548 - val_acc: 0.7521\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.96730 to 0.95483, saving model to hindi_checkpoints\\model_autoencoder2.hdf5\n",
      "Epoch 14/20\n",
      "22064/22064 [==============================] - 177s 8ms/step - loss: 0.7642 - acc: 0.7970 - val_loss: 0.9742 - val_acc: 0.7493\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.95483\n",
      "Epoch 15/20\n",
      "22064/22064 [==============================] - 184s 8ms/step - loss: 0.7703 - acc: 0.7963 - val_loss: 0.9689 - val_acc: 0.7446\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.95483\n",
      "Epoch 16/20\n",
      "22064/22064 [==============================] - 184s 8ms/step - loss: 0.7555 - acc: 0.7998 - val_loss: 0.9642 - val_acc: 0.7507\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.95483\n",
      "Epoch 17/20\n",
      "22064/22064 [==============================] - 181s 8ms/step - loss: 0.7564 - acc: 0.7999 - val_loss: 0.9710 - val_acc: 0.7481\n",
      "Restoring model weights from the end of the best epoch\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.95483\n",
      "Epoch 00017: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b5bbb304e0>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_point = ModelCheckpoint('hindi_checkpoints\\\\model_autoencoder2.hdf5', verbose=True, save_best_only=True)\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=4, verbose=True, restore_best_weights=True)\n",
    "model.fit_generator(autoencoder_generator2(X_train), validation_data=autoencoder_generator2(X_dev), steps_per_epoch=len(X_train)*2, validation_steps=len(X_dev)*2, epochs=n_epochs, verbose=1, callbacks=[early_stop,check_point])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_processor = lambda x: to_categorical(get_name_val(x).reshape(1,-1), num_classes=n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Model(inputs=model.input, outputs=model.layers[3].output)\n",
    "autoencoded_fn_vecs = encoder.predict_generator(autoencoder_generator(df['hindi_Name'][fullnames_mask]), steps=sum(fullnames_mask))\n",
    "autoencoded_sn_vecs = encoder.predict_generator(autoencoder_generator(df['hindi_Surname'][fullnames_mask]), steps=sum(fullnames_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoded_vecs = np.hstack([autoencoded_fn_vecs,autoencoded_sn_vecs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14710, 8)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoded_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=n_classes, random_state=RANDOM_STATE).fit(autoencoded_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoded_vecs_clusters = km.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.021792811762700384"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#how well do name vector clusters correspond with community\n",
    "adjusted_rand_score(autoencoded_vecs_clusters, labels[fullnames_mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slight improvement in correspondence to communities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoencoder does not learn name representations that map to community very well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying out architecture with larger embedding vector (as n_vocab for Hindi script is larger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_vector(args):\n",
    "        layer_to_repeat = args[0]\n",
    "        sequence_layer = args[1]\n",
    "        return RepeatVector(K.shape(sequence_layer)[1])(layer_to_repeat)\n",
    "\n",
    "inp = Input(batch_shape=(1,None,n_vocab))\n",
    "x = Bidirectional(GRU(32, return_sequences=True))(inp)\n",
    "x = Bidirectional(GRU(32, return_sequences=False))(x)\n",
    "x = Dense(8)(x)\n",
    "x = Lambda(repeat_vector, output_shape=(None,8))([x, inp])\n",
    "x = GRU(64, return_sequences=True)(x)\n",
    "x = GRU(64, return_sequences=True)(x)\n",
    "out = Dense(n_vocab, activation='softmax')(x)\n",
    "model = Model(inputs=inp, outputs=out)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (1, None, 128)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (1, None, 64)        30912       input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_6 (Bidirectional) (1, 64)              18624       bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (1, 8)               520         bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (1, None, 8)         0           dense_4[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "gru_10 (GRU)                    (1, None, 64)        14016       lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "gru_11 (GRU)                    (1, None, 64)        24768       gru_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (1, None, 128)       8320        gru_11[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 97,160\n",
      "Trainable params: 97,160\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "22064/22064 [==============================] - 192s 9ms/step - loss: 1.7794 - acc: 0.5132 - val_loss: 1.5227 - val_acc: 0.5864\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.52266, saving model to hindi_checkpoints\\model_autoencoder3.hdf5\n",
      "Epoch 2/20\n",
      "22064/22064 [==============================] - 175s 8ms/step - loss: 1.0505 - acc: 0.7124 - val_loss: 1.0581 - val_acc: 0.7141\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.52266 to 1.05812, saving model to hindi_checkpoints\\model_autoencoder3.hdf5\n",
      "Epoch 3/20\n",
      "22064/22064 [==============================] - 173s 8ms/step - loss: 0.8156 - acc: 0.7791 - val_loss: 0.9683 - val_acc: 0.7410\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.05812 to 0.96833, saving model to hindi_checkpoints\\model_autoencoder3.hdf5\n",
      "Epoch 4/20\n",
      "22064/22064 [==============================] - 173s 8ms/step - loss: 0.7184 - acc: 0.8052 - val_loss: 0.8595 - val_acc: 0.7747\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.96833 to 0.85947, saving model to hindi_checkpoints\\model_autoencoder3.hdf5\n",
      "Epoch 5/20\n",
      "22064/22064 [==============================] - 173s 8ms/step - loss: 0.6582 - acc: 0.8218 - val_loss: 0.8232 - val_acc: 0.7753\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.85947 to 0.82325, saving model to hindi_checkpoints\\model_autoencoder3.hdf5\n",
      "Epoch 6/20\n",
      "22064/22064 [==============================] - 173s 8ms/step - loss: 0.6216 - acc: 0.8317 - val_loss: 0.7656 - val_acc: 0.7989\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.82325 to 0.76561, saving model to hindi_checkpoints\\model_autoencoder3.hdf5\n",
      "Epoch 7/20\n",
      "22064/22064 [==============================] - 168s 8ms/step - loss: 0.6003 - acc: 0.8387 - val_loss: 0.7197 - val_acc: 0.8088\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.76561 to 0.71970, saving model to hindi_checkpoints\\model_autoencoder3.hdf5\n",
      "Epoch 8/20\n",
      "22064/22064 [==============================] - 168s 8ms/step - loss: 0.5810 - acc: 0.8436 - val_loss: 0.7931 - val_acc: 0.7900\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.71970\n",
      "Epoch 9/20\n",
      "22064/22064 [==============================] - 173s 8ms/step - loss: 0.5660 - acc: 0.8475 - val_loss: 0.7632 - val_acc: 0.8022\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.71970\n",
      "Epoch 10/20\n",
      "22064/22064 [==============================] - 165s 7ms/step - loss: 0.5506 - acc: 0.8521 - val_loss: 0.6993 - val_acc: 0.8168\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.71970 to 0.69933, saving model to hindi_checkpoints\\model_autoencoder3.hdf5\n",
      "Epoch 11/20\n",
      "22064/22064 [==============================] - 159s 7ms/step - loss: 0.5465 - acc: 0.8531 - val_loss: 0.7116 - val_acc: 0.8095\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.69933\n",
      "Epoch 12/20\n",
      "22064/22064 [==============================] - 158s 7ms/step - loss: 0.5369 - acc: 0.8558 - val_loss: 0.6901 - val_acc: 0.8213\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.69933 to 0.69005, saving model to hindi_checkpoints\\model_autoencoder3.hdf5\n",
      "Epoch 13/20\n",
      "22064/22064 [==============================] - 158s 7ms/step - loss: 0.5334 - acc: 0.8553 - val_loss: 0.7390 - val_acc: 0.8068\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.69005\n",
      "Epoch 14/20\n",
      "22064/22064 [==============================] - 158s 7ms/step - loss: 0.5244 - acc: 0.8582 - val_loss: 0.6872 - val_acc: 0.8201\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.69005 to 0.68717, saving model to hindi_checkpoints\\model_autoencoder3.hdf5\n",
      "Epoch 15/20\n",
      "22064/22064 [==============================] - 158s 7ms/step - loss: 0.5177 - acc: 0.8606 - val_loss: 0.7424 - val_acc: 0.8092\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.68717\n",
      "Epoch 16/20\n",
      "22064/22064 [==============================] - 158s 7ms/step - loss: 0.5151 - acc: 0.8618 - val_loss: 0.6699 - val_acc: 0.8266\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.68717 to 0.66987, saving model to hindi_checkpoints\\model_autoencoder3.hdf5\n",
      "Epoch 17/20\n",
      "22064/22064 [==============================] - 162s 7ms/step - loss: 0.5212 - acc: 0.8602 - val_loss: 0.6736 - val_acc: 0.8272\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.66987\n",
      "Epoch 18/20\n",
      "22064/22064 [==============================] - 168s 8ms/step - loss: 0.5121 - acc: 0.8629 - val_loss: 0.6499 - val_acc: 0.8291\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.66987 to 0.64987, saving model to hindi_checkpoints\\model_autoencoder3.hdf5\n",
      "Epoch 19/20\n",
      "22064/22064 [==============================] - 168s 8ms/step - loss: 0.5066 - acc: 0.8630 - val_loss: 0.6884 - val_acc: 0.8222\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.64987\n",
      "Epoch 20/20\n",
      "22064/22064 [==============================] - 174s 8ms/step - loss: 0.5091 - acc: 0.8630 - val_loss: 0.6707 - val_acc: 0.8265\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.64987\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x296f51ee908>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_point = ModelCheckpoint('hindi_checkpoints\\\\model_autoencoder3.hdf5', verbose=True, save_best_only=True)\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=4, verbose=True, restore_best_weights=True)\n",
    "model.fit_generator(autoencoder_generator2(X_train), validation_data=autoencoder_generator2(X_dev), steps_per_epoch=len(X_train)*2, validation_steps=len(X_dev)*2, epochs=n_epochs, verbose=1, callbacks=[early_stop,check_point])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "22064/22064 [==============================] - 176s 8ms/step - loss: 0.5125 - acc: 0.8619 - val_loss: 0.6554 - val_acc: 0.8319\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.65538, saving model to hindi_checkpoints\\model_autoencoder3.hdf5\n",
      "Epoch 2/20\n",
      "22064/22064 [==============================] - 167s 8ms/step - loss: 0.5118 - acc: 0.8629 - val_loss: 0.6656 - val_acc: 0.8293\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.65538\n",
      "Epoch 3/20\n",
      "22064/22064 [==============================] - 166s 8ms/step - loss: 0.5066 - acc: 0.8637 - val_loss: 0.6385 - val_acc: 0.8349\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.65538 to 0.63845, saving model to hindi_checkpoints\\model_autoencoder3.hdf5\n",
      "Epoch 4/20\n",
      "22064/22064 [==============================] - 170s 8ms/step - loss: 0.5054 - acc: 0.8648 - val_loss: 0.6497 - val_acc: 0.8323\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.63845\n",
      "Epoch 5/20\n",
      "22064/22064 [==============================] - 169s 8ms/step - loss: 0.5073 - acc: 0.8629 - val_loss: 0.6802 - val_acc: 0.8274\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.63845\n",
      "Epoch 6/20\n",
      "22064/22064 [==============================] - 173s 8ms/step - loss: 0.5150 - acc: 0.8624 - val_loss: 0.6574 - val_acc: 0.8298\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.63845\n",
      "Epoch 7/20\n",
      "22064/22064 [==============================] - 186s 8ms/step - loss: 0.5107 - acc: 0.8631 - val_loss: 0.6449 - val_acc: 0.8292\n",
      "Restoring model weights from the end of the best epoch\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.63845\n",
      "Epoch 00007: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x296f268bb38>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_point = ModelCheckpoint('hindi_checkpoints\\\\model_autoencoder3.hdf5', verbose=True, save_best_only=True)\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=4, verbose=True, restore_best_weights=True)\n",
    "model.fit_generator(autoencoder_generator2(X_train), validation_data=autoencoder_generator2(X_dev), steps_per_epoch=len(X_train)*2, validation_steps=len(X_dev)*2, epochs=n_epochs, verbose=1, callbacks=[early_stop,check_point])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_processor = lambda x: to_categorical(get_name_val(x).reshape(1,-1), num_classes=n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Model(inputs=model.input, outputs=model.layers[3].output)\n",
    "autoencoded_fn_vecs = encoder.predict_generator(autoencoder_generator(df['hindi_Name'][fullnames_mask]), steps=sum(fullnames_mask))\n",
    "autoencoded_sn_vecs = encoder.predict_generator(autoencoder_generator(df['hindi_Surname'][fullnames_mask]), steps=sum(fullnames_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoded_vecs = np.hstack([autoencoded_fn_vecs,autoencoded_sn_vecs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14710, 16)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoded_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=n_classes, random_state=RANDOM_STATE).fit(autoencoded_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoded_vecs_clusters = km.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03886996465147215"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#how well do name vector clusters correspond with community\n",
    "adjusted_rand_score(autoencoded_vecs_clusters, labels[fullnames_mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Hindi script shows some benefits for Naive Bayes models but does not show significant difference for Random Forests and Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
